{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gradient Boosting Demo: All Three Engines\n",
    "\n",
    "This notebook demonstrates the `boost_tree()` model specification with all three supported engines:\n",
    "- **XGBoost**: Fast, scalable gradient boosting\n",
    "- **LightGBM**: Microsoft's efficient gradient boosting framework\n",
    "- **CatBoost**: Yandex's gradient boosting with excellent categorical support\n",
    "\n",
    "## Key Features:\n",
    "- Unified API across all engines\n",
    "- 8 tunable parameters\n",
    "- Feature importance extraction\n",
    "- Early stopping support\n",
    "- Comprehensive performance metrics\n",
    "- Side-by-side engine comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from py_parsnip import boost_tree\n",
    "from py_workflows import workflow\n",
    "from py_rsample import initial_split, vfold_cv\n",
    "from py_tune import tune, tune_grid, grid_regular, finalize_workflow\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Introduction to Gradient Boosting\n",
    "\n",
    "Gradient boosting builds an ensemble of weak learners (typically decision trees) sequentially, where each new tree tries to correct the errors of the previous ones.\n",
    "\n",
    "### The 8 Tunable Parameters in boost_tree():\n",
    "\n",
    "1. **trees**: Number of boosting iterations (n_estimators)\n",
    "2. **tree_depth**: Maximum tree depth\n",
    "3. **learn_rate**: Step size shrinkage (0 to 1)\n",
    "4. **mtry**: Number of features to sample per split\n",
    "5. **min_n**: Minimum samples in leaf node\n",
    "6. **loss_reduction**: Minimum loss reduction for split\n",
    "7. **sample_size**: Fraction of observations per tree (0 to 1)\n",
    "8. **stop_iter**: Early stopping rounds\n",
    "\n",
    "### Engine-Specific Parameter Mappings:\n",
    "\n",
    "| Parameter | XGBoost | LightGBM | CatBoost |\n",
    "|-----------|---------|----------|----------|\n",
    "| trees | n_estimators | n_estimators | iterations |\n",
    "| tree_depth | max_depth | max_depth | depth |\n",
    "| learn_rate | learning_rate | learning_rate | learning_rate |\n",
    "| mtry | colsample_bytree | colsample_bytree | rsm |\n",
    "| min_n | min_child_weight | min_data_in_leaf | min_data_in_leaf |\n",
    "| loss_reduction | gamma | min_split_gain | (not supported) |\n",
    "| sample_size | subsample | subsample | subsample |\n",
    "| stop_iter | early_stopping_rounds | early_stopping_rounds | early_stopping_rounds |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "\n",
    "Create a synthetic regression dataset with non-linear relationships that benefit from gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with non-linear relationships\n",
    "n = 500\n",
    "data = pd.DataFrame({\n",
    "    'x1': np.random.uniform(0, 10, n),\n",
    "    'x2': np.random.uniform(0, 10, n),\n",
    "    'x3': np.random.uniform(0, 10, n),\n",
    "    'x4': np.random.uniform(0, 10, n),\n",
    "    'x5': np.random.uniform(0, 10, n),\n",
    "    'x6': np.random.randint(0, 5, n),  # Categorical-like\n",
    "})\n",
    "\n",
    "# Create target with complex non-linear relationships\n",
    "data['y'] = (\n",
    "    2 * data['x1'] +\n",
    "    3 * np.sin(data['x2']) +\n",
    "    0.5 * data['x3'] ** 2 +\n",
    "    1.5 * np.log(data['x4'] + 1) +\n",
    "    0.8 * data['x5'] * data['x6'] +\n",
    "    np.random.normal(0, 2, n)\n",
    ")\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(data['y'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train = data.iloc[:400].copy()\n",
    "test = data.iloc[400:].copy()\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. XGBoost Engine\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is one of the most popular gradient boosting implementations, known for its speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 3.1 Basic XGBoost Model with All 8 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost specification with all parameters\n",
    "spec_xgb = boost_tree(\n",
    "    trees=200,              # Number of boosting rounds\n",
    "    tree_depth=6,           # Maximum tree depth\n",
    "    learn_rate=0.1,         # Learning rate (eta)\n",
    "    mtry=4,                 # Features to sample per split\n",
    "    min_n=10,               # Minimum child weight\n",
    "    loss_reduction=0.01,    # Gamma (minimum loss reduction)\n",
    "    sample_size=0.8,        # Row sampling fraction\n",
    "    stop_iter=20,           # Early stopping rounds\n",
    "    engine=\"xgboost\"\n",
    ").set_mode(\"regression\")\n",
    "\n",
    "print(\"XGBoost Model Specification:\")\n",
    "print(spec_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "start_time = time.time()\n",
    "fit_xgb = spec_xgb.fit(train, \"y ~ x1 + x2 + x3 + x4 + x5 + x6\")\n",
    "xgb_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model fitted in {xgb_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 3.2 XGBoost Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "predictions_xgb = fit_xgb.predict(test, type=\"numeric\")\n",
    "print(\"Predictions (first 10):\")\n",
    "print(predictions_xgb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "fit_xgb = fit_xgb.evaluate(test)\n",
    "print(\"Model evaluated on test data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive outputs\n",
    "outputs_xgb, feature_importance_xgb, stats_xgb = fit_xgb.extract_outputs()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"XGBoost Feature Importance\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance_xgb)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_xgb['variable'], feature_importance_xgb['importance'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('XGBoost Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics by split\n",
    "print(\"=\"*60)\n",
    "print(\"XGBoost Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metric_cols = ['rmse', 'mae', 'r_squared', 'mape']\n",
    "metrics_subset = stats_xgb[stats_xgb['metric'].isin(metric_cols)]\n",
    "\n",
    "if not metrics_subset.empty:\n",
    "    metrics_pivot = metrics_subset.pivot(index='metric', columns='split', values='value')\n",
    "    print(metrics_pivot)\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Lower RMSE/MAE is better\")\n",
    "    print(\"- Higher R-squared is better (closer to 1)\")\n",
    "    print(\"- Compare train vs test to assess overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "test_outputs = outputs_xgb[outputs_xgb['split'] == 'test'].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_outputs['actuals'], test_outputs['forecast'], alpha=0.6, s=50)\n",
    "plt.plot([test_outputs['actuals'].min(), test_outputs['actuals'].max()],\n",
    "         [test_outputs['actuals'].min(), test_outputs['actuals'].max()],\n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('XGBoost: Predictions vs Actuals (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 4. LightGBM Engine\n",
    "\n",
    "LightGBM is Microsoft's gradient boosting framework that uses histogram-based algorithms for faster training and lower memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 4.1 Basic LightGBM Model with All 8 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM specification with same parameters for comparison\n",
    "spec_lgb = boost_tree(\n",
    "    trees=200,              # Number of boosting rounds\n",
    "    tree_depth=6,           # Maximum tree depth\n",
    "    learn_rate=0.1,         # Learning rate\n",
    "    mtry=4,                 # Features to sample per split\n",
    "    min_n=10,               # Minimum data in leaf\n",
    "    loss_reduction=0.01,    # Minimum split gain\n",
    "    sample_size=0.8,        # Row sampling fraction\n",
    "    stop_iter=20,           # Early stopping rounds\n",
    "    engine=\"lightgbm\"\n",
    ").set_mode(\"regression\")\n",
    "\n",
    "print(\"LightGBM Model Specification:\")\n",
    "print(spec_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "start_time = time.time()\n",
    "fit_lgb = spec_lgb.fit(train, \"y ~ x1 + x2 + x3 + x4 + x5 + x6\")\n",
    "lgb_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model fitted in {lgb_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 4.2 LightGBM Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "predictions_lgb = fit_lgb.predict(test, type=\"numeric\")\n",
    "fit_lgb = fit_lgb.evaluate(test)\n",
    "\n",
    "print(\"LightGBM predictions (first 10):\")\n",
    "print(predictions_lgb.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 4.3 LightGBM Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive outputs\n",
    "outputs_lgb, feature_importance_lgb, stats_lgb = fit_lgb.extract_outputs()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Feature Importance\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance_lgb)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_lgb['variable'], feature_importance_lgb['importance'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('LightGBM Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 4.4 LightGBM Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics by split\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_subset_lgb = stats_lgb[stats_lgb['metric'].isin(metric_cols)]\n",
    "\n",
    "if not metrics_subset_lgb.empty:\n",
    "    metrics_pivot_lgb = metrics_subset_lgb.pivot(index='metric', columns='split', values='value')\n",
    "    print(metrics_pivot_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "test_outputs_lgb = outputs_lgb[outputs_lgb['split'] == 'test'].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_outputs_lgb['actuals'], test_outputs_lgb['forecast'], alpha=0.6, s=50, color='green')\n",
    "plt.plot([test_outputs_lgb['actuals'].min(), test_outputs_lgb['actuals'].max()],\n",
    "         [test_outputs_lgb['actuals'].min(), test_outputs_lgb['actuals'].max()],\n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('LightGBM: Predictions vs Actuals (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 5. CatBoost Engine\n",
    "\n",
    "CatBoost is Yandex's gradient boosting library with built-in support for categorical features and robust default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### 5.1 Basic CatBoost Model with All 8 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CatBoost specification with same parameters for comparison\n",
    "spec_cat = boost_tree(\n",
    "    trees=200,              # Number of boosting iterations\n",
    "    tree_depth=6,           # Maximum tree depth\n",
    "    learn_rate=0.1,         # Learning rate\n",
    "    mtry=4,                 # Features to sample per split (rsm)\n",
    "    min_n=10,               # Minimum data in leaf\n",
    "    loss_reduction=0.01,    # Not supported in CatBoost\n",
    "    sample_size=0.8,        # Row sampling fraction\n",
    "    stop_iter=20,           # Early stopping rounds\n",
    "    engine=\"catboost\"\n",
    ").set_mode(\"regression\")\n",
    "\n",
    "print(\"CatBoost Model Specification:\")\n",
    "print(spec_cat)\n",
    "print(\"\\nNote: loss_reduction parameter is not supported by CatBoost and will be ignored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "start_time = time.time()\n",
    "fit_cat = spec_cat.fit(train, \"y ~ x1 + x2 + x3 + x4 + x5 + x6\")\n",
    "cat_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model fitted in {cat_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### 5.2 CatBoost Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "predictions_cat = fit_cat.predict(test, type=\"numeric\")\n",
    "fit_cat = fit_cat.evaluate(test)\n",
    "\n",
    "print(\"CatBoost predictions (first 10):\")\n",
    "print(predictions_cat.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### 5.3 CatBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive outputs\n",
    "outputs_cat, feature_importance_cat, stats_cat = fit_cat.extract_outputs()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CatBoost Feature Importance\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance_cat)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_cat['variable'], feature_importance_cat['importance'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('CatBoost Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "### 5.4 CatBoost Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics by split\n",
    "print(\"=\"*60)\n",
    "print(\"CatBoost Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_subset_cat = stats_cat[stats_cat['metric'].isin(metric_cols)]\n",
    "\n",
    "if not metrics_subset_cat.empty:\n",
    "    metrics_pivot_cat = metrics_subset_cat.pivot(index='metric', columns='split', values='value')\n",
    "    print(metrics_pivot_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "test_outputs_cat = outputs_cat[outputs_cat['split'] == 'test'].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_outputs_cat['actuals'], test_outputs_cat['forecast'], alpha=0.6, s=50, color='purple')\n",
    "plt.plot([test_outputs_cat['actuals'].min(), test_outputs_cat['actuals'].max()],\n",
    "         [test_outputs_cat['actuals'].min(), test_outputs_cat['actuals'].max()],\n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('CatBoost: Predictions vs Actuals (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Engine Comparison\n",
    "\n",
    "Compare all three engines on the same dataset to understand their relative strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### 6.1 Performance Metrics Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = []\n",
    "\n",
    "# Extract test metrics for each engine\n",
    "for engine_name, stats_df in [('XGBoost', stats_xgb), ('LightGBM', stats_lgb), ('CatBoost', stats_cat)]:\n",
    "    test_metrics = stats_df[(stats_df['split'] == 'test') & (stats_df['metric'].isin(metric_cols))]\n",
    "    \n",
    "    metrics_dict = {'Engine': engine_name}\n",
    "    for _, row in test_metrics.iterrows():\n",
    "        metrics_dict[row['metric'].upper()] = row['value']\n",
    "    \n",
    "    comparison_data.append(metrics_dict)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: TEST SET METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nLower RMSE/MAE/MAPE and higher R_SQUARED are better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add training time comparison\n",
    "time_comparison = pd.DataFrame({\n",
    "    'Engine': ['XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Training Time (seconds)': [xgb_train_time, lgb_train_time, cat_train_time]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING TIME COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(time_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### 6.2 Visual Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE and R-squared comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_values = comparison_df['RMSE'].values\n",
    "engines = comparison_df['Engine'].values\n",
    "colors = ['#1f77b4', '#2ca02c', '#9467bd']\n",
    "\n",
    "axes[0].bar(engines, rmse_values, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('RMSE (lower is better)', fontsize=12)\n",
    "axes[0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# R-squared comparison\n",
    "r2_values = comparison_df['R_SQUARED'].values\n",
    "\n",
    "axes[1].bar(engines, r2_values, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('R-squared (higher is better)', fontsize=12)\n",
    "axes[1].set_title('R-squared Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([r2_values.min() * 0.95, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "### 6.3 Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances across engines\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Normalize importances to 0-1 scale for fair comparison\n",
    "for idx, (importance_df, engine_name, color) in enumerate([\n",
    "    (feature_importance_xgb, 'XGBoost', '#1f77b4'),\n",
    "    (feature_importance_lgb, 'LightGBM', '#2ca02c'),\n",
    "    (feature_importance_cat, 'CatBoost', '#9467bd')\n",
    "]):\n",
    "    # Normalize to sum to 1\n",
    "    normalized_importance = importance_df.copy()\n",
    "    total = normalized_importance['importance'].sum()\n",
    "    if total > 0:\n",
    "        normalized_importance['importance'] = normalized_importance['importance'] / total\n",
    "    \n",
    "    axes[idx].barh(normalized_importance['variable'], normalized_importance['importance'], color=color, alpha=0.7)\n",
    "    axes[idx].set_xlabel('Normalized Importance', fontsize=11)\n",
    "    axes[idx].set_title(f'{engine_name}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Importance Comparison Across Engines', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Importances are normalized to sum to 1 for fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "### 6.4 When to Use Each Engine\n",
    "\n",
    "#### XGBoost:\n",
    "- **Strengths**: \n",
    "  - Mature, well-documented, widely used\n",
    "  - Excellent performance on structured data\n",
    "  - Rich ecosystem and community support\n",
    "  - Strong regularization options (gamma, lambda, alpha)\n",
    "- **Best for**: General-purpose gradient boosting, competitions, production systems\n",
    "- **Considerations**: Can be memory-intensive for large datasets\n",
    "\n",
    "#### LightGBM:\n",
    "- **Strengths**:\n",
    "  - Faster training speed (histogram-based)\n",
    "  - Lower memory consumption\n",
    "  - Good performance on large datasets\n",
    "  - Handles sparse data efficiently\n",
    "- **Best for**: Large datasets (>10k rows), speed-critical applications\n",
    "- **Considerations**: May overfit on small datasets (<1k rows)\n",
    "\n",
    "#### CatBoost:\n",
    "- **Strengths**:\n",
    "  - Excellent handling of categorical features (no preprocessing needed)\n",
    "  - Robust default parameters (less tuning required)\n",
    "  - Good out-of-the-box performance\n",
    "  - Less prone to overfitting\n",
    "- **Best for**: Datasets with many categorical features, quick prototyping\n",
    "- **Considerations**: Can be slower than LightGBM on very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning with tune_grid\n",
    "\n",
    "Demonstrate automated hyperparameter tuning for gradient boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model specification with tunable parameters\n",
    "spec_tune = boost_tree(\n",
    "    trees=tune('trees'),\n",
    "    tree_depth=tune('tree_depth'),\n",
    "    learn_rate=tune('learn_rate'),\n",
    "    engine=\"xgboost\"\n",
    ").set_mode(\"regression\")\n",
    "\n",
    "print(\"Model specification with tunable parameters:\")\n",
    "print(spec_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "wf_tune = workflow().add_formula(\"y ~ x1 + x2 + x3 + x4 + x5 + x6\").add_model(spec_tune)\n",
    "\n",
    "# Create cross-validation folds\n",
    "folds = vfold_cv(train, v=3)\n",
    "\n",
    "print(f\"Created workflow with {len(folds)} cross-validation folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_info = {\n",
    "    'trees': {'range': (50, 200), 'trans': 'log'},\n",
    "    'tree_depth': {'range': (3, 8)},\n",
    "    'learn_rate': {'range': (0.01, 0.3), 'trans': 'log'}\n",
    "}\n",
    "\n",
    "param_grid = grid_regular(param_info, levels=3)\n",
    "\n",
    "print(f\"Parameter grid with {len(param_grid)} combinations:\")\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metrics = metric_set(rmse, mae, r_squared)\n",
    "\n",
    "# Run grid search\n",
    "print(\"Running grid search... (this may take a minute)\")\n",
    "tune_results = tune_grid(\n",
    "    wf_tune,\n",
    "    folds,\n",
    "    grid=param_grid,\n",
    "    metrics=metrics,\n",
    "    control={'save_pred': False}\n",
    ")\n",
    "\n",
    "print(\"\\nGrid search complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best configurations\n",
    "best_configs = tune_results.show_best('rmse', n=5, maximize=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 5 CONFIGURATIONS BY RMSE\")\n",
    "print(\"=\"*80)\n",
    "print(best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best parameters\n",
    "best_params = tune_results.select_best('rmse', maximize=False)\n",
    "\n",
    "print(\"Best parameters (lowest RMSE):\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tuning results\n",
    "all_metrics = tune_results.collect_metrics()\n",
    "\n",
    "# Calculate mean RMSE for each config\n",
    "rmse_summary = all_metrics[all_metrics['metric'] == 'rmse'].groupby('.config').agg({\n",
    "    'value': ['mean', 'std']\n",
    "}).reset_index()\n",
    "rmse_summary.columns = ['.config', 'mean_rmse', 'std_rmse']\n",
    "\n",
    "# Merge with parameter values\n",
    "rmse_with_params = rmse_summary.merge(param_grid, on='.config')\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RMSE vs trees (colored by learn_rate)\n",
    "scatter1 = axes[0].scatter(rmse_with_params['trees'], rmse_with_params['mean_rmse'], \n",
    "                           c=rmse_with_params['learn_rate'], s=100, alpha=0.7, cmap='viridis')\n",
    "axes[0].set_xlabel('Number of Trees', fontsize=12)\n",
    "axes[0].set_ylabel('Mean RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE vs Trees (colored by learning rate)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Learning Rate')\n",
    "\n",
    "# RMSE vs tree_depth (colored by learn_rate)\n",
    "scatter2 = axes[1].scatter(rmse_with_params['tree_depth'], rmse_with_params['mean_rmse'],\n",
    "                           c=rmse_with_params['learn_rate'], s=100, alpha=0.7, cmap='viridis')\n",
    "axes[1].set_xlabel('Tree Depth', fontsize=12)\n",
    "axes[1].set_ylabel('Mean RMSE', fontsize=12)\n",
    "axes[1].set_title('RMSE vs Tree Depth (colored by learning rate)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize workflow with best parameters\n",
    "final_wf = finalize_workflow(wf_tune, best_params)\n",
    "\n",
    "# Fit on full training data\n",
    "final_fit = final_wf.fit(train)\n",
    "\n",
    "# Predict on test data\n",
    "final_predictions = final_fit.predict(test)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_rmse = rmse(test['y'], final_predictions['.pred'])\n",
    "test_r2 = r_squared(test['y'], final_predictions['.pred'])\n",
    "\n",
    "print(\"Final tuned model performance on test set:\")\n",
    "print(f\"RMSE: {test_rmse['rmse'].values[0]:.4f}\")\n",
    "print(f\"R-squared: {test_r2['r_squared'].values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-58",
   "metadata": {},
   "source": [
    "## 8. Extract Outputs for All Three Engines\n",
    "\n",
    "Comprehensive overview of the three-DataFrame output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "### 8.1 Outputs DataFrame (Observation-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"OUTPUTS DATAFRAME: Observation-level predictions and residuals\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nXGBoost Outputs (first 10 rows):\")\n",
    "print(outputs_xgb.head(10))\n",
    "print(f\"\\nShape: {outputs_xgb.shape}\")\n",
    "print(f\"Columns: {list(outputs_xgb.columns)}\")\n",
    "\n",
    "print(\"\\nKey columns:\")\n",
    "print(\"- actuals: True target values\")\n",
    "print(\"- fitted: Model predictions on training data\")\n",
    "print(\"- forecast: Best available prediction (actuals if present, else fitted)\")\n",
    "print(\"- residuals: actuals - fitted\")\n",
    "print(\"- split: 'train' or 'test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-61",
   "metadata": {},
   "source": [
    "### 8.2 Feature Importance DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE DATAFRAME: Variable importance scores\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare feature importances side-by-side\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'Variable': feature_importance_xgb['variable'],\n",
    "    'XGBoost': feature_importance_xgb['importance'].values,\n",
    "    'LightGBM': feature_importance_lgb['importance'].values,\n",
    "    'CatBoost': feature_importance_cat['importance'].values\n",
    "})\n",
    "\n",
    "print(importance_comparison)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher importance = feature contributes more to predictions\")\n",
    "print(\"- Importance measures vary by engine (gain, split count, etc.)\")\n",
    "print(\"- Use for feature selection and understanding model behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-63",
   "metadata": {},
   "source": [
    "### 8.3 Stats DataFrame (Model-level Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STATS DATAFRAME: Model-level metrics and metadata\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nXGBoost Stats (performance metrics):\")\n",
    "print(stats_xgb[stats_xgb['metric'].isin(['rmse', 'mae', 'r_squared', 'mape', 'model_type', 'n_obs_train'])])\n",
    "\n",
    "print(\"\\nKey metrics:\")\n",
    "print(\"- rmse: Root Mean Squared Error\")\n",
    "print(\"- mae: Mean Absolute Error\")\n",
    "print(\"- r_squared: Coefficient of determination\")\n",
    "print(\"- mape: Mean Absolute Percentage Error\")\n",
    "print(\"\\nMetrics are split by 'train' and 'test' to assess overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-65",
   "metadata": {},
   "source": [
    "### 8.4 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare residuals across engines\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (outputs_df, engine_name, color) in enumerate([\n",
    "    (outputs_xgb, 'XGBoost', '#1f77b4'),\n",
    "    (outputs_lgb, 'LightGBM', '#2ca02c'),\n",
    "    (outputs_cat, 'CatBoost', '#9467bd')\n",
    "]):\n",
    "    test_data = outputs_df[outputs_df['split'] == 'test'].copy()\n",
    "    \n",
    "    axes[idx].scatter(test_data['forecast'], test_data['residuals'], alpha=0.6, s=50, color=color)\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Predicted Values', fontsize=11)\n",
    "    axes[idx].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[idx].set_title(f'{engine_name} Residuals', fontsize=13, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Residual Plots: Test Set Comparison', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Good residual plots should:\")\n",
    "print(\"- Show random scatter around zero\")\n",
    "print(\"- Have constant variance (homoscedasticity)\")\n",
    "print(\"- Show no obvious patterns or trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-67",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo covered:\n",
    "\n",
    "### 1. **All 8 Parameters** of boost_tree():\n",
    "   - trees, tree_depth, learn_rate, mtry, min_n, loss_reduction, sample_size, stop_iter\n",
    "   - Consistent API across all three engines\n",
    "\n",
    "### 2. **Three Gradient Boosting Engines**:\n",
    "   - **XGBoost**: Industry standard, excellent performance\n",
    "   - **LightGBM**: Fast training, efficient memory usage\n",
    "   - **CatBoost**: Great categorical handling, robust defaults\n",
    "\n",
    "### 3. **Feature Importance**:\n",
    "   - Extract and compare feature importances\n",
    "   - Understand which features drive predictions\n",
    "   - Different engines may rank features differently\n",
    "\n",
    "### 4. **Early Stopping**:\n",
    "   - Prevent overfitting with stop_iter parameter\n",
    "   - Automatically uses validation set (last 20% of training)\n",
    "   - Supported by all three engines\n",
    "\n",
    "### 5. **Performance Comparison**:\n",
    "   - Side-by-side metrics table\n",
    "   - Training time comparison\n",
    "   - Visual performance analysis\n",
    "\n",
    "### 6. **Hyperparameter Tuning**:\n",
    "   - Use tune_grid() for automated search\n",
    "   - Cross-validation for robust evaluation\n",
    "   - Visualize tuning results\n",
    "\n",
    "### 7. **Comprehensive Outputs**:\n",
    "   - **Outputs**: Observation-level predictions and residuals\n",
    "   - **Feature Importance**: Variable importance scores\n",
    "   - **Stats**: Model-level metrics by split\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "1. All three engines provide similar performance with proper tuning\n",
    "2. LightGBM is typically fastest for large datasets\n",
    "3. CatBoost requires less tuning and handles categoricals well\n",
    "4. XGBoost has the most mature ecosystem and documentation\n",
    "5. Use cross-validation and hyperparameter tuning for best results\n",
    "6. Monitor train vs test metrics to detect overfitting\n",
    "7. Feature importance helps interpret and debug models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
