{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965ab41f",
   "metadata": {},
   "source": [
    "# Backtesting with Data Vintages\n",
    "\n",
    "This notebook demonstrates production-realistic backtesting using point-in-time data vintages.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. Creating synthetic vintage data\n",
    "2. VintageCV setup for time-series cross-validation\n",
    "3. WorkflowSet backtesting across multiple models\n",
    "4. Vintage drift analysis\n",
    "5. Forecast horizon performance\n",
    "6. Comparing vintage vs final data\n",
    "7. Production forecasting workflow\n",
    "\n",
    "**Use Case:** Commodity price forecasting with data revisions\n",
    "\n",
    "**Why Vintages Matter:**\n",
    "- Real-world data gets revised (GDP, employment, earnings)\n",
    "- Training on \"final\" data = data leakage\n",
    "- Vintage data simulates production conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b137249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_workflows import workflow\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_backtest import create_vintage_data, VintageCV, validate_vintage_data\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5c874",
   "metadata": {},
   "source": [
    "## 1. Generate Commodity Price Data\n",
    "\n",
    "Create realistic commodity price time series with:\n",
    "- Trend component\n",
    "- Seasonal pattern\n",
    "- Exogenous predictors (USD index, demand)\n",
    "- Random shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate commodity price data (monthly)\n",
    "start_date = pd.to_datetime('2020-01-01')\n",
    "n_months = 60  # 5 years of monthly data\n",
    "dates = pd.date_range(start=start_date, periods=n_months, freq='MS')\n",
    "\n",
    "# Trend component\n",
    "trend = np.linspace(50, 80, n_months)\n",
    "\n",
    "# Seasonal component (annual cycle)\n",
    "seasonality = 10 * np.sin(2 * np.pi * np.arange(n_months) / 12)\n",
    "\n",
    "# Exogenous variables\n",
    "usd_index = 100 + np.cumsum(np.random.randn(n_months) * 2)  # USD strength\n",
    "demand_index = 80 + np.cumsum(np.random.randn(n_months) * 1.5)  # Demand\n",
    "\n",
    "# True price relationship\n",
    "true_price = (\n",
    "    trend +\n",
    "    seasonality +\n",
    "    0.2 * (usd_index - 100) +  # USD effect\n",
    "    0.3 * (demand_index - 80) +  # Demand effect\n",
    "    np.random.randn(n_months) * 3  # Random noise\n",
    ")\n",
    "\n",
    "# Create \"final\" dataset (what we'd have with perfect hindsight)\n",
    "final_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'price': true_price,\n",
    "    'usd_index': usd_index,\n",
    "    'demand_index': demand_index\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(final_data)} months of commodity price data\")\n",
    "print(f\"Date range: {final_data['date'].min().date()} to {final_data['date'].max().date()}\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(final_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(final_data['date'], final_data['price'], linewidth=2, label='Price')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title('Commodity Price Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# USD Index\n",
    "axes[1].plot(final_data['date'], final_data['usd_index'], linewidth=2, color='orange', label='USD Index')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('USD Index')\n",
    "axes[1].set_title('USD Strength Index')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Demand Index\n",
    "axes[2].plot(final_data['date'], final_data['demand_index'], linewidth=2, color='green', label='Demand Index')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Demand Index')\n",
    "axes[2].set_title('Demand Index')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56aa26",
   "metadata": {},
   "source": [
    "## 2. Create Vintage Data\n",
    "\n",
    "Simulate data revisions by creating multiple vintages.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Observation Date (date):** When event occurred\n",
    "- **As-of Date (as_of_date):** When data was available\n",
    "- **Vintages:** Multiple snapshots of same observation at different as-of dates\n",
    "\n",
    "**Real-world examples:**\n",
    "- GDP gets revised 3+ times\n",
    "- Employment data revised monthly\n",
    "- Corporate earnings restated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic vintage data\n",
    "# Simulate 3 revisions with 5% standard deviation\n",
    "vintage_data = create_vintage_data(\n",
    "    final_data=final_data,\n",
    "    date_col='date',\n",
    "    n_revisions=4,  # Create 4 vintages per observation\n",
    "    revision_std=0.05,  # 5% std deviation in revisions\n",
    "    revision_delay_months=1,  # First revision available 1 month after observation\n",
    "    value_cols=['price', 'usd_index', 'demand_index']  # Columns to revise\n",
    ")\n",
    "\n",
    "print(f\"\\nVintage data created!\")\n",
    "print(f\"Total rows: {len(vintage_data)} (original: {len(final_data)})\")\n",
    "print(f\"Vintages per observation: {len(vintage_data) // len(final_data)}\")\n",
    "print(f\"\\nColumns: {vintage_data.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(vintage_data.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac92a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate vintage data structure\n",
    "validate_vintage_data(\n",
    "    data=vintage_data,\n",
    "    as_of_col='as_of_date',\n",
    "    date_col='date'\n",
    ")\n",
    "print(\"✓ Vintage data validation passed!\")\n",
    "\n",
    "# Examine revisions for a specific observation\n",
    "example_date = pd.to_datetime('2020-06-01')\n",
    "example_vintages = vintage_data[vintage_data['date'] == example_date].sort_values('as_of_date')\n",
    "\n",
    "print(f\"\\nRevisions for {example_date.date()}:\")\n",
    "print(example_vintages[['date', 'as_of_date', 'price', 'usd_index', 'demand_index']])\n",
    "\n",
    "# Calculate revision magnitude\n",
    "if len(example_vintages) > 1:\n",
    "    first_vintage = example_vintages.iloc[0]['price']\n",
    "    final_vintage = example_vintages.iloc[-1]['price']\n",
    "    revision_pct = abs((final_vintage - first_vintage) / first_vintage) * 100\n",
    "    print(f\"\\nPrice revision: {first_vintage:.2f} → {final_vintage:.2f} ({revision_pct:.1f}% change)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae058829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vintage revisions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Select a few observations to track revisions\n",
    "sample_dates = final_data['date'][::12][:3]  # Every 12 months, first 3\n",
    "\n",
    "for sample_date in sample_dates:\n",
    "    vintages = vintage_data[vintage_data['date'] == sample_date].sort_values('as_of_date')\n",
    "\n",
    "    axes[0].plot(vintages['as_of_date'], vintages['price'],\n",
    "                marker='o', label=f'Obs: {sample_date.date()}')\n",
    "\n",
    "axes[0].set_xlabel('As-of Date (When Data Available)')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].set_title('Price Revisions Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Revision magnitude distribution\n",
    "first_vintages = vintage_data.groupby('date').first()['price']\n",
    "final_vintages = vintage_data.groupby('date').last()['price']\n",
    "revision_pct = abs((final_vintages - first_vintages) / first_vintages * 100)\n",
    "\n",
    "axes[1].hist(revision_pct, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Absolute Revision (%)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Price Revisions')\n",
    "axes[1].axvline(revision_pct.mean(), color='red', linestyle='--',\n",
    "               label=f'Mean: {revision_pct.mean():.1f}%')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad4610",
   "metadata": {},
   "source": [
    "## 3. VintageCV: Cross-Validation with Vintages\n",
    "\n",
    "Create vintage-aware CV splits that respect point-in-time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VintageCV\n",
    "vintage_cv = VintageCV(\n",
    "    data=vintage_data,\n",
    "    as_of_col='as_of_date',\n",
    "    date_col='date',\n",
    "    initial='2 years',  # Initial training window\n",
    "    assess='6 months',  # Test/assessment period\n",
    "    skip='3 months',  # Gap between folds\n",
    "    cumulative=False,  # Rolling window (not expanding)\n",
    "    vintage_selection='latest'  # Use most recent vintage available\n",
    ")\n",
    "\n",
    "print(f\"VintageCV created with {vintage_cv.n_splits} splits\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Initial training: 2 years\")\n",
    "print(f\"  Assessment period: 6 months\")\n",
    "print(f\"  Skip between folds: 3 months\")\n",
    "print(f\"  Window type: Rolling (cumulative=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine CV splits\n",
    "print(\"\\nCV Split Details:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, split in enumerate(vintage_cv.splits[:3]):  # Show first 3 splits\n",
    "    train_data = split.train_data()\n",
    "    test_data = split.test_data()\n",
    "\n",
    "    train_dates = train_data['date']\n",
    "    test_dates = test_data['date']\n",
    "    train_as_of = train_data['as_of_date'].max()\n",
    "    test_as_of = test_data['as_of_date'].max()\n",
    "\n",
    "    print(f\"\\nSplit {i+1}:\")\n",
    "    print(f\"  Train dates: {train_dates.min().date()} to {train_dates.max().date()} ({len(train_data)} rows)\")\n",
    "    print(f\"  Test dates:  {test_dates.min().date()} to {test_dates.max().date()} ({len(test_data)} rows)\")\n",
    "    print(f\"  Train as-of: {train_as_of.date()}\")\n",
    "    print(f\"  Test as-of:  {test_as_of.date()}\")\n",
    "    print(f\"  → Ensures no future information leakage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV splits\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, vintage_cv.n_splits))\n",
    "\n",
    "for i, split in enumerate(vintage_cv.splits):\n",
    "    train_data = split.train_data()\n",
    "    test_data = split.test_data()\n",
    "\n",
    "    # Plot train period\n",
    "    ax.barh(i, (train_data['date'].max() - train_data['date'].min()).days,\n",
    "           left=train_data['date'].min(), height=0.3,\n",
    "           color=colors[i], alpha=0.6, label=f'Split {i+1} Train' if i < 3 else None)\n",
    "\n",
    "    # Plot test period\n",
    "    ax.barh(i, (test_data['date'].max() - test_data['date'].min()).days,\n",
    "           left=test_data['date'].min(), height=0.3,\n",
    "           color=colors[i], alpha=0.9, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CV Split')\n",
    "ax.set_title('VintageCV Time Splits (Dark = Test, Light = Train)')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1663f96",
   "metadata": {},
   "source": [
    "## 4. WorkflowSet Backtesting\n",
    "\n",
    "Evaluate multiple model workflows across all vintage CV splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple workflows for comparison\n",
    "formulas = [\n",
    "    \"price ~ usd_index + demand_index\",  # Simple linear\n",
    "    \"price ~ usd_index + demand_index + I(usd_index**2)\",  # With polynomial\n",
    "]\n",
    "\n",
    "models = [\n",
    "    linear_reg(),\n",
    "    rand_forest(trees=50, min_n=5).set_mode('regression')\n",
    "]\n",
    "\n",
    "# Create WorkflowSet\n",
    "wf_set = WorkflowSet.from_cross(preproc=formulas, models=models)\n",
    "\n",
    "print(f\"Created WorkflowSet with {len(wf_set.workflows)} workflows:\")\n",
    "for wf_id in wf_set.workflows.keys():\n",
    "    print(f\"  - {wf_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f921ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest across all CV splits\n",
    "print(\"\\nRunning backtest...\")\n",
    "print(\"This evaluates all workflows on all CV splits with vintage data\")\n",
    "\n",
    "backtest_results = wf_set.fit_backtests(\n",
    "    vintage_cv,\n",
    "    metrics=metric_set(rmse, mae, r_squared)\n",
    ")\n",
    "\n",
    "print(\"\\nBacktest complete!\")\n",
    "print(f\"Evaluated {len(wf_set.workflows)} workflows × {vintage_cv.n_splits} CV splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349b8d4",
   "metadata": {},
   "source": [
    "## 5. Analyze Backtest Results\n",
    "\n",
    "Examine performance across workflows and time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccab82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics across all splits\n",
    "metrics_summary = backtest_results.collect_metrics()\n",
    "\n",
    "print(\"\\nBacktest Metrics Summary:\")\n",
    "print(metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank workflows by performance\n",
    "ranked = backtest_results.rank_results('rmse', n=10)\n",
    "\n",
    "print(\"\\nWorkflow Rankings (by RMSE):\")\n",
    "print(ranked[['wflow_id', 'rmse_mean', 'rmse_std', 'mae_mean', 'r_squared_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize workflow comparison\n",
    "backtest_results.autoplot('rmse', top_n=4)\n",
    "plt.title('Backtest Performance Comparison (RMSE)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "backtest_results.autoplot('r_squared', top_n=4)\n",
    "plt.title('Backtest Performance Comparison (R²)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447a4f8",
   "metadata": {},
   "source": [
    "## 6. Vintage Drift Analysis\n",
    "\n",
    "Analyze how model performance changes over time (concept drift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vintage drift\n",
    "drift_analysis = backtest_results.analyze_vintage_drift('rmse')\n",
    "\n",
    "print(\"\\nVintage Drift Analysis (RMSE over time):\")\n",
    "print(drift_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b92e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift over time\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for wflow_id in drift_analysis['wflow_id'].unique()[:4]:  # Top 4 workflows\n",
    "    wflow_data = drift_analysis[drift_analysis['wflow_id'] == wflow_id]\n",
    "    ax.plot(wflow_data['vintage_date'], wflow_data['rmse'],\n",
    "           marker='o', label=wflow_id, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Vintage Date (Forecast Date)')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Model Performance Over Time (Vintage Drift)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Increasing RMSE over time suggests concept drift\")\n",
    "print(\"- Stable RMSE suggests robust model\")\n",
    "print(\"- Models may perform differently in different time periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6cf7c",
   "metadata": {},
   "source": [
    "## 7. Forecast Horizon Analysis\n",
    "\n",
    "Examine how accuracy degrades with forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze forecast horizon performance\n",
    "horizon_analysis = backtest_results.analyze_forecast_horizon('rmse')\n",
    "\n",
    "print(\"\\nForecast Horizon Analysis:\")\n",
    "print(horizon_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize horizon degradation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for wflow_id in horizon_analysis['wflow_id'].unique()[:4]:\n",
    "    wflow_data = horizon_analysis[horizon_analysis['wflow_id'] == wflow_id]\n",
    "\n",
    "    axes[0].plot(wflow_data['horizon_months'], wflow_data['rmse'],\n",
    "                marker='o', label=wflow_id, linewidth=2)\n",
    "\n",
    "    axes[1].plot(wflow_data['horizon_months'], wflow_data['r_squared'],\n",
    "                marker='o', label=wflow_id, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Forecast Horizon (Months)')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE vs Forecast Horizon')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Forecast Horizon (Months)')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].set_title('R² vs Forecast Horizon')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTypical pattern: Accuracy decreases with longer horizons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01870d",
   "metadata": {},
   "source": [
    "## 8. Compare Vintage vs Final Data\n",
    "\n",
    "Demonstrate the importance of using vintage data for realistic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on FINAL data (data leakage!)\n",
    "wf_final = workflow().add_formula(formulas[0]).add_model(models[0])\n",
    "fit_final = wf_final.fit(final_data)\n",
    "\n",
    "# Evaluate on same test period as last CV split\n",
    "last_split = vintage_cv.splits[-1]\n",
    "test_dates_last = last_split.test_data()['date'].unique()\n",
    "\n",
    "final_test = final_data[final_data['date'].isin(test_dates_last)]\n",
    "preds_final = fit_final.predict(final_test)\n",
    "\n",
    "rmse_final = rmse(final_test['price'], preds_final['.pred']).iloc[0]['value']\n",
    "\n",
    "# Compare with vintage-based backtest\n",
    "rmse_vintage = ranked.iloc[0]['rmse_mean']  # Best vintage-based model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VINTAGE vs FINAL DATA COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRMSE with FINAL data (data leakage):  {rmse_final:.4f}\")\n",
    "print(f\"RMSE with VINTAGE data (realistic):   {rmse_vintage:.4f}\")\n",
    "print(f\"\\nDifference: {abs(rmse_vintage - rmse_final):.4f}\")\n",
    "print(f\"Vintage RMSE is {(rmse_vintage/rmse_final - 1)*100:+.1f}% vs final\")\n",
    "print(f\"\\nUsing final data gives OVERLY OPTIMISTIC estimates!\")\n",
    "print(f\"Vintage backtesting provides REALISTIC production performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5850df8",
   "metadata": {},
   "source": [
    "## 9. Production Forecasting Workflow\n",
    "\n",
    "Demonstrate the full production workflow with vintage-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best workflow from backtest\n",
    "best_wflow_id = ranked.iloc[0]['wflow_id']\n",
    "best_wf = wf_set.workflows[best_wflow_id]\n",
    "\n",
    "print(f\"Best workflow: {best_wflow_id}\")\n",
    "\n",
    "# Get latest available vintage for training\n",
    "latest_as_of = vintage_data['as_of_date'].max()\n",
    "latest_vintage = vintage_data[vintage_data['as_of_date'] == latest_as_of].copy()\n",
    "\n",
    "print(f\"\\nLatest vintage as-of: {latest_as_of.date()}\")\n",
    "print(f\"Training data: {len(latest_vintage)} observations\")\n",
    "print(f\"Date range: {latest_vintage['date'].min().date()} to {latest_vintage['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972de170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train production model on latest vintage\n",
    "prod_model = best_wf.fit(latest_vintage)\n",
    "\n",
    "print(\"\\nProduction model trained on latest vintage data!\")\n",
    "print(f\"Model: {prod_model.spec.model_type}\")\n",
    "print(f\"Engine: {prod_model.spec.engine}\")\n",
    "\n",
    "# Get coefficients\n",
    "outputs, coeffs, stats = prod_model.extract_outputs()\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coeffs[['variable', 'estimate', 'std_error', 'p_value']])\n",
    "\n",
    "print(\"\\nModel Statistics:\")\n",
    "print(stats[['split', 'rmse', 'mae', 'r_squared', 'n']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts for next 6 months\n",
    "last_date = latest_vintage['date'].max()\n",
    "forecast_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=6, freq='MS')\n",
    "\n",
    "# Create forecast data (would come from external source in production)\n",
    "# For demo, extrapolate trends\n",
    "last_usd = latest_vintage['usd_index'].iloc[-1]\n",
    "last_demand = latest_vintage['demand_index'].iloc[-1]\n",
    "\n",
    "forecast_data = pd.DataFrame({\n",
    "    'date': forecast_dates,\n",
    "    'usd_index': last_usd + np.cumsum(np.random.randn(6) * 2),\n",
    "    'demand_index': last_demand + np.cumsum(np.random.randn(6) * 1.5)\n",
    "})\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts = prod_model.predict(forecast_data)\n",
    "\n",
    "forecast_summary = pd.DataFrame({\n",
    "    'date': forecast_dates,\n",
    "    'predicted_price': forecasts['.pred'].values,\n",
    "    'usd_index': forecast_data['usd_index'].values,\n",
    "    'demand_index': forecast_data['demand_index'].values\n",
    "})\n",
    "\n",
    "print(\"\\nProduction Forecasts (Next 6 Months):\")\n",
    "print(forecast_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967af867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize production forecast\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Historical data (latest vintage)\n",
    "ax.plot(latest_vintage['date'], latest_vintage['price'],\n",
    "       linewidth=2, label='Historical (Latest Vintage)', color='blue')\n",
    "\n",
    "# Forecasts\n",
    "ax.plot(forecast_dates, forecast_summary['predicted_price'],\n",
    "       linewidth=2, linestyle='--', marker='o', markersize=8,\n",
    "       label='Forecast', color='red')\n",
    "\n",
    "# Add vertical line at forecast start\n",
    "ax.axvline(last_date, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title('Production Forecast with Vintage-Trained Model')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8940837",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Why Vintages Matter:**\n",
    "   - Real-world data gets revised (GDP, earnings, economic indicators)\n",
    "   - Training on \"final\" data = data leakage = overly optimistic evaluation\n",
    "   - Vintage data simulates production conditions accurately\n",
    "\n",
    "2. **VintageCV Features:**\n",
    "   - Point-in-time data selection (no future information)\n",
    "   - Respects data revision timelines\n",
    "   - Multiple vintage selection strategies\n",
    "   - Integration with WorkflowSet\n",
    "\n",
    "3. **Backtest Analysis:**\n",
    "   - Rank workflows by realistic performance\n",
    "   - Detect concept drift (performance over time)\n",
    "   - Analyze forecast horizon degradation\n",
    "   - Compare multiple models on same vintages\n",
    "\n",
    "4. **Production Workflow:**\n",
    "   - Train on latest available vintage\n",
    "   - Track model performance over time\n",
    "   - Retrain when drift detected\n",
    "   - Document vintage metadata\n",
    "\n",
    "5. **Best Practices:**\n",
    "   - Always use vintage data for time-series forecasting\n",
    "   - Validate vintage data structure\n",
    "   - Monitor drift over time\n",
    "   - Document as-of dates for reproducibility\n",
    "   - Test on realistic forecast horizons\n",
    "\n",
    "6. **When to Use:**\n",
    "   - Economic forecasting (GDP, employment, inflation)\n",
    "   - Financial modeling (earnings, revenue)\n",
    "   - Supply chain forecasting (inventory, demand)\n",
    "   - Any domain with data revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd53147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Vintage revision timeline\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "sample_dates = final_data['date'][::12][:3]\n",
    "for sample_date in sample_dates:\n",
    "    vintages = vintage_data[vintage_data['date'] == sample_date].sort_values('as_of_date')\n",
    "    ax1.plot(vintages['as_of_date'], vintages['price'],\n",
    "            marker='o', label=f'Obs: {sample_date.date()}', linewidth=2)\n",
    "ax1.set_xlabel('As-of Date')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_title('Data Revision Timeline (Multiple Vintages per Observation)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Workflow comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "top_workflows = ranked.head(4)\n",
    "ax2.barh(range(len(top_workflows)), top_workflows['rmse_mean'], xerr=top_workflows['rmse_std'])\n",
    "ax2.set_yticks(range(len(top_workflows)))\n",
    "ax2.set_yticklabels(top_workflows['wflow_id'])\n",
    "ax2.set_xlabel('RMSE (Mean ± Std)')\n",
    "ax2.set_title('Workflow Performance Comparison')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Vintage drift\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "for wflow_id in drift_analysis['wflow_id'].unique()[:3]:\n",
    "    wflow_data = drift_analysis[drift_analysis['wflow_id'] == wflow_id]\n",
    "    ax3.plot(wflow_data['vintage_date'], wflow_data['rmse'],\n",
    "            marker='o', label=wflow_id, linewidth=2)\n",
    "ax3.set_xlabel('Vintage Date')\n",
    "ax3.set_ylabel('RMSE')\n",
    "ax3.set_title('Model Drift Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# 4. Production forecast\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.plot(latest_vintage['date'].tail(24), latest_vintage['price'].tail(24),\n",
    "        linewidth=2, label='Historical', color='blue')\n",
    "ax4.plot(forecast_dates, forecast_summary['predicted_price'],\n",
    "        linewidth=2, linestyle='--', marker='o', markersize=8,\n",
    "        label='Forecast', color='red')\n",
    "ax4.axvline(last_date, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Price ($)')\n",
    "ax4.set_title('Production Forecast (Vintage-Trained Model)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.suptitle('Vintage Backtesting Summary', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Vintage backtesting provides realistic production performance estimates!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
