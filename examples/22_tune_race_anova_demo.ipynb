{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tuning: Racing with ANOVA (tune_race_anova)\n",
    "\n",
    "This notebook demonstrates **statistical racing** for hyperparameter optimization using repeated measures ANOVA.\n",
    "\n",
    "## Key Benefits:\n",
    "- **50-80% faster** than exhaustive grid search\n",
    "- **Statistically rigorous**: Uses repeated measures ANOVA to filter poor configs\n",
    "- **Early elimination**: Drops unpromising parameter combinations\n",
    "- **Efficient**: More evaluations on promising configs\n",
    "\n",
    "## Racing Algorithm:\n",
    "1. Evaluate all configs on first resample\n",
    "2. After each resample, run ANOVA test\n",
    "3. Eliminate configs significantly worse than best\n",
    "4. Continue with survivors only\n",
    "5. Stop when one winner or min resamples reached\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import rand_forest, boost_tree, linear_reg\n",
    "from py_rsample import vfold_cv\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_tune import (\n",
    "    tune, grid_regular, tune_grid,\n",
    "    tune_race_anova, control_race\n",
    ")\n",
    "\n",
    "print(\"\u2713 All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../_md/__data/preem.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.drop(columns=['date'])  # Drop date to avoid patsy categorical issues\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)[:5]}...\")\n",
    "\n",
    "# Display summary\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formula (exclude date from predictors)\n",
    "FORMULA = \"target ~ totaltar + mean_med_diesel_crack_input1_trade_month_lag2 + mean_nwe_hsfo_crack_trade_month_lag1\"\n",
    "\n",
    "print(f\"Formula: {FORMULA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Tuning with ANOVA Racing\n",
    "\n",
    "### 1.1 Setup: Create Large Grid (81 combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow with tunable Random Forest\n",
    "wf_rf = (\n",
    "    workflow()\n",
    "    .add_formula(FORMULA)\n",
    "    .add_model(\n",
    "        rand_forest(\n",
    "            mtry=tune(),        # Number of variables to sample\n",
    "            trees=tune(),       # Number of trees\n",
    "            min_n=tune()        # Minimum samples per leaf\n",
    "        ).set_mode(\"regression\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create large 3D grid\n",
    "rf_grid = grid_regular(\n",
    "    {\n",
    "        \"mtry\": {\"range\": (1, 3)},       # 1-3 variables\n",
    "        \"trees\": {\"range\": (50, 500)},   # Trees\n",
    "        \"min_n\": {\"range\": (2, 40)}      # Min samples\n",
    "    },\n",
    "    levels=5  # 5x5x5 = 125 combinations\n",
    ")\n",
    "\n",
    "print(f\"Grid size: {len(rf_grid)} parameter combinations\")\n",
    "print(f\"\\nFirst 5 configurations:\")\n",
    "print(rf_grid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation folds\n",
    "cv_folds = vfold_cv(df, v=10)\n",
    "\n",
    "print(f\"Created {len(cv_folds)} CV folds\")\n",
    "print(f\"Total evaluations without racing: {len(rf_grid)} configs \u00d7 {len(cv_folds)} folds = {len(rf_grid) * len(cv_folds)} model fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline: Standard Grid Search (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time standard grid search\n",
    "print(\"Running standard tune_grid (exhaustive search)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_results = tune_grid(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse, mae)\n",
    ")\n",
    "\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\u2713 Grid search complete in {grid_time:.1f} seconds\")\n",
    "print(f\"Evaluated: {len(rf_grid)} configs \u00d7 {len(cv_folds)} folds = {len(rf_grid) * len(cv_folds)} fits\")\n",
    "\n",
    "# Show best\n",
    "print(\"\\nTop 5 configurations:\")\n",
    "grid_results.show_best(metric=\"rmse\", n=5, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Racing with ANOVA\n",
    "\n",
    "Now let's use `tune_race_anova()` with the same grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure racing\n",
    "race_ctrl = control_race(\n",
    "    burn_in=3,          # Evaluate all configs on first 3 folds\n",
    "    num_ties=5,         # Keep top 5 if tied\n",
    "    alpha=0.05,         # Significance level for ANOVA\n",
    "    verbose_elim=True,       # Show progress\n",
    "    save_pred=False\n",
    ")\n",
    "\n",
    "print(\"Racing configuration:\")\n",
    "print(f\"  Burn-in: {race_ctrl.burn_in} folds (all configs evaluated)\")\n",
    "print(f\"  Significance: \u03b1 = {race_ctrl.alpha}\")\n",
    "print(f\"  Ties kept: {race_ctrl.num_ties}\")\n",
    "print(f\"\\nThis will eliminate poor configs early!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ANOVA racing\n",
    "print(\"Running tune_race_anova (efficient search)...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "race_results = tune_race_anova(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse, mae),\n",
    "    control=race_ctrl\n",
    ")\n",
    "\n",
    "race_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\u2713 Racing complete in {race_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare timing\n",
    "speedup = grid_time / race_time\n",
    "reduction_pct = (1 - race_time / grid_time) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nStandard grid search:  {grid_time:.1f} seconds\")\n",
    "print(f\"ANOVA racing:          {race_time:.1f} seconds\")\n",
    "print(f\"\\nSpeedup:               {speedup:.2f}x faster\")\n",
    "print(f\"Time reduction:        {reduction_pct:.1f}%\")\n",
    "\n",
    "# Count actual evaluations in racing\n",
    "race_metrics = race_results.metrics\n",
    "n_race_evals = len(race_metrics[race_metrics['metric'] == 'rmse'])\n",
    "n_grid_evals = len(grid_results.metrics[grid_results.metrics['metric'] == 'rmse'])\n",
    "\n",
    "print(f\"\\nModel fits (grid):     {n_grid_evals}\")\n",
    "print(f\"Model fits (racing):   {n_race_evals}\")\n",
    "print(f\"Evaluation reduction:  {(1 - n_race_evals/n_grid_evals)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best results\n",
    "grid_best = grid_results.select_best(metric=\"rmse\", maximize=False)\n",
    "race_best = race_results.select_best(metric=\"rmse\", maximize=False)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(\"\\nGrid search:\")\n",
    "for param, value in grid_best.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nRacing:\")\n",
    "for param, value in race_best.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Check if same winner\n",
    "if grid_best == race_best:\n",
    "    print(\"\\n\u2713 Both methods found the SAME best configuration!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0 Different winners (both should be close in performance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 from racing\n",
    "print(\"Top 10 configurations from racing:\")\n",
    "race_results.show_best(metric=\"rmse\", n=10, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Tuning with ANOVA Racing\n",
    "\n",
    "Let's test racing on a different model type with continuous parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost workflow\n",
    "wf_xgb = (\n",
    "    workflow()\n",
    "    .add_formula(FORMULA)\n",
    "    .add_model(\n",
    "        boost_tree(\n",
    "            trees=tune(),\n",
    "            tree_depth=tune(),\n",
    "            learn_rate=tune(),\n",
    "            min_n=tune()\n",
    "        ).set_mode(\"regression\").set_engine(\"xgboost\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4D grid\n",
    "xgb_grid = grid_regular(\n",
    "    {\n",
    "        \"trees\": {\"range\": (50, 300)},\n",
    "        \"tree_depth\": {\"range\": (3, 10)},\n",
    "        \"learn_rate\": {\"range\": (0.001, 0.3), \"trans\": \"log\"},\n",
    "        \"min_n\": {\"range\": (2, 40)}\n",
    "    },\n",
    "    levels=4  # 4^4 = 256 combinations\n",
    ")\n",
    "\n",
    "print(f\"XGBoost grid: {len(xgb_grid)} combinations\")\n",
    "print(f\"Full grid search would require {len(xgb_grid) * len(cv_folds)} model fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run racing on XGBoost\n",
    "print(\"Running tune_race_anova on XGBoost...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_race_results = tune_race_anova(\n",
    "    wf_xgb,\n",
    "    resamples=cv_folds,\n",
    "    grid=xgb_grid,\n",
    "    metrics=metric_set(rmse, mae, r_squared),\n",
    "    control=race_ctrl\n",
    ")\n",
    "\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\u2713 XGBoost racing complete in {xgb_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best XGBoost configurations\n",
    "print(\"Top 5 XGBoost configurations:\")\n",
    "xgb_race_results.show_best(metric=\"rmse\", n=5, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count evaluations\n",
    "xgb_evals = len(xgb_race_results.metrics[xgb_race_results.metrics['metric'] == 'rmse'])\n",
    "xgb_full = len(xgb_grid) * len(cv_folds)\n",
    "\n",
    "print(f\"\\nXGBoost evaluations:\")\n",
    "print(f\"  Racing: {xgb_evals} fits\")\n",
    "print(f\"  Full grid: {xgb_full} fits\")\n",
    "print(f\"  Saved: {xgb_full - xgb_evals} fits ({(1 - xgb_evals/xgb_full)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Controlling Racing Behavior\n",
    "\n",
    "### 3.1 Aggressive Racing (eliminate quickly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggressive racing: higher alpha, fewer ties\n",
    "aggressive_ctrl = control_race(\n",
    "    burn_in=2,          # Start eliminating after 2 folds\n",
    "    num_ties=3,         # Keep only top 3\n",
    "    alpha=0.10,         # Less stringent (eliminate more)\n",
    "    verbose_elim=True\n",
    ")\n",
    "\n",
    "print(\"Running aggressive racing...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "aggressive_results = tune_race_anova(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse),\n",
    "    control=aggressive_ctrl\n",
    ")\n",
    "\n",
    "aggressive_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Aggressive racing: {aggressive_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Conservative Racing (keep more candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative racing: lower alpha, more ties\n",
    "conservative_ctrl = control_race(\n",
    "    burn_in=4,          # Longer burn-in\n",
    "    num_ties=10,        # Keep top 10\n",
    "    alpha=0.01,         # Very stringent (keep more)\n",
    "    verbose_elim=True\n",
    ")\n",
    "\n",
    "print(\"Running conservative racing...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "conservative_results = tune_race_anova(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse),\n",
    "    control=conservative_ctrl\n",
    ")\n",
    "\n",
    "conservative_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Conservative racing: {conservative_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare racing strategies\n",
    "strategies = {\n",
    "    'Standard': race_time,\n",
    "    'Aggressive': aggressive_time,\n",
    "    'Conservative': conservative_time\n",
    "}\n",
    "\n",
    "print(\"Racing strategy comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for name, t in strategies.items():\n",
    "    print(f\"{name:15s}: {t:6.1f} seconds ({speedup * grid_time / t:.2f}x speedup)\")\n",
    "\n",
    "print(\"\\n\u2713 More aggressive = faster but might miss good configs\")\n",
    "print(\"\u2713 More conservative = slower but safer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Best Practices\n",
    "\n",
    "### When to use ANOVA racing:\n",
    "- \u2713 **Large parameter grids** (50+ configurations)\n",
    "- \u2713 **Many resamples** (5-10 CV folds)\n",
    "- \u2713 **Expensive models** (slow to train)\n",
    "- \u2713 **Clear winners** (some configs much better)\n",
    "\n",
    "### Configuration guidelines:\n",
    "- **burn_in**: 2-4 folds (need baseline for ANOVA)\n",
    "- **alpha**: 0.05 standard, 0.10 aggressive, 0.01 conservative\n",
    "- **num_ties**: 5-10 (keep enough for robust selection)\n",
    "\n",
    "### Expected speedup:\n",
    "- **Typical**: 2-5x faster than grid search\n",
    "- **Best case**: 10x faster with clear winner\n",
    "- **Worst case**: Similar to grid if all configs equally good\n",
    "\n",
    "### Trade-offs:\n",
    "- \u2713 Much faster than exhaustive search\n",
    "- \u2713 Statistically principled elimination\n",
    "- \u26a0 Might eliminate close competitors early\n",
    "- \u26a0 Requires multiple resamples (doesn't help with single holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY: tune_race_anova()\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: {df.shape[0]} observations, {df.shape[1]} features\")\n",
    "print(f\"Cross-validation: {len(cv_folds)} folds\")\n",
    "print(f\"\\nRandom Forest tuning:\")\n",
    "print(f\"  Grid size: {len(rf_grid)} configurations\")\n",
    "print(f\"  Grid search: {grid_time:.1f}s ({len(rf_grid) * len(cv_folds)} fits)\")\n",
    "print(f\"  Racing: {race_time:.1f}s ({n_race_evals} fits)\")\n",
    "print(f\"  Speedup: {speedup:.2f}x\")\n",
    "print(f\"\\nXGBoost tuning:\")\n",
    "print(f\"  Grid size: {len(xgb_grid)} configurations\")\n",
    "print(f\"  Racing: {xgb_time:.1f}s ({xgb_evals} fits)\")\n",
    "print(f\"  Saved: {xgb_full - xgb_evals} evaluations\")\n",
    "print(f\"\\n\u2713 Racing provides significant speedup with minimal accuracy loss\")\n",
    "print(\"\u2713 Perfect for initial hyperparameter screening\")\n",
    "print(\"\u2713 Combine with Bayesian optimization for best results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}