{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-tune: Hyperparameter Tuning Demo\n",
    "\n",
    "This notebook demonstrates the hyperparameter tuning capabilities in py-tidymodels using py-tune.\n",
    "\n",
    "## Key Features:\n",
    "- `tune()` - Mark parameters for tuning\n",
    "- `tune_grid()` - Grid search with cross-validation\n",
    "- `fit_resamples()` - Evaluate without tuning\n",
    "- `grid_regular()` / `grid_random()` - Parameter grid generation\n",
    "- `TuneResults` - Result analysis and selection\n",
    "- `finalize_workflow()` - Finalize with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_parsnip import linear_reg\n",
    "from py_workflows import workflow\n",
    "from py_rsample import vfold_cv\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_tune import (\n",
    "    tune,\n",
    "    tune_grid,\n",
    "    fit_resamples,\n",
    "    grid_regular,\n",
    "    grid_random,\n",
    "    finalize_workflow\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Data\n",
    "\n",
    "Generate synthetic data with a non-linear relationship that benefits from regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "n = 200\n",
    "X = np.random.uniform(-3, 3, (n, 5))\n",
    "# True relationship with some noise\n",
    "y = 2 * X[:, 0] + 1.5 * X[:, 1]**2 - X[:, 2] + 0.5 * X[:, 3] + np.random.normal(0, 1, n)\n",
    "\n",
    "data = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(5)])\n",
    "data['y'] = y\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "\n",
    "# Split into train and test\n",
    "train_data = data.iloc[:150]\n",
    "test_data = data.iloc[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Specification with Tunable Parameters\n",
    "\n",
    "Use `tune()` to mark parameters that should be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model specification with tunable parameters\n",
    "lr_spec = linear_reg(\n",
    "    penalty=tune('penalty'),  # L2 regularization strength\n",
    "    mixture=tune('mixture')   # Elastic net mixing (0=ridge, 1=lasso)\n",
    ")\n",
    "\n",
    "print(\"Model specification with tunable parameters:\")\n",
    "print(lr_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Workflow and Resamples\n",
    "\n",
    "Set up the workflow and cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "wf = workflow().add_formula(\"y ~ x1 + x2 + x3 + x4 + x5\").add_model(lr_spec)\n",
    "\n",
    "# Create cross-validation folds\n",
    "folds = vfold_cv(train_data, v=5)\n",
    "\n",
    "print(f\"Created workflow with {len(folds)} cross-validation folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Parameter Grid\n",
    "\n",
    "Create a regular grid of parameter values to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space\n",
    "param_info = {\n",
    "    'penalty': {'range': (0.001, 10.0), 'trans': 'log'},  # Log-spaced\n",
    "    'mixture': {'range': (0, 1)}  # Linear spacing\n",
    "}\n",
    "\n",
    "# Create regular grid\n",
    "param_grid = grid_regular(param_info, levels=4)\n",
    "\n",
    "print(f\"Parameter grid with {len(param_grid)} combinations:\\n\")\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Grid Search\n",
    "\n",
    "Use `tune_grid()` to evaluate all parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to compute\n",
    "metrics = metric_set(rmse, mae, r_squared)\n",
    "\n",
    "# Run grid search\n",
    "print(\"Running grid search... (this may take a minute)\")\n",
    "tune_results = tune_grid(\n",
    "    wf,\n",
    "    folds,\n",
    "    grid=param_grid,\n",
    "    metrics=metrics,\n",
    "    control={'save_pred': False}\n",
    ")\n",
    "\n",
    "print(\"\\nGrid search complete!\")\n",
    "print(f\"Evaluated {len(tune_results.grid)} configurations across {len(folds)} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results\n",
    "\n",
    "Collect and visualize tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all metrics\n",
    "all_metrics = tune_results.collect_metrics()\n",
    "\n",
    "print(\"Sample of tuning results (long format):\\n\")\n",
    "print(all_metrics.head(15))\n",
    "\n",
    "print(\"\\nColumns in all_metrics:\", list(all_metrics.columns))\n",
    "\n",
    "# The data is in long format with columns: metric, value, .resample, .config\n",
    "# We can pivot to wide format if needed for visualization\n",
    "metrics_wide = all_metrics.pivot(\n",
    "    index=[\".config\", \".resample\"],\n",
    "    columns=\"metric\",\n",
    "    values=\"value\"\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nWide format (pivot by metric):\")\n",
    "print(metrics_wide.head(10))\n",
    "\n",
    "# Calculate summary statistics for each config\n",
    "print(\"\\nMean and std for each metric by config:\")\n",
    "metrics_summary = all_metrics.groupby(['.config', 'metric'])['value'].agg(['mean', 'std']).reset_index()\n",
    "print(metrics_summary.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Show Best Results\n",
    "\n",
    "Identify top performing parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 configurations by RMSE\n",
    "best_configs = tune_results.show_best('rmse', n=5, maximize=False)\n",
    "\n",
    "print(\"Top 5 configurations by RMSE:\\n\")\n",
    "print(best_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Select Best Parameters\n",
    "\n",
    "Choose the optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best parameters (absolute best)\n",
    "best_params = tune_results.select_best('rmse', maximize=False)\n",
    "\n",
    "print(\"Best parameters (lowest RMSE):\\n\")\n",
    "print(best_params)\n",
    "\n",
    "# Select using one-standard-error rule (simpler model)\n",
    "simple_params = tune_results.select_by_one_std_err('rmse', maximize=False)\n",
    "\n",
    "print(\"\\nSimpler model (1-SE rule):\\n\")\n",
    "print(simple_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Tuning Results\n",
    "\n",
    "Plot performance across parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean RMSE for each config\n",
    "rmse_summary = all_metrics[all_metrics['metric'] == 'rmse'].groupby('.config').agg({\n",
    "    'value': ['mean', 'std']\n",
    "}).reset_index()\n",
    "rmse_summary.columns = ['.config', 'mean_rmse', 'std_rmse']\n",
    "\n",
    "# Merge with parameter values\n",
    "rmse_with_params = rmse_summary.merge(param_grid, on='.config')\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: RMSE vs penalty for different mixture values\n",
    "for mixture_val in sorted(rmse_with_params['mixture'].unique()):\n",
    "    subset = rmse_with_params[rmse_with_params['mixture'] == mixture_val]\n",
    "    axes[0].plot(subset['penalty'], subset['mean_rmse'], 'o-', \n",
    "                 label=f'mixture={mixture_val:.2f}', markersize=8)\n",
    "\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Penalty (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Mean RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE vs Penalty by Mixture', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Heatmap of RMSE\n",
    "pivot_data = rmse_with_params.pivot(index='mixture', columns='penalty', values='mean_rmse')\n",
    "im = axes[1].imshow(pivot_data, cmap='viridis', aspect='auto')\n",
    "axes[1].set_xticks(range(len(pivot_data.columns)))\n",
    "axes[1].set_xticklabels([f'{x:.3f}' for x in pivot_data.columns], rotation=45)\n",
    "axes[1].set_yticks(range(len(pivot_data.index)))\n",
    "axes[1].set_yticklabels([f'{y:.2f}' for y in pivot_data.index])\n",
    "axes[1].set_xlabel('Penalty', fontsize=12)\n",
    "axes[1].set_ylabel('Mixture', fontsize=12)\n",
    "axes[1].set_title('RMSE Heatmap', fontsize=14)\n",
    "plt.colorbar(im, ax=axes[1], label='Mean RMSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest performance achieved at:\")\n",
    "best_row = rmse_with_params.loc[rmse_with_params['mean_rmse'].idxmin()]\n",
    "print(f\"Penalty: {best_row['penalty']:.4f}\")\n",
    "print(f\"Mixture: {best_row['mixture']:.2f}\")\n",
    "print(f\"Mean RMSE: {best_row['mean_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Finalize and Fit Best Model\n",
    "\n",
    "Create final workflow with optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize workflow with best parameters\n",
    "final_wf = finalize_workflow(wf, best_params)\n",
    "\n",
    "# Fit on full training data\n",
    "final_fit = final_wf.fit(train_data)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = final_fit.predict(test_data)\n",
    "\n",
    "print(\"Final model trained with best parameters!\")\n",
    "print(f\"\\nBest parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Final Model\n",
    "\n",
    "Assess performance on held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "test_metrics = metrics(test_data['y'], test_predictions['.pred'])\n",
    "\n",
    "print(\"Test set performance:\\n\")\n",
    "print(test_metrics)\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_data['y'], test_predictions['.pred'], alpha=0.6, s=50)\n",
    "plt.plot([test_data['y'].min(), test_data['y'].max()],\n",
    "         [test_data['y'].min(), test_data['y'].max()],\n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual', fontsize=12)\n",
    "plt.ylabel('Predicted', fontsize=12)\n",
    "plt.title('Final Model: Predictions vs Actuals (Test Set)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = test_data['y'].values - test_predictions['.pred'].values\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_predictions['.pred'], residuals, alpha=0.6, s=50)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "plt.title('Residual Plot', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Random Grid Search\n",
    "\n",
    "Demonstrate random parameter sampling for larger search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random grid\n",
    "random_grid = grid_random(param_info, size=20, seed=42)\n",
    "\n",
    "print(f\"Random grid with {len(random_grid)} combinations:\\n\")\n",
    "print(random_grid.head(10))\n",
    "\n",
    "# Run random grid search (smaller sample for demo)\n",
    "print(\"\\nRunning random grid search...\")\n",
    "random_results = tune_grid(\n",
    "    wf,\n",
    "    folds,\n",
    "    grid=random_grid,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Compare best result\n",
    "random_best = random_results.select_best('rmse', maximize=False)\n",
    "print(\"\\nBest from random search:\")\n",
    "print(random_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Baseline: fit_resamples()\n",
    "\n",
    "Evaluate a fixed model without tuning for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline workflow with fixed parameters\n",
    "baseline_spec = linear_reg(penalty=0.1, mixture=0.5)\n",
    "baseline_wf = workflow().add_formula(\"y ~ x1 + x2 + x3 + x4 + x5\").add_model(baseline_spec)\n",
    "\n",
    "# Evaluate without tuning\n",
    "print(\"Evaluating baseline model...\")\n",
    "baseline_results = fit_resamples(\n",
    "    baseline_wf,\n",
    "    folds,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Collect metrics\n",
    "baseline_metrics = baseline_results.collect_metrics()\n",
    "\n",
    "print(\"\\nBaseline model performance (penalty=0.1, mixture=0.5):\\n\")\n",
    "print(baseline_metrics.groupby('metric')['value'].agg(['mean', 'std']))\n",
    "\n",
    "# Compare with tuned model\n",
    "tuned_rmse = tune_results.show_best('rmse', n=1, maximize=False)['mean'].iloc[0]\n",
    "baseline_rmse = baseline_metrics[baseline_metrics['metric'] == 'rmse']['value'].mean()\n",
    "\n",
    "improvement = (baseline_rmse - tuned_rmse) / baseline_rmse * 100\n",
    "print(f\"\\nTuning improved RMSE by {improvement:.2f}%\")\n",
    "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"Tuned RMSE: {tuned_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo covered:\n",
    "\n",
    "1. **Marking parameters for tuning** with `tune()`\n",
    "2. **Grid generation** with `grid_regular()` and `grid_random()`\n",
    "3. **Grid search** with `tune_grid()` and cross-validation\n",
    "4. **Result analysis** with `TuneResults` methods:\n",
    "   - `collect_metrics()` - Get all metrics\n",
    "   - `show_best()` - Show top configurations\n",
    "   - `select_best()` - Select absolute best\n",
    "   - `select_by_one_std_err()` - Select simpler model\n",
    "5. **Model finalization** with `finalize_workflow()`\n",
    "6. **Baseline evaluation** with `fit_resamples()`\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "- Hyperparameter tuning can significantly improve model performance\n",
    "- Cross-validation provides robust estimates of generalization\n",
    "- The one-standard-error rule helps avoid overfitting\n",
    "- Random grids are efficient for exploring large parameter spaces\n",
    "- Always compare tuned models against simple baselines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
