{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Regression Models in py-tidymodels\n",
    "\n",
    "This notebook demonstrates the comprehensive sklearn regression model integration in py-tidymodels, showcasing five powerful algorithms:\n",
    "\n",
    "1. **decision_tree()** - Tree-based recursive partitioning\n",
    "2. **nearest_neighbor()** - Instance-based learning\n",
    "3. **svm_rbf()** - Support Vector Machines with RBF kernel\n",
    "4. **svm_linear()** - Support Vector Machines with linear kernel\n",
    "5. **mlp()** - Multi-layer Perceptron neural networks\n",
    "\n",
    "Each model follows the tidymodels philosophy: specify → fit → predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tidymodels.specify import decision_tree, nearest_neighbor, svm_rbf, svm_linear, mlp\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree Regression\n",
    "\n",
    "Decision trees recursively partition the feature space using binary splits. They're interpretable and can capture non-linear patterns, but are prone to overfitting.\n",
    "\n",
    "### Key Parameters:\n",
    "- `tree_depth`: Maximum depth of the tree (controls complexity)\n",
    "- `min_n`: Minimum samples required to split a node\n",
    "- `cost_complexity`: Pruning parameter (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "np.random.seed(42)\n",
    "X_tree = np.sort(np.random.rand(200, 1) * 10, axis=0)\n",
    "y_tree = np.sin(X_tree).ravel() + np.random.randn(200) * 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tree, y_tree, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame({'X': X_train.ravel(), 'y': y_train})\n",
    "test_df = pd.DataFrame({'X': X_test.ravel(), 'y': y_test})\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Depth Tuning\n",
    "\n",
    "Let's compare different tree depths to understand the bias-variance tradeoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different tree depths\n",
    "depths = [2, 5, 10, None]  # None means unlimited depth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "plot_df = pd.DataFrame({'X': X_plot.ravel()})\n",
    "\n",
    "for idx, depth in enumerate(depths):\n",
    "    # Specify and fit model\n",
    "    model = decision_tree(mode='regression', tree_depth=depth, min_n=5)\n",
    "    fitted = model.fit(train_df, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted.predict(plot_df)\n",
    "    test_preds = fitted.predict(test_df)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_df['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_df['y'], test_preds['predictions'])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_df['X'], train_df['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.scatter(test_df['X'], test_df['y'], alpha=0.3, label='Test', s=20, color='orange')\n",
    "    ax.plot(X_plot, predictions['predictions'], 'r-', linewidth=2, label='Predictions')\n",
    "    \n",
    "    depth_label = 'Unlimited' if depth is None else depth\n",
    "    ax.set_title(f'Tree Depth: {depth_label}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Shallow trees (depth=2): High bias, underfit the data\")\n",
    "print(\"- Medium trees (depth=5): Good balance\")\n",
    "print(\"- Deep trees (depth=10+): Low bias but high variance, overfit training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Decision trees can tell us which features are most important for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-feature dataset\n",
    "X_multi, y_multi = make_regression(\n",
    "    n_samples=300, \n",
    "    n_features=8, \n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    noise=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "feature_names = [f'Feature_{i+1}' for i in range(8)]\n",
    "df_multi = pd.DataFrame(X_multi, columns=feature_names)\n",
    "df_multi['target'] = y_multi\n",
    "\n",
    "# Fit decision tree\n",
    "model = decision_tree(mode='regression', tree_depth=8, min_n=10)\n",
    "fitted = model.fit(df_multi, 'target')\n",
    "\n",
    "# Extract feature importance\n",
    "importance = fitted.model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Rankings:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Example\n",
    "\n",
    "Let's demonstrate how overfitting manifests with unlimited tree depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different complexities\n",
    "depths_range = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths_range:\n",
    "    model = decision_tree(mode='regression', tree_depth=depth, min_n=2)\n",
    "    fitted = model.fit(train_df, 'y')\n",
    "    \n",
    "    train_pred = fitted.predict(train_df)\n",
    "    test_pred = fitted.predict(test_df)\n",
    "    \n",
    "    train_scores.append(r2_score(train_df['y'], train_pred['predictions']))\n",
    "    test_scores.append(r2_score(test_df['y'], test_pred['predictions']))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(depths_range, train_scores, 'o-', label='Training R²', linewidth=2)\n",
    "plt.plot(depths_range, test_scores, 's-', label='Test R²', linewidth=2)\n",
    "plt.axvline(x=5, color='red', linestyle='--', alpha=0.5, label='Optimal Depth')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Decision Tree: Training vs Test Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"The gap between training and test scores indicates overfitting.\")\n",
    "print(f\"Optimal depth appears to be around {depths_range[np.argmax(test_scores)]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest Neighbor Regression (k-NN)\n",
    "\n",
    "k-NN is a non-parametric method that predicts based on the k closest training examples.\n",
    "\n",
    "### Key Parameters:\n",
    "- `neighbors`: Number of neighbors to consider\n",
    "- `dist_power`: Distance metric power (1=Manhattan, 2=Euclidean)\n",
    "- `weight_func`: How to weight neighbors ('uniform' or 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate smooth non-linear data\n",
    "np.random.seed(42)\n",
    "X_knn = np.sort(np.random.rand(150, 1) * 10, axis=0)\n",
    "y_knn = 2 * np.sin(X_knn).ravel() + 0.5 * X_knn.ravel() + np.random.randn(150) * 0.5\n",
    "\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
    "    X_knn, y_knn, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "train_knn = pd.DataFrame({'X': X_train_knn.ravel(), 'y': y_train_knn})\n",
    "test_knn = pd.DataFrame({'X': X_test_knn.ravel(), 'y': y_test_knn})\n",
    "\n",
    "print(\"k-NN dataset created with 150 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k Selection\n",
    "\n",
    "The number of neighbors is crucial - too few leads to overfitting, too many leads to undersmoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different k values\n",
    "k_values = [1, 3, 10, 30]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "plot_knn = pd.DataFrame({'X': X_plot.ravel()})\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    # Specify and fit model\n",
    "    model = nearest_neighbor(mode='regression', neighbors=k)\n",
    "    fitted = model.fit(train_knn, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted.predict(plot_knn)\n",
    "    test_preds = fitted.predict(test_knn)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_knn['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_knn['y'], test_preds['predictions'])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_knn['X'], train_knn['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.scatter(test_knn['X'], test_knn['y'], alpha=0.3, label='Test', s=20, color='orange')\n",
    "    ax.plot(X_plot, predictions['predictions'], 'g-', linewidth=2, label='Predictions')\n",
    "    \n",
    "    ax.set_title(f'k = {k}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- k=1: Very jagged, overfits to training noise\")\n",
    "print(\"- k=3-10: Smooth predictions, captures underlying pattern\")\n",
    "print(\"- k=30: Over-smoothed, misses local variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics\n",
    "\n",
    "The `dist_power` parameter controls the distance metric:\n",
    "- `dist_power=1`: Manhattan distance (L1)\n",
    "- `dist_power=2`: Euclidean distance (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distance metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (power, name) in enumerate([(1, 'Manhattan (L1)'), (2, 'Euclidean (L2)')]):\n",
    "    model = nearest_neighbor(mode='regression', neighbors=5, dist_power=power)\n",
    "    fitted = model.fit(train_knn, 'y')\n",
    "    \n",
    "    predictions = fitted.predict(plot_knn)\n",
    "    test_preds = fitted.predict(test_knn)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(test_knn['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_knn['y'], test_preds['predictions'])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_knn['X'], train_knn['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.plot(X_plot, predictions['predictions'], 'g-', linewidth=2, label='Predictions')\n",
    "    ax.set_title(f'{name}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIn 1D, Manhattan and Euclidean distances give similar results.\")\n",
    "print(\"Differences become more pronounced in higher dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Functions\n",
    "\n",
    "Weighting determines how much influence each neighbor has:\n",
    "- `uniform`: All neighbors weighted equally\n",
    "- `distance`: Closer neighbors have more influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighting functions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, weight in enumerate(['uniform', 'distance']):\n",
    "    model = nearest_neighbor(mode='regression', neighbors=10, weight_func=weight)\n",
    "    fitted = model.fit(train_knn, 'y')\n",
    "    \n",
    "    predictions = fitted.predict(plot_knn)\n",
    "    test_preds = fitted.predict(test_knn)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(test_knn['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_knn['y'], test_preds['predictions'])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_knn['X'], train_knn['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.plot(X_plot, predictions['predictions'], 'g-', linewidth=2, label='Predictions')\n",
    "    ax.set_title(f'Weight: {weight}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistance weighting generally provides smoother predictions.\")\n",
    "print(\"Closer neighbors have more influence, making the model more adaptive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machine with RBF Kernel\n",
    "\n",
    "SVM with RBF (Radial Basis Function) kernel can model complex non-linear relationships by implicitly mapping data to high-dimensional space.\n",
    "\n",
    "### Key Parameters:\n",
    "- `cost`: Penalty for misclassification (C parameter)\n",
    "- `rbf_sigma`: Kernel coefficient (gamma), controls smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex non-linear data\n",
    "np.random.seed(42)\n",
    "X_svm = np.sort(np.random.rand(200, 1) * 10, axis=0)\n",
    "y_svm = (np.sin(X_svm) * X_svm).ravel() + np.random.randn(200) * 1.5\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X_svm, y_svm, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "train_svm = pd.DataFrame({'X': X_train_svm.ravel(), 'y': y_train_svm})\n",
    "test_svm = pd.DataFrame({'X': X_test_svm.ravel(), 'y': y_test_svm})\n",
    "\n",
    "print(\"SVM dataset created with complex non-linear pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Sigma Tuning\n",
    "\n",
    "These parameters control the bias-variance tradeoff:\n",
    "- **cost**: Higher values fit training data more closely (risk overfitting)\n",
    "- **rbf_sigma**: Higher values create more flexible decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different cost values\n",
    "cost_values = [0.1, 1.0, 10.0, 100.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "plot_svm = pd.DataFrame({'X': X_plot.ravel()})\n",
    "\n",
    "for idx, cost in enumerate(cost_values):\n",
    "    # Specify and fit model (using default rbf_sigma)\n",
    "    model = svm_rbf(mode='regression', cost=cost)\n",
    "    fitted = model.fit(train_svm, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted.predict(plot_svm)\n",
    "    test_preds = fitted.predict(test_svm)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_svm['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_svm['y'], test_preds['predictions'])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_svm['X'], train_svm['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.scatter(test_svm['X'], test_svm['y'], alpha=0.3, label='Test', s=20, color='orange')\n",
    "    ax.plot(X_plot, predictions['predictions'], 'purple', linewidth=2, label='Predictions')\n",
    "    \n",
    "    ax.set_title(f'Cost = {cost}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCost parameter effects:\")\n",
    "print(\"- Low cost (0.1): More regularization, smoother fit\")\n",
    "print(\"- High cost (100): Less regularization, fits training data more closely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sigma values\n",
    "sigma_values = [0.01, 0.1, 0.5, 2.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, sigma in enumerate(sigma_values):\n",
    "    # Specify and fit model\n",
    "    model = svm_rbf(mode='regression', cost=1.0, rbf_sigma=sigma)\n",
    "    fitted = model.fit(train_svm, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted.predict(plot_svm)\n",
    "    test_preds = fitted.predict(test_svm)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_svm['y'], test_preds['predictions']))\n",
    "    r2 = r2_score(test_svm['y'], test_preds['predictions'])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(train_svm['X'], train_svm['y'], alpha=0.3, label='Train', s=20)\n",
    "    ax.plot(X_plot, predictions['predictions'], 'purple', linewidth=2, label='Predictions')\n",
    "    \n",
    "    ax.set_title(f'RBF Sigma = {sigma}\\nRMSE: {rmse:.3f}, R²: {r2:.3f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRBF Sigma effects:\")\n",
    "print(\"- Low sigma (0.01): Wide influence, smoother predictions\")\n",
    "print(\"- High sigma (2.0): Narrow influence, more local fitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Pattern Modeling\n",
    "\n",
    "SVM-RBF excels at capturing complex non-linear relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with multiple non-linear patterns\n",
    "X_complex, y_complex = make_friedman1(n_samples=300, n_features=5, noise=1.0, random_state=42)\n",
    "df_complex = pd.DataFrame(X_complex, columns=[f'X{i+1}' for i in range(5)])\n",
    "df_complex['y'] = y_complex\n",
    "\n",
    "train_complex, test_complex = train_test_split(df_complex, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit SVM-RBF\n",
    "model_rbf = svm_rbf(mode='regression', cost=10.0, rbf_sigma=0.1)\n",
    "fitted_rbf = model_rbf.fit(train_complex, 'y')\n",
    "\n",
    "# Predict\n",
    "train_pred = fitted_rbf.predict(train_complex)\n",
    "test_pred = fitted_rbf.predict(test_complex)\n",
    "\n",
    "# Evaluate\n",
    "train_rmse = np.sqrt(mean_squared_error(train_complex['y'], train_pred['predictions']))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_complex['y'], test_pred['predictions']))\n",
    "train_r2 = r2_score(train_complex['y'], train_pred['predictions'])\n",
    "test_r2 = r2_score(test_complex['y'], test_pred['predictions'])\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training\n",
    "axes[0].scatter(train_complex['y'], train_pred['predictions'], alpha=0.5)\n",
    "axes[0].plot([train_complex['y'].min(), train_complex['y'].max()], \n",
    "             [train_complex['y'].min(), train_complex['y'].max()], \n",
    "             'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Training Set\\nRMSE: {train_rmse:.3f}, R²: {train_r2:.3f}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test\n",
    "axes[1].scatter(test_complex['y'], test_pred['predictions'], alpha=0.5, color='orange')\n",
    "axes[1].plot([test_complex['y'].min(), test_complex['y'].max()], \n",
    "             [test_complex['y'].min(), test_complex['y'].max()], \n",
    "             'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title(f'Test Set\\nRMSE: {test_rmse:.3f}, R²: {test_r2:.3f}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSVM-RBF effectively captures complex non-linear relationships.\")\n",
    "print(f\"Test R² of {test_r2:.3f} demonstrates strong generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Visualization Concept\n",
    "\n",
    "The RBF kernel implicitly maps data to infinite-dimensional space. We can visualize the effect on a 2D problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D circular pattern\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "theta = np.random.rand(n_samples) * 2 * np.pi\n",
    "r = np.random.rand(n_samples) * 5\n",
    "\n",
    "X_circle = np.column_stack([r * np.cos(theta), r * np.sin(theta)])\n",
    "y_circle = r**2 + np.random.randn(n_samples) * 2\n",
    "\n",
    "df_circle = pd.DataFrame(X_circle, columns=['X1', 'X2'])\n",
    "df_circle['y'] = y_circle\n",
    "\n",
    "# Fit SVM-RBF\n",
    "model_circle = svm_rbf(mode='regression', cost=10.0, rbf_sigma=0.2)\n",
    "fitted_circle = model_circle.fit(df_circle, 'y')\n",
    "\n",
    "# Create prediction grid\n",
    "x1_range = np.linspace(X_circle[:, 0].min() - 1, X_circle[:, 0].max() + 1, 50)\n",
    "x2_range = np.linspace(X_circle[:, 1].min() - 1, X_circle[:, 1].max() + 1, 50)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "grid_df = pd.DataFrame({\n",
    "    'X1': X1_grid.ravel(),\n",
    "    'X2': X2_grid.ravel()\n",
    "})\n",
    "\n",
    "# Predict on grid\n",
    "grid_pred = fitted_circle.predict(grid_df)\n",
    "Z = grid_pred['predictions'].values.reshape(X1_grid.shape)\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(X_circle[:, 0], X_circle[:, 1], y_circle, c=y_circle, cmap='viridis', alpha=0.6)\n",
    "ax1.plot_surface(X1_grid, X2_grid, Z, alpha=0.3, cmap='coolwarm')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_zlabel('y')\n",
    "ax1.set_title('SVM-RBF: 3D View')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contourf(X1_grid, X2_grid, Z, levels=20, cmap='coolwarm', alpha=0.7)\n",
    "scatter = ax2.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle, cmap='viridis', \n",
    "                     edgecolor='black', linewidth=0.5, s=50)\n",
    "plt.colorbar(contour, ax=ax2, label='Predicted y')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_title('SVM-RBF: Contour Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe RBF kernel creates smooth, non-linear decision surfaces.\")\n",
    "print(\"It effectively handles circular/radial patterns in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine with Linear Kernel\n",
    "\n",
    "SVM with linear kernel is efficient for high-dimensional data and when the relationship is approximately linear.\n",
    "\n",
    "### Key Parameters:\n",
    "- `cost`: Penalty for misclassification (C parameter)\n",
    "- `margin`: Epsilon in epsilon-SVR (width of the tube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional linear data\n",
    "X_linear, y_linear = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=50,  # High dimensional\n",
    "    n_informative=30,\n",
    "    n_redundant=10,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_linear = pd.DataFrame(X_linear, columns=[f'X{i+1}' for i in range(50)])\n",
    "df_linear['y'] = y_linear\n",
    "\n",
    "train_linear, test_linear = train_test_split(df_linear, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"High-dimensional dataset: {X_linear.shape[1]} features\")\n",
    "print(f\"Training samples: {len(train_linear)}\")\n",
    "print(f\"Test samples: {len(test_linear)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-dimensional Data Performance\n",
    "\n",
    "Linear SVM is computationally efficient in high dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare different cost values\n",
    "cost_values = [0.1, 1.0, 10.0, 100.0]\n",
    "results = []\n",
    "\n",
    "for cost in cost_values:\n",
    "    # Time the fitting\n",
    "    start = time.time()\n",
    "    model = svm_linear(mode='regression', cost=cost)\n",
    "    fitted = model.fit(train_linear, 'y')\n",
    "    fit_time = time.time() - start\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = fitted.predict(train_linear)\n",
    "    test_pred = fitted.predict(test_linear)\n",
    "    \n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_linear['y'], train_pred['predictions']))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_linear['y'], test_pred['predictions']))\n",
    "    train_r2 = r2_score(train_linear['y'], train_pred['predictions'])\n",
    "    test_r2 = r2_score(test_linear['y'], test_pred['predictions'])\n",
    "    \n",
    "    results.append({\n",
    "        'Cost': cost,\n",
    "        'Fit Time (s)': f'{fit_time:.4f}',\n",
    "        'Train RMSE': f'{train_rmse:.3f}',\n",
    "        'Test RMSE': f'{test_rmse:.3f}',\n",
    "        'Train R²': f'{train_r2:.3f}',\n",
    "        'Test R²': f'{test_r2:.3f}'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nLinear SVM Performance on High-Dimensional Data:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNote: Linear SVM trains quickly even with 50 features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to SVM-RBF\n",
    "\n",
    "Let's compare linear and RBF kernels on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with moderate non-linearity\n",
    "X_comp, y_comp = make_friedman1(n_samples=300, n_features=10, noise=1.0, random_state=42)\n",
    "df_comp = pd.DataFrame(X_comp, columns=[f'X{i+1}' for i in range(10)])\n",
    "df_comp['y'] = y_comp\n",
    "\n",
    "train_comp, test_comp = train_test_split(df_comp, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit both models\n",
    "models = {\n",
    "    'Linear SVM': svm_linear(mode='regression', cost=1.0),\n",
    "    'RBF SVM': svm_rbf(mode='regression', cost=1.0, rbf_sigma=0.1)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit and predict\n",
    "    start = time.time()\n",
    "    fitted = model.fit(train_comp, 'y')\n",
    "    fit_time = time.time() - start\n",
    "    \n",
    "    train_pred = fitted.predict(train_comp)\n",
    "    test_pred = fitted.predict(test_comp)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(train_comp['y'], train_pred['predictions'])\n",
    "    test_r2 = r2_score(test_comp['y'], test_pred['predictions'])\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_comp['y'], test_pred['predictions']))\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'Fit Time (s)': f'{fit_time:.4f}',\n",
    "        'Train R²': f'{train_r2:.3f}',\n",
    "        'Test R²': f'{test_r2:.3f}',\n",
    "        'Test RMSE': f'{test_rmse:.3f}'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nLinear vs RBF Kernel Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Linear SVM: Faster training, good for approximately linear relationships\")\n",
    "print(\"- RBF SVM: Better for non-linear patterns, but slower to train\")\n",
    "print(\"- Choice depends on data complexity and computational budget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM on Simple Data\n",
    "\n",
    "Visualize linear SVM on 1D data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple linear data with noise\n",
    "np.random.seed(42)\n",
    "X_simple = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
    "y_simple = 2 * X_simple.ravel() + 3 + np.random.randn(100) * 2\n",
    "\n",
    "train_simple = pd.DataFrame({'X': X_simple[:70].ravel(), 'y': y_simple[:70]})\n",
    "test_simple = pd.DataFrame({'X': X_simple[70:].ravel(), 'y': y_simple[70:]})\n",
    "\n",
    "# Fit linear SVM\n",
    "model_simple = svm_linear(mode='regression', cost=1.0)\n",
    "fitted_simple = model_simple.fit(train_simple, 'y')\n",
    "\n",
    "# Predict\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "plot_simple = pd.DataFrame({'X': X_plot.ravel()})\n",
    "predictions_simple = fitted_simple.predict(plot_simple)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(train_simple['X'], train_simple['y'], alpha=0.5, label='Train', s=50)\n",
    "plt.scatter(test_simple['X'], test_simple['y'], alpha=0.5, label='Test', s=50, color='orange')\n",
    "plt.plot(X_plot, predictions_simple['predictions'], 'r-', linewidth=2, label='Linear SVM')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear SVM on Simple Linear Data')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_pred_simple = fitted_simple.predict(test_simple)\n",
    "test_r2 = r2_score(test_simple['y'], test_pred_simple['predictions'])\n",
    "print(f\"\\nTest R²: {test_r2:.3f}\")\n",
    "print(\"Linear SVM works well when the underlying relationship is approximately linear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-layer Perceptron (MLP)\n",
    "\n",
    "MLPs are feedforward neural networks that can model complex non-linear relationships through multiple hidden layers.\n",
    "\n",
    "### Key Parameters:\n",
    "- `hidden_units`: Number of neurons in each hidden layer\n",
    "- `num_layers`: Number of hidden layers\n",
    "- `activation`: Activation function ('relu', 'tanh', 'logistic')\n",
    "- `epochs`: Number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex non-linear data\n",
    "X_mlp, y_mlp = make_friedman1(n_samples=500, n_features=10, noise=1.5, random_state=42)\n",
    "df_mlp = pd.DataFrame(X_mlp, columns=[f'X{i+1}' for i in range(10)])\n",
    "df_mlp['y'] = y_mlp\n",
    "\n",
    "train_mlp, test_mlp = train_test_split(df_mlp, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"MLP dataset: {len(train_mlp)} training, {len(test_mlp)} test samples\")\n",
    "print(f\"Features: {X_mlp.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "Compare different network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different architectures\n",
    "architectures = [\n",
    "    {'hidden_units': 10, 'num_layers': 1, 'name': 'Small (10)'},\n",
    "    {'hidden_units': 50, 'num_layers': 1, 'name': 'Medium (50)'},\n",
    "    {'hidden_units': 100, 'num_layers': 1, 'name': 'Large (100)'},\n",
    "    {'hidden_units': 50, 'num_layers': 2, 'name': 'Deep (50x2)'}\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"Training {arch['name']}...\")\n",
    "    \n",
    "    model = mlp(\n",
    "        mode='regression',\n",
    "        hidden_units=arch['hidden_units'],\n",
    "        num_layers=arch['num_layers'],\n",
    "        activation='relu',\n",
    "        epochs=200\n",
    "    )\n",
    "    \n",
    "    fitted = model.fit(train_mlp, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = fitted.predict(train_mlp)\n",
    "    test_pred = fitted.predict(test_mlp)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(train_mlp['y'], train_pred['predictions'])\n",
    "    test_r2 = r2_score(test_mlp['y'], test_pred['predictions'])\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_mlp['y'], test_pred['predictions']))\n",
    "    \n",
    "    arch_results.append({\n",
    "        'Architecture': arch['name'],\n",
    "        'Train R²': f'{train_r2:.3f}',\n",
    "        'Test R²': f'{test_r2:.3f}',\n",
    "        'Test RMSE': f'{test_rmse:.3f}',\n",
    "        'Overfitting': f'{train_r2 - test_r2:.3f}'\n",
    "    })\n",
    "\n",
    "arch_df = pd.DataFrame(arch_results)\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(arch_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Larger networks have more capacity but risk overfitting\")\n",
    "print(\"- Deeper networks can capture more complex patterns\")\n",
    "print(\"- Monitor train/test gap to detect overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers Impact\n",
    "\n",
    "Visualize how the number of layers affects learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different numbers of layers (fixed width)\n",
    "layer_counts = [1, 2, 3, 4]\n",
    "layer_results = []\n",
    "\n",
    "for num_layers in layer_counts:\n",
    "    print(f\"Training {num_layers} layer(s)...\")\n",
    "    \n",
    "    model = mlp(\n",
    "        mode='regression',\n",
    "        hidden_units=50,\n",
    "        num_layers=num_layers,\n",
    "        activation='relu',\n",
    "        epochs=200\n",
    "    )\n",
    "    \n",
    "    fitted = model.fit(train_mlp, 'y')\n",
    "    \n",
    "    test_pred = fitted.predict(test_mlp)\n",
    "    test_r2 = r2_score(test_mlp['y'], test_pred['predictions'])\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_mlp['y'], test_pred['predictions']))\n",
    "    \n",
    "    layer_results.append({\n",
    "        'Layers': num_layers,\n",
    "        'Test R²': test_r2,\n",
    "        'Test RMSE': test_rmse\n",
    "    })\n",
    "\n",
    "layer_df = pd.DataFrame(layer_results)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(layer_df['Layers'], layer_df['Test R²'], 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Hidden Layers')\n",
    "axes[0].set_ylabel('Test R²')\n",
    "axes[0].set_title('Test R² vs Network Depth')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(layer_df['Layers'], layer_df['Test RMSE'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].set_xlabel('Number of Hidden Layers')\n",
    "axes[1].set_ylabel('Test RMSE')\n",
    "axes[1].set_title('Test RMSE vs Network Depth')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLayer Results:\")\n",
    "print(layer_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Different activation functions affect learning dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activation functions\n",
    "activations = ['relu', 'tanh', 'logistic']\n",
    "activation_results = []\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"Training with {activation} activation...\")\n",
    "    \n",
    "    model = mlp(\n",
    "        mode='regression',\n",
    "        hidden_units=50,\n",
    "        num_layers=2,\n",
    "        activation=activation,\n",
    "        epochs=200\n",
    "    )\n",
    "    \n",
    "    fitted = model.fit(train_mlp, 'y')\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = fitted.predict(train_mlp)\n",
    "    test_pred = fitted.predict(test_mlp)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(train_mlp['y'], train_pred['predictions'])\n",
    "    test_r2 = r2_score(test_mlp['y'], test_pred['predictions'])\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_mlp['y'], test_pred['predictions']))\n",
    "    \n",
    "    activation_results.append({\n",
    "        'Activation': activation,\n",
    "        'Train R²': f'{train_r2:.3f}',\n",
    "        'Test R²': f'{test_r2:.3f}',\n",
    "        'Test RMSE': f'{test_rmse:.3f}'\n",
    "    })\n",
    "\n",
    "activation_df = pd.DataFrame(activation_results)\n",
    "print(\"\\nActivation Function Comparison:\")\n",
    "print(activation_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nActivation Function Notes:\")\n",
    "print(\"- ReLU: Fast, avoids vanishing gradients, most common\")\n",
    "print(\"- Tanh: Centered around 0, good for normalized data\")\n",
    "print(\"- Logistic (Sigmoid): Bounded [0,1], can suffer from vanishing gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Visualization on 1D Data\n",
    "\n",
    "See how MLP captures non-linearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex 1D pattern\n",
    "np.random.seed(42)\n",
    "X_1d = np.sort(np.random.rand(200, 1) * 10, axis=0)\n",
    "y_1d = np.sin(X_1d).ravel() + 0.1 * X_1d.ravel()**2 + np.random.randn(200) * 0.5\n",
    "\n",
    "train_1d = pd.DataFrame({'X': X_1d[:140].ravel(), 'y': y_1d[:140]})\n",
    "test_1d = pd.DataFrame({'X': X_1d[140:].ravel(), 'y': y_1d[140:]})\n",
    "\n",
    "# Fit MLP\n",
    "model_1d = mlp(\n",
    "    mode='regression',\n",
    "    hidden_units=50,\n",
    "    num_layers=2,\n",
    "    activation='relu',\n",
    "    epochs=300\n",
    ")\n",
    "fitted_1d = model_1d.fit(train_1d, 'y')\n",
    "\n",
    "# Predict\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "plot_1d = pd.DataFrame({'X': X_plot.ravel()})\n",
    "predictions_1d = fitted_1d.predict(plot_1d)\n",
    "test_pred_1d = fitted_1d.predict(test_1d)\n",
    "\n",
    "# Metrics\n",
    "test_r2 = r2_score(test_1d['y'], test_pred_1d['predictions'])\n",
    "test_rmse = np.sqrt(mean_squared_error(test_1d['y'], test_pred_1d['predictions']))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(train_1d['X'], train_1d['y'], alpha=0.5, label='Train', s=30)\n",
    "plt.scatter(test_1d['X'], test_1d['y'], alpha=0.5, label='Test', s=30, color='orange')\n",
    "plt.plot(X_plot, predictions_1d['predictions'], 'purple', linewidth=2, label='MLP')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'MLP Neural Network (50x2 layers)\\nTest R²: {test_r2:.3f}, RMSE: {test_rmse:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMLP successfully captures complex non-linear patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Let's compare all five models on the same dataset to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison dataset\n",
    "X_compare, y_compare = make_friedman1(n_samples=400, n_features=10, noise=1.0, random_state=42)\n",
    "df_compare = pd.DataFrame(X_compare, columns=[f'X{i+1}' for i in range(10)])\n",
    "df_compare['y'] = y_compare\n",
    "\n",
    "train_compare, test_compare = train_test_split(df_compare, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Comparison dataset: {len(train_compare)} train, {len(test_compare)} test\")\n",
    "print(f\"Features: {X_compare.shape[1]}\")\n",
    "print(f\"Target range: [{y_compare.min():.1f}, {y_compare.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Table\n",
    "\n",
    "Train and evaluate all five models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define all models\n",
    "all_models = {\n",
    "    'Decision Tree': decision_tree(mode='regression', tree_depth=8, min_n=10),\n",
    "    'k-NN': nearest_neighbor(mode='regression', neighbors=5, weight_func='distance'),\n",
    "    'SVM-RBF': svm_rbf(mode='regression', cost=10.0, rbf_sigma=0.1),\n",
    "    'SVM-Linear': svm_linear(mode='regression', cost=1.0),\n",
    "    'MLP': mlp(mode='regression', hidden_units=50, num_layers=2, activation='relu', epochs=200)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in all_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Time training\n",
    "    start = time.time()\n",
    "    fitted = model.fit(train_compare, 'y')\n",
    "    fit_time = time.time() - start\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = fitted.predict(train_compare)\n",
    "    test_pred = fitted.predict(test_compare)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_compare['y'], train_pred['predictions']))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_compare['y'], test_pred['predictions']))\n",
    "    train_r2 = r2_score(train_compare['y'], train_pred['predictions'])\n",
    "    test_r2 = r2_score(test_compare['y'], test_pred['predictions'])\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'Fit Time (s)': f'{fit_time:.3f}',\n",
    "        'Train RMSE': f'{train_rmse:.2f}',\n",
    "        'Test RMSE': f'{test_rmse:.2f}',\n",
    "        'Train R²': f'{train_r2:.3f}',\n",
    "        'Test R²': f'{test_r2:.3f}',\n",
    "        'Overfit Gap': f'{train_r2 - test_r2:.3f}'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - ALL 5 ALGORITHMS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison\n",
    "\n",
    "Plot model performance side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric values for plotting\n",
    "models_list = comparison_df['Model'].values\n",
    "test_r2_values = [float(x) for x in comparison_df['Test R²'].values]\n",
    "test_rmse_values = [float(x) for x in comparison_df['Test RMSE'].values]\n",
    "fit_times = [float(x) for x in comparison_df['Fit Time (s)'].values]\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# R² comparison\n",
    "colors = ['steelblue', 'green', 'purple', 'red', 'orange']\n",
    "axes[0].barh(models_list, test_r2_values, color=colors)\n",
    "axes[0].set_xlabel('Test R²')\n",
    "axes[0].set_title('Model Accuracy (Higher is Better)')\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].barh(models_list, test_rmse_values, color=colors)\n",
    "axes[1].set_xlabel('Test RMSE')\n",
    "axes[1].set_title('Prediction Error (Lower is Better)')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Training time comparison\n",
    "axes[2].barh(models_list, fit_times, color=colors)\n",
    "axes[2].set_xlabel('Fit Time (seconds)')\n",
    "axes[2].set_title('Training Speed (Lower is Better)')\n",
    "axes[2].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Model\n",
    "\n",
    "Here's a practical guide for choosing between these models:\n",
    "\n",
    "#### Decision Tree\n",
    "**Best for:**\n",
    "- Interpretability is critical\n",
    "- Mixed feature types (numeric + categorical)\n",
    "- Quick baseline model\n",
    "- Feature importance analysis\n",
    "\n",
    "**Avoid when:**\n",
    "- Need smooth predictions\n",
    "- Data is high-dimensional\n",
    "- Small dataset (prone to overfitting)\n",
    "\n",
    "#### k-Nearest Neighbors\n",
    "**Best for:**\n",
    "- Non-parametric problems\n",
    "- Local patterns matter\n",
    "- Online learning (easy to update)\n",
    "- Small to medium datasets\n",
    "\n",
    "**Avoid when:**\n",
    "- High-dimensional data (curse of dimensionality)\n",
    "- Need fast predictions (slow at inference)\n",
    "- Sparse data\n",
    "\n",
    "#### SVM-RBF\n",
    "**Best for:**\n",
    "- Complex non-linear patterns\n",
    "- Medium-sized datasets\n",
    "- Need good generalization\n",
    "- Well-defined feature space\n",
    "\n",
    "**Avoid when:**\n",
    "- Very large datasets (slow training)\n",
    "- Need interpretability\n",
    "- Relationship is mostly linear\n",
    "\n",
    "#### SVM-Linear\n",
    "**Best for:**\n",
    "- High-dimensional data\n",
    "- Linear/near-linear relationships\n",
    "- Large datasets\n",
    "- Need fast training\n",
    "\n",
    "**Avoid when:**\n",
    "- Highly non-linear patterns\n",
    "- Small number of features\n",
    "- Need feature importance\n",
    "\n",
    "#### Multi-layer Perceptron (MLP)\n",
    "**Best for:**\n",
    "- Complex non-linear patterns\n",
    "- Large datasets\n",
    "- High-dimensional inputs\n",
    "- When you can tune hyperparameters\n",
    "\n",
    "**Avoid when:**\n",
    "- Small datasets (overfitting)\n",
    "- Need interpretability\n",
    "- Limited computational resources\n",
    "- Quick prototyping needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recommendation matrix\n",
    "recommendation_data = {\n",
    "    'Scenario': [\n",
    "        'Small dataset (<100 samples)',\n",
    "        'Large dataset (>10,000 samples)',\n",
    "        'High dimensional (>50 features)',\n",
    "        'Need interpretability',\n",
    "        'Complex non-linearity',\n",
    "        'Mostly linear relationship',\n",
    "        'Fast training required',\n",
    "        'Fast prediction required',\n",
    "        'Feature importance needed'\n",
    "    ],\n",
    "    'Best Choice': [\n",
    "        'k-NN or Decision Tree',\n",
    "        'SVM-Linear or MLP',\n",
    "        'SVM-Linear or MLP',\n",
    "        'Decision Tree',\n",
    "        'SVM-RBF or MLP',\n",
    "        'SVM-Linear',\n",
    "        'Decision Tree or SVM-Linear',\n",
    "        'Decision Tree or SVM-Linear',\n",
    "        'Decision Tree'\n",
    "    ],\n",
    "    'Avoid': [\n",
    "        'MLP (overfitting)',\n",
    "        'k-NN (memory)',\n",
    "        'k-NN (curse of dimensionality)',\n",
    "        'MLP or SVM',\n",
    "        'SVM-Linear',\n",
    "        'k-NN (inefficient)',\n",
    "        'SVM-RBF or MLP',\n",
    "        'k-NN (slow)',\n",
    "        'k-NN or SVM'\n",
    "    ]\n",
    "}\n",
    "\n",
    "recommendation_df = pd.DataFrame(recommendation_data)\n",
    "print(\"\\nMODEL SELECTION GUIDE\")\n",
    "print(\"=\"*80)\n",
    "print(recommendation_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding the Three-DataFrame Structure\n",
    "\n",
    "All py-tidymodels models follow a consistent output structure. Let's examine the three DataFrames returned by each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature_1': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'feature_2': [2.5, 3.5, 4.5, 5.5, 6.5],\n",
    "    'target': [10.0, 20.0, 30.0, 40.0, 50.0]\n",
    "})\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'feature_1': [2.5, 3.5],\n",
    "    'feature_2': [3.0, 4.0]\n",
    "})\n",
    "\n",
    "print(\"Sample training data:\")\n",
    "print(sample_data)\n",
    "print(\"\\nNew data for prediction:\")\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 1: Model Specification\n",
    "\n",
    "The specification object contains model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification\n",
    "model_spec = decision_tree(mode='regression', tree_depth=5, min_n=2)\n",
    "\n",
    "print(\"Model Specification Object:\")\n",
    "print(f\"Type: {type(model_spec)}\")\n",
    "print(f\"\\nModel type: {model_spec.model_type}\")\n",
    "print(f\"Mode: {model_spec.mode}\")\n",
    "print(f\"Engine: {model_spec.engine}\")\n",
    "print(f\"\\nParameters:\")\n",
    "for param, value in model_spec.parameters.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nKey insight: Specification is reusable and doesn't contain any fitted data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 2: Fitted Model\n",
    "\n",
    "The fitted object contains the trained model and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "fitted_model = model_spec.fit(sample_data, 'target')\n",
    "\n",
    "print(\"Fitted Model Object:\")\n",
    "print(f\"Type: {type(fitted_model)}\")\n",
    "print(f\"\\nTarget variable: {fitted_model.target}\")\n",
    "print(f\"Feature names: {fitted_model.feature_names}\")\n",
    "print(f\"\\nUnderlying sklearn model: {type(fitted_model.model)}\")\n",
    "print(f\"Model is fitted: {hasattr(fitted_model.model, 'tree_')}\")\n",
    "\n",
    "# Access the underlying sklearn model\n",
    "print(\"\\nDirect access to sklearn model attributes:\")\n",
    "print(f\"Tree depth: {fitted_model.model.get_depth()}\")\n",
    "print(f\"Number of leaves: {fitted_model.model.get_n_leaves()}\")\n",
    "\n",
    "print(\"\\nKey insight: Fitted object wraps sklearn model with tidymodels interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 3: Predictions\n",
    "\n",
    "The predictions object is a clean DataFrame with results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = fitted_model.predict(new_data)\n",
    "\n",
    "print(\"Predictions Object:\")\n",
    "print(f\"Type: {type(predictions)}\")\n",
    "print(f\"Shape: {predictions.shape}\")\n",
    "print(f\"\\nColumns: {predictions.columns.tolist()}\")\n",
    "print(\"\\nPredictions DataFrame:\")\n",
    "print(predictions)\n",
    "\n",
    "# Predictions can be easily manipulated\n",
    "predictions['rounded'] = predictions['predictions'].round(1)\n",
    "print(\"\\nWith added column:\")\n",
    "print(predictions)\n",
    "\n",
    "print(\"\\nKey insight: Predictions are standard pandas DataFrames for easy manipulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Workflow Example\n",
    "\n",
    "Let's see the full three-step workflow with each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "workflow_data = pd.DataFrame({\n",
    "    'X1': np.random.rand(50),\n",
    "    'X2': np.random.rand(50),\n",
    "    'X3': np.random.rand(50),\n",
    "    'y': np.random.rand(50) * 100\n",
    "})\n",
    "\n",
    "test_data = workflow_data.iloc[:5].drop('y', axis=1)\n",
    "\n",
    "models_to_demo = {\n",
    "    'Decision Tree': decision_tree(mode='regression', tree_depth=5),\n",
    "    'k-NN': nearest_neighbor(mode='regression', neighbors=5),\n",
    "    'SVM-Linear': svm_linear(mode='regression', cost=1.0)\n",
    "}\n",
    "\n",
    "print(\"THREE-STEP WORKFLOW FOR EACH MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model_spec in models_to_demo.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Specify\n",
    "    print(\"Step 1 - Specification:\")\n",
    "    print(f\"  Created: {type(model_spec).__name__}\")\n",
    "    print(f\"  Parameters: {model_spec.parameters}\")\n",
    "    \n",
    "    # Step 2: Fit\n",
    "    fitted = model_spec.fit(workflow_data, 'y')\n",
    "    print(\"Step 2 - Fitted Model:\")\n",
    "    print(f\"  Type: {type(fitted).__name__}\")\n",
    "    print(f\"  Features: {fitted.feature_names}\")\n",
    "    \n",
    "    # Step 3: Predict\n",
    "    preds = fitted.predict(test_data)\n",
    "    print(\"Step 3 - Predictions:\")\n",
    "    print(f\"  Shape: {preds.shape}\")\n",
    "    print(f\"  Columns: {preds.columns.tolist()}\")\n",
    "    print(f\"  Sample values: {preds['predictions'].iloc[:3].values}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All models follow the same consistent interface!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the five sklearn regression models available in py-tidymodels:\n",
    "\n",
    "1. **Decision Tree**: Interpretable, captures non-linearity, but prone to overfitting\n",
    "2. **k-NN**: Non-parametric, good for local patterns, but slow in high dimensions\n",
    "3. **SVM-RBF**: Excellent for complex non-linear relationships, moderate training time\n",
    "4. **SVM-Linear**: Fast and efficient for high-dimensional linear problems\n",
    "5. **MLP**: Powerful for complex patterns, requires more data and tuning\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Consistent Interface**: All models follow specify → fit → predict\n",
    "- **Three-Step Workflow**: Specification, fitted model, predictions\n",
    "- **Model Selection**: Choose based on data characteristics and requirements\n",
    "- **Hyperparameter Tuning**: Critical for optimal performance\n",
    "- **Bias-Variance Tradeoff**: Balance model complexity with generalization\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore ensemble methods (Random Forest, Gradient Boosting)\n",
    "- Implement cross-validation for hyperparameter tuning\n",
    "- Try preprocessing and feature engineering\n",
    "- Combine models in workflows for production pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
