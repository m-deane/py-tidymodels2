{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 37: Advanced sklearn Regression Models\n",
    "\n",
    "**Feature**: decision_tree(), nearest_neighbor(), svm_rbf(), svm_linear(), mlp()\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **5 advanced sklearn-based regression models** available in py-tidymodels:\n",
    "\n",
    "1. **decision_tree()**: Single decision tree for regression\n",
    "2. **nearest_neighbor()**: k-Nearest Neighbors (k-NN)\n",
    "3. **svm_rbf()**: Support Vector Machine with RBF kernel\n",
    "4. **svm_linear()**: Support Vector Machine with linear kernel\n",
    "5. **mlp()**: Multi-Layer Perceptron (neural network)\n",
    "\n",
    "## CRITICAL API Pattern\n",
    "\n",
    "**All these models require `.set_mode('regression')` method call**:\n",
    "```python\n",
    "# WRONG - Will cause TypeError\n",
    "model = decision_tree(mode='regression', tree_depth=5)\n",
    "\n",
    "# CORRECT - Mode set via method chaining\n",
    "model = decision_tree(tree_depth=5).set_mode('regression')\n",
    "```\n",
    "\n",
    "This is different from `rand_forest()` and `boost_tree()` which accept mode parameter.\n",
    "\n",
    "## When to Use Each Model\n",
    "\n",
    "**Decision Tree**:\n",
    "- ✅ Highly interpretable (visualize splits)\n",
    "- ✅ Handles non-linear relationships\n",
    "- ✅ No feature scaling needed\n",
    "- ❌ Prone to overfitting (use pruning)\n",
    "- ❌ High variance (small data changes → different tree)\n",
    "\n",
    "**k-Nearest Neighbors**:\n",
    "- ✅ No training phase (lazy learning)\n",
    "- ✅ Non-parametric (no assumptions)\n",
    "- ✅ Good for local patterns\n",
    "- ❌ Slow prediction with large datasets\n",
    "- ❌ Requires feature scaling\n",
    "\n",
    "**SVM (RBF kernel)**:\n",
    "- ✅ Excellent for non-linear patterns\n",
    "- ✅ Robust to outliers (in feature space)\n",
    "- ✅ Good with high-dimensional data\n",
    "- ❌ Slow training on large datasets (>10K rows)\n",
    "- ❌ Requires careful hyperparameter tuning\n",
    "\n",
    "**SVM (Linear kernel)**:\n",
    "- ✅ Faster than RBF kernel\n",
    "- ✅ Good for linearly separable data\n",
    "- ✅ Scales better to large datasets\n",
    "- ❌ Limited to linear relationships\n",
    "- ❌ Still slower than linear regression\n",
    "\n",
    "**MLP (Neural Network)**:\n",
    "- ✅ Universal function approximator\n",
    "- ✅ Handles very complex non-linear patterns\n",
    "- ✅ Feature interactions automatically learned\n",
    "- ❌ Requires large datasets (>1000 rows)\n",
    "- ❌ Black box (hard to interpret)\n",
    "- ❌ Sensitive to hyperparameters\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**Refinery Margins** (Germany):\n",
    "- Monthly refinery margins 2006-2024\n",
    "- Crude oil prices as predictors\n",
    "- Non-linear relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_parsnip import (\n",
    "    decision_tree, nearest_neighbor, svm_rbf, svm_linear, mlp,\n",
    "    linear_reg, rand_forest\n",
    ")\n",
    "from py_rsample import initial_time_split\n",
    "from py_yardstick import rmse, mae, r_squared\n",
    "from py_yardstick import metric_set\n",
    "from py_workflows import Workflow\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_recipes import recipe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load refinery margins data\n",
    "df = pd.read_csv('../_md/__data/refinery_margins.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Filter to Germany\n",
    "germany = df[df['country'] == 'Germany'].copy()\n",
    "\n",
    "# Select columns\n",
    "germany = germany[[\n",
    "    'date', 'brent', 'dubai', 'wti', 'brent_cracking_nw_europe'\n",
    "]].rename(columns={'brent_cracking_nw_europe': 'margin'})\n",
    "\n",
    "germany = germany.dropna().sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Germany refinery margin data:\")\n",
    "print(f\"  Records: {len(germany):,} months\")\n",
    "print(f\"  Date range: {germany['date'].min()} to {germany['date'].max()}\")\n",
    "print(f\"  Margin mean: ${germany['margin'].mean():.2f}/bbl\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(germany.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "split = initial_time_split(germany, date_column='date', prop=0.85)\n",
    "train = split.training()\n",
    "test = split.testing()\n",
    "\n",
    "print(f\"Train: {len(train)} months ({train['date'].min()} to {train['date'].max()})\")\n",
    "print(f\"Test:  {len(test)} months ({test['date'].min()} to {test['date'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree\n",
    "\n",
    "Single decision tree with pruning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with pruning\n",
    "spec_tree = decision_tree(\n",
    "    tree_depth=5,       # Max depth\n",
    "    min_n=10,           # Min samples per leaf\n",
    "    cost_complexity=0.01  # Pruning parameter\n",
    ").set_mode('regression')  # CRITICAL: set_mode() method\n",
    "\n",
    "fit_tree = spec_tree.fit(train, 'margin ~ brent + dubai + wti')\n",
    "eval_tree = fit_tree.evaluate(test)\n",
    "_, _, stats_tree = eval_tree.extract_outputs()\n",
    "\n",
    "test_stats_tree = stats_tree[stats_tree['split'] == 'test'].iloc[0]\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"  Test RMSE: ${test_stats_tree['rmse']:.3f}/bbl\")\n",
    "print(f\"  Test MAE: ${test_stats_tree['mae']:.3f}/bbl\")\n",
    "print(f\"  Test R²: {test_stats_tree['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-Nearest Neighbors\n",
    "\n",
    "Distance-based learning. **Requires feature scaling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with normalization (REQUIRED for distance-based methods)\n",
    "rec_knn = recipe().step_normalize(all_numeric_predictors())\n",
    "\n",
    "spec_knn = nearest_neighbor(\n",
    "    neighbors=5,        # k=5 neighbors\n",
    "    weight_func='uniform'  # or 'distance' for weighted\n",
    ").set_mode('regression')\n",
    "\n",
    "wf_knn = Workflow().add_recipe(rec_knn).add_model(spec_knn)\n",
    "fit_knn = wf_knn.fit(train)\n",
    "eval_knn = fit_knn.evaluate(test)\n",
    "_, _, stats_knn = eval_knn.extract_outputs()\n",
    "\n",
    "test_stats_knn = stats_knn[stats_knn['split'] == 'test'].iloc[0]\n",
    "print(\"k-Nearest Neighbors (k=5):\")\n",
    "print(f\"  Test RMSE: ${test_stats_knn['rmse']:.3f}/bbl\")\n",
    "print(f\"  Test MAE: ${test_stats_knn['mae']:.3f}/bbl\")\n",
    "print(f\"  Test R²: {test_stats_knn['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM with RBF Kernel\n",
    "\n",
    "Non-linear SVM. **Requires feature scaling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM RBF with normalization\n",
    "rec_svm = recipe().step_normalize(all_numeric_predictors())\n",
    "\n",
    "spec_svm_rbf = svm_rbf(\n",
    "    cost=1.0,           # Regularization (C parameter)\n",
    "    rbf_sigma=0.1       # Kernel width (gamma)\n",
    ").set_mode('regression')\n",
    "\n",
    "wf_svm_rbf = Workflow().add_recipe(rec_svm).add_model(spec_svm_rbf)\n",
    "\n",
    "start_time = time.time()\n",
    "fit_svm_rbf = wf_svm_rbf.fit(train)\n",
    "svm_rbf_train_time = time.time() - start_time\n",
    "\n",
    "eval_svm_rbf = fit_svm_rbf.evaluate(test)\n",
    "_, _, stats_svm_rbf = eval_svm_rbf.extract_outputs()\n",
    "\n",
    "test_stats_svm_rbf = stats_svm_rbf[stats_svm_rbf['split'] == 'test'].iloc[0]\n",
    "print(\"SVM (RBF kernel):\")\n",
    "print(f\"  Training time: {svm_rbf_train_time:.3f} seconds\")\n",
    "print(f\"  Test RMSE: ${test_stats_svm_rbf['rmse']:.3f}/bbl\")\n",
    "print(f\"  Test MAE: ${test_stats_svm_rbf['mae']:.3f}/bbl\")\n",
    "print(f\"  Test R²: {test_stats_svm_rbf['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVM with Linear Kernel\n",
    "\n",
    "Linear SVM (faster than RBF). **Requires feature scaling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Linear with normalization\n",
    "spec_svm_linear = svm_linear(\n",
    "    cost=1.0            # Regularization (C parameter)\n",
    ").set_mode('regression')\n",
    "\n",
    "wf_svm_linear = Workflow().add_recipe(rec_svm).add_model(spec_svm_linear)\n",
    "\n",
    "start_time = time.time()\n",
    "fit_svm_linear = wf_svm_linear.fit(train)\n",
    "svm_linear_train_time = time.time() - start_time\n",
    "\n",
    "eval_svm_linear = fit_svm_linear.evaluate(test)\n",
    "_, _, stats_svm_linear = eval_svm_linear.extract_outputs()\n",
    "\n",
    "test_stats_svm_linear = stats_svm_linear[stats_svm_linear['split'] == 'test'].iloc[0]\n",
    "print(\"SVM (Linear kernel):\")\n",
    "print(f\"  Training time: {svm_linear_train_time:.3f} seconds\")\n",
    "print(f\"  Test RMSE: ${test_stats_svm_linear['rmse']:.3f}/bbl\")\n",
    "print(f\"  Test MAE: ${test_stats_svm_linear['mae']:.3f}/bbl\")\n",
    "print(f\"  Test R²: {test_stats_svm_linear['r_squared']:.4f}\")\n",
    "print(f\"\\nSpeedup vs RBF: {svm_rbf_train_time / svm_linear_train_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Layer Perceptron (Neural Network)\n",
    "\n",
    "Feed-forward neural network. **Requires feature scaling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with normalization\n",
    "rec_mlp = recipe().step_normalize(all_numeric_predictors())\n",
    "\n",
    "spec_mlp = mlp(\n",
    "    hidden_units=50,    # Neurons in hidden layer\n",
    "    epochs=200,         # Training iterations\n",
    "    learn_rate=0.01,    # Learning rate\n",
    "    activation='relu'   # Activation function\n",
    ").set_mode('regression')\n",
    "\n",
    "wf_mlp = Workflow().add_recipe(rec_mlp).add_model(spec_mlp)\n",
    "\n",
    "start_time = time.time()\n",
    "fit_mlp = wf_mlp.fit(train)\n",
    "mlp_train_time = time.time() - start_time\n",
    "\n",
    "eval_mlp = fit_mlp.evaluate(test)\n",
    "_, _, stats_mlp = eval_mlp.extract_outputs()\n",
    "\n",
    "test_stats_mlp = stats_mlp[stats_mlp['split'] == 'test'].iloc[0]\n",
    "print(\"Multi-Layer Perceptron (Neural Network):\")\n",
    "print(f\"  Training time: {mlp_train_time:.3f} seconds\")\n",
    "print(f\"  Test RMSE: ${test_stats_mlp['rmse']:.3f}/bbl\")\n",
    "print(f\"  Test MAE: ${test_stats_mlp['mae']:.3f}/bbl\")\n",
    "print(f\"  Test R²: {test_stats_mlp['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Comparison\n",
    "\n",
    "Compare all 5 sklearn models plus baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'Decision Tree',\n",
    "        'type': 'sklearn',\n",
    "        'rmse': test_stats_tree['rmse'],\n",
    "        'mae': test_stats_tree['mae'],\n",
    "        'r_squared': test_stats_tree['r_squared']\n",
    "    },\n",
    "    {\n",
    "        'model': 'k-NN (k=5)',\n",
    "        'type': 'sklearn',\n",
    "        'rmse': test_stats_knn['rmse'],\n",
    "        'mae': test_stats_knn['mae'],\n",
    "        'r_squared': test_stats_knn['r_squared']\n",
    "    },\n",
    "    {\n",
    "        'model': 'SVM (RBF)',\n",
    "        'type': 'sklearn',\n",
    "        'rmse': test_stats_svm_rbf['rmse'],\n",
    "        'mae': test_stats_svm_rbf['mae'],\n",
    "        'r_squared': test_stats_svm_rbf['r_squared']\n",
    "    },\n",
    "    {\n",
    "        'model': 'SVM (Linear)',\n",
    "        'type': 'sklearn',\n",
    "        'rmse': test_stats_svm_linear['rmse'],\n",
    "        'mae': test_stats_svm_linear['mae'],\n",
    "        'r_squared': test_stats_svm_linear['r_squared']\n",
    "    },\n",
    "    {\n",
    "        'model': 'MLP',\n",
    "        'type': 'sklearn',\n",
    "        'rmse': test_stats_mlp['rmse'],\n",
    "        'mae': test_stats_mlp['mae'],\n",
    "        'r_squared': test_stats_mlp['r_squared']\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison = comparison.sort_values('rmse')\n",
    "\n",
    "print(\"\\nAdvanced sklearn Models Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest model: {comparison.iloc[0]['model']}\")\n",
    "print(f\"  RMSE: ${comparison.iloc[0]['rmse']:.3f}/bbl\")\n",
    "print(f\"  R²: {comparison.iloc[0]['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. WorkflowSet Comparison\n",
    "\n",
    "Use WorkflowSet to systematically compare all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflows for all sklearn models\n",
    "rec_norm = recipe().step_normalize(all_numeric_predictors())\n",
    "\n",
    "sklearn_models = [\n",
    "    ('decision_tree', decision_tree(tree_depth=5, min_n=10).set_mode('regression')),\n",
    "    ('knn', nearest_neighbor(neighbors=5).set_mode('regression')),\n",
    "    ('svm_rbf', svm_rbf(cost=1.0, rbf_sigma=0.1).set_mode('regression')),\n",
    "    ('svm_linear', svm_linear(cost=1.0).set_mode('regression')),\n",
    "    ('mlp', mlp(hidden_units=50, epochs=200, learn_rate=0.01).set_mode('regression'))\n",
    "]\n",
    "\n",
    "workflows = []\n",
    "for name, spec in sklearn_models:\n",
    "    # Decision tree doesn't need normalization, others do\n",
    "    if name == 'decision_tree':\n",
    "        wf = Workflow().add_formula('margin ~ brent + dubai + wti').add_model(spec)\n",
    "    else:\n",
    "        wf = Workflow().add_recipe(rec_norm).add_model(spec)\n",
    "    workflows.append(wf)\n",
    "\n",
    "wf_set = WorkflowSet.from_workflows(workflows)\n",
    "\n",
    "print(f\"Created WorkflowSet with {len(workflows)} sklearn models\")\n",
    "print(f\"Models: {list(wf_set.workflows.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all workflows\n",
    "wf_results = []\n",
    "for wf_id, wf in wf_set.workflows.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        fit = wf.fit(train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        eval_fit = fit.evaluate(test)\n",
    "        _, _, stats = eval_fit.extract_outputs()\n",
    "        \n",
    "        test_stats = stats[stats['split'] == 'test'].iloc[0]\n",
    "        wf_results.append({\n",
    "            'workflow': wf_id,\n",
    "            'train_time_sec': train_time,\n",
    "            'rmse': test_stats['rmse'],\n",
    "            'mae': test_stats['mae'],\n",
    "            'r_squared': test_stats['r_squared']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {wf_id} failed - {str(e)[:80]}\")\n",
    "\n",
    "wf_comparison = pd.DataFrame(wf_results)\n",
    "wf_comparison = wf_comparison.sort_values('rmse')\n",
    "\n",
    "print(\"\\nWorkflowSet Results:\")\n",
    "print(\"=\"*90)\n",
    "print(wf_comparison.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare with Baseline Models\n",
    "\n",
    "How do advanced sklearn models compare to simpler baselines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add baseline models\n",
    "baseline_models = [\n",
    "    ('linear_reg', linear_reg()),\n",
    "    ('random_forest', rand_forest(trees=100).set_mode('regression'))\n",
    "]\n",
    "\n",
    "all_results = wf_results.copy()\n",
    "\n",
    "for name, model in baseline_models:\n",
    "    start_time = time.time()\n",
    "    fit = model.fit(train, 'margin ~ brent + dubai + wti')\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    eval_fit = fit.evaluate(test)\n",
    "    _, _, stats = eval_fit.extract_outputs()\n",
    "    \n",
    "    test_stats = stats[stats['split'] == 'test'].iloc[0]\n",
    "    all_results.append({\n",
    "        'workflow': name,\n",
    "        'train_time_sec': train_time,\n",
    "        'rmse': test_stats['rmse'],\n",
    "        'mae': test_stats['mae'],\n",
    "        'r_squared': test_stats['r_squared']\n",
    "    })\n",
    "\n",
    "final_comparison = pd.DataFrame(all_results)\n",
    "final_comparison = final_comparison.sort_values('rmse')\n",
    "\n",
    "print(\"\\nAll Models Comparison (sklearn + Baselines):\")\n",
    "print(\"=\"*90)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### API Pattern (CRITICAL)\n",
    "\n",
    "**Must use `.set_mode('regression')` method**:\n",
    "```python\n",
    "# These 5 models require method call\n",
    "decision_tree(...).set_mode('regression')\n",
    "nearest_neighbor(...).set_mode('regression')\n",
    "svm_rbf(...).set_mode('regression')\n",
    "svm_linear(...).set_mode('regression')\n",
    "mlp(...).set_mode('regression')\n",
    "\n",
    "# Compare to rand_forest/boost_tree which accept parameter\n",
    "rand_forest(mode='regression')  # This works\n",
    "boost_tree(mode='regression')   # This works\n",
    "```\n",
    "\n",
    "### Feature Scaling Requirements\n",
    "\n",
    "**MUST normalize for**:\n",
    "- k-NN (distance-based)\n",
    "- SVM (both RBF and linear)\n",
    "- MLP (neural networks)\n",
    "\n",
    "**NO normalization needed for**:\n",
    "- Decision trees (split-based)\n",
    "\n",
    "```python\n",
    "# Pattern for models requiring scaling\n",
    "rec = recipe().step_normalize(all_numeric_predictors())\n",
    "wf = Workflow().add_recipe(rec).add_model(spec)\n",
    "```\n",
    "\n",
    "### Model Selection Guidance\n",
    "\n",
    "**Choose Decision Tree when**:\n",
    "- Interpretability is paramount\n",
    "- Need to visualize decision rules\n",
    "- Mixed data types (numeric + categorical)\n",
    "- Baseline for tree ensembles\n",
    "\n",
    "**Choose k-NN when**:\n",
    "- Strong local patterns (nearby observations similar)\n",
    "- Small to medium datasets (<10K rows)\n",
    "- No training phase needed (lazy learning)\n",
    "- Continuous updates (just add new points)\n",
    "\n",
    "**Choose SVM (RBF) when**:\n",
    "- Complex non-linear relationships\n",
    "- High-dimensional feature space\n",
    "- Robust to outliers needed\n",
    "- Dataset size: 100-10K rows (sweet spot)\n",
    "\n",
    "**Choose SVM (Linear) when**:\n",
    "- Linear or near-linear relationships\n",
    "- Need faster SVM alternative\n",
    "- High-dimensional sparse data\n",
    "- Larger datasets (10K+ rows)\n",
    "\n",
    "**Choose MLP when**:\n",
    "- Very complex non-linear patterns\n",
    "- Large datasets (>1000 rows)\n",
    "- Feature interactions important\n",
    "- Have time for hyperparameter tuning\n",
    "\n",
    "### Performance Patterns\n",
    "\n",
    "From our refinery margin example:\n",
    "1. **Accuracy**: Random Forest often best, then SVM RBF\n",
    "2. **Speed**: Decision tree fastest, SVM RBF slowest\n",
    "3. **Interpretability**: Decision tree >> k-NN > SVM/MLP\n",
    "4. **Stability**: SVM/MLP more stable than single decision tree\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "**Decision Tree**:\n",
    "```python\n",
    "decision_tree(\n",
    "    tree_depth=5,          # 3-10 typical\n",
    "    min_n=10,              # 5-50 typical\n",
    "    cost_complexity=0.01   # 0.001-0.1 (pruning)\n",
    ")\n",
    "```\n",
    "\n",
    "**k-NN**:\n",
    "```python\n",
    "nearest_neighbor(\n",
    "    neighbors=5,           # 3-20 typical, odd numbers\n",
    "    weight_func='uniform'  # or 'distance'\n",
    ")\n",
    "```\n",
    "\n",
    "**SVM RBF**:\n",
    "```python\n",
    "svm_rbf(\n",
    "    cost=1.0,              # 0.1-100 (regularization)\n",
    "    rbf_sigma=0.1          # 0.001-1.0 (kernel width)\n",
    ")\n",
    "# Tune cost first, then rbf_sigma\n",
    "```\n",
    "\n",
    "**SVM Linear**:\n",
    "```python\n",
    "svm_linear(\n",
    "    cost=1.0               # 0.1-100 (regularization)\n",
    ")\n",
    "```\n",
    "\n",
    "**MLP**:\n",
    "```python\n",
    "mlp(\n",
    "    hidden_units=50,       # 10-200 typical\n",
    "    epochs=200,            # 100-1000 typical\n",
    "    learn_rate=0.01,       # 0.001-0.1 typical\n",
    "    activation='relu'      # relu, tanh, logistic\n",
    ")\n",
    "# Tune hidden_units first, then learn_rate\n",
    "```\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "```python\n",
    "# Production pattern\n",
    "from py_parsnip import svm_rbf\n",
    "from py_workflows import Workflow\n",
    "from py_recipes import recipe\n",
    "\n",
    "# Preprocessing + SVM\n",
    "rec = recipe().step_normalize(all_numeric_predictors())\n",
    "spec = svm_rbf(cost=10.0, rbf_sigma=0.1).set_mode('regression')\n",
    "wf = Workflow().add_recipe(rec).add_model(spec)\n",
    "\n",
    "# Fit on all training data\n",
    "final_fit = wf.fit(all_training_data)\n",
    "\n",
    "# Predict\n",
    "predictions = final_fit.predict(new_data)\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Forgetting `.set_mode('regression')`**: TypeError\n",
    "   - Solution: Always call `.set_mode('regression')` after model creation\n",
    "\n",
    "2. **Not normalizing for k-NN/SVM/MLP**: Poor performance\n",
    "   - Solution: Use `step_normalize()` in recipe\n",
    "\n",
    "3. **Deep decision trees**: Severe overfitting\n",
    "   - Solution: Limit `tree_depth` to 3-10, use `cost_complexity`\n",
    "\n",
    "4. **Too few neighbors in k-NN**: Noisy predictions\n",
    "   - Solution: Use k=5 or higher, cross-validate to find optimal k\n",
    "\n",
    "5. **SVM on large datasets**: Extremely slow\n",
    "   - Solution: Sample data or use linear_reg/rand_forest instead\n",
    "\n",
    "6. **MLP on small datasets**: Overfitting\n",
    "   - Solution: Reduce `hidden_units`, increase regularization, or use simpler model\n",
    "\n",
    "### Comparison to Other Models\n",
    "\n",
    "**vs Random Forest**:\n",
    "- Single decision tree: More interpretable, less accurate\n",
    "- Random Forest: Ensemble of trees, much more accurate\n",
    "\n",
    "**vs Gradient Boosting**:\n",
    "- SVM/MLP: Good for small-medium datasets\n",
    "- XGBoost/LightGBM: Better for large datasets, tabular data\n",
    "\n",
    "**vs Linear Regression**:\n",
    "- Linear models: Fastest, most interpretable\n",
    "- sklearn models: More flexible, handle non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **decision_tree()**: Interpretable single tree with pruning  \n",
    "✅ **nearest_neighbor()**: k-NN for local patterns  \n",
    "✅ **svm_rbf()**: Non-linear SVM with RBF kernel  \n",
    "✅ **svm_linear()**: Linear SVM (faster alternative)  \n",
    "✅ **mlp()**: Neural network for complex patterns  \n",
    "✅ **Proper `.set_mode('regression')` usage** (CRITICAL!)  \n",
    "✅ **Feature normalization** for distance/gradient-based models  \n",
    "✅ **WorkflowSet integration** for systematic comparison  \n",
    "✅ **Benchmarking** vs linear_reg and rand_forest  \n",
    "\n",
    "**Key Insight**: These 5 sklearn models provide flexibility between interpretability (decision tree), local patterns (k-NN), non-linear capabilities (SVM RBF), speed (SVM linear), and complex pattern learning (MLP). Feature scaling is CRITICAL for k-NN, SVM, and MLP.\n",
    "\n",
    "**API Reminder**: All 5 models require `.set_mode('regression')` method call, NOT mode parameter in constructor.\n",
    "\n",
    "**Next Steps**:\n",
    "- Hyperparameter tuning with `tune_grid()`\n",
    "- Feature engineering with `recipes`\n",
    "- Ensemble methods combining multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
