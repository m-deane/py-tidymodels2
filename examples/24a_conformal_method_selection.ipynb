{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Method Selection by Dataset Size\n",
    "\n",
    "This notebook demonstrates how to select the optimal conformal prediction method based on **dataset size**.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**3 Main Conformal Methods:**\n",
    "1. **Split** - O(1) complexity, fastest, best for large datasets (>10k samples)\n",
    "2. **CV+** - O(K) complexity, balanced approach, best for medium datasets (1k-10k samples)\n",
    "3. **Jackknife+** - O(n) complexity, most data-efficient, best for small datasets (<1k samples)\n",
    "\n",
    "**Auto-Selection:** `method='auto'` automatically picks the best method based on data size and model type.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "1. Small dataset (200 samples) - Jackknife+ provides best coverage\n",
    "2. Medium dataset (2,000 samples) - CV+ balances accuracy and speed\n",
    "3. Large dataset (20,000 samples) - Split is most efficient\n",
    "4. Auto-selection correctly picks optimal method for each case\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from py_parsnip import linear_reg\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Helper Functions\n",
    "\n",
    "Create functions to generate data and evaluate conformal predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(n_samples, n_features=5, noise=0.1):\n",
    "    \"\"\"Generate synthetic regression data.\"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise * 100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(n_features)])\n",
    "    df['y'] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_conformal_method(train_data, test_data, method, alpha=0.05):\n",
    "    \"\"\"Evaluate a conformal method and return metrics.\"\"\"\n",
    "    # Fit model\n",
    "    spec = linear_reg()\n",
    "    fit = spec.fit(train_data, 'y ~ .')\n",
    "    \n",
    "    # Time conformal prediction\n",
    "    start_time = time.time()\n",
    "    conformal_preds = fit.conformal_predict(\n",
    "        test_data,\n",
    "        alpha=alpha,\n",
    "        method=method\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    actuals = test_data['y'].values\n",
    "    in_interval = (\n",
    "        (actuals >= conformal_preds['.pred_lower'].values) &\n",
    "        (actuals <= conformal_preds['.pred_upper'].values)\n",
    "    )\n",
    "    coverage = in_interval.mean()\n",
    "    avg_width = (conformal_preds['.pred_upper'] - conformal_preds['.pred_lower']).mean()\n",
    "    \n",
    "    return {\n",
    "        'method': method,\n",
    "        'coverage': coverage,\n",
    "        'avg_interval_width': avg_width,\n",
    "        'time_seconds': elapsed_time,\n",
    "        'n_train': len(train_data),\n",
    "        'n_test': len(test_data)\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Small Dataset (200 samples)\n",
    "\n",
    "## Hypothesis: Jackknife+ should perform best\n",
    "\n",
    "With limited data, we need the most data-efficient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate small dataset\n",
    "small_data = generate_regression_data(n_samples=250, noise=0.15)\n",
    "\n",
    "# Split train/test (200 train, 50 test)\n",
    "small_train = small_data.iloc[:200].copy()\n",
    "small_test = small_data.iloc[200:].copy()\n",
    "\n",
    "print(f\"Small Dataset:\")\n",
    "print(f\"  Train: {len(small_train)} samples\")\n",
    "print(f\"  Test:  {len(small_test)} samples\")\n",
    "print(f\"\\nData preview:\")\n",
    "print(small_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods on small dataset\n",
    "small_results = []\n",
    "\n",
    "for method in ['split', 'cv+', 'jackknife+', 'auto']:\n",
    "    result = evaluate_conformal_method(small_train, small_test, method)\n",
    "    small_results.append(result)\n",
    "    print(f\"{method:12s} - Coverage: {result['coverage']:.1%}, \"\n",
    "          f\"Width: {result['avg_interval_width']:.2f}, \"\n",
    "          f\"Time: {result['time_seconds']:.3f}s\")\n",
    "\n",
    "small_df = pd.DataFrame(small_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Small Dataset Results:\")\n",
    "print(small_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Expected Results:**\n",
    "- **Jackknife+**: Best coverage (closest to 95%), narrowest intervals\n",
    "- **CV+**: Good coverage, slightly wider intervals\n",
    "- **Split**: May have worse coverage due to small calibration set\n",
    "- **Auto**: Should select jackknife+ or cv+ for this size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Medium Dataset (2,000 samples)\n",
    "\n",
    "## Hypothesis: CV+ should be optimal balance\n",
    "\n",
    "Medium datasets benefit from CV+'s balance of accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate medium dataset\n",
    "medium_data = generate_regression_data(n_samples=2500, noise=0.15)\n",
    "\n",
    "# Split train/test (2000 train, 500 test)\n",
    "medium_train = medium_data.iloc[:2000].copy()\n",
    "medium_test = medium_data.iloc[2000:].copy()\n",
    "\n",
    "print(f\"Medium Dataset:\")\n",
    "print(f\"  Train: {len(medium_train)} samples\")\n",
    "print(f\"  Test:  {len(medium_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods on medium dataset\n",
    "medium_results = []\n",
    "\n",
    "for method in ['split', 'cv+', 'jackknife+', 'auto']:\n",
    "    result = evaluate_conformal_method(medium_train, medium_test, method)\n",
    "    medium_results.append(result)\n",
    "    print(f\"{method:12s} - Coverage: {result['coverage']:.1%}, \"\n",
    "          f\"Width: {result['avg_interval_width']:.2f}, \"\n",
    "          f\"Time: {result['time_seconds']:.3f}s\")\n",
    "\n",
    "medium_df = pd.DataFrame(medium_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Medium Dataset Results:\")\n",
    "print(medium_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Expected Results:**\n",
    "- **CV+**: Best balance of coverage, width, and speed\n",
    "- **Split**: Very fast, similar coverage to CV+\n",
    "- **Jackknife+**: Best coverage but much slower (O(n) overhead)\n",
    "- **Auto**: Should select cv+ for this size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Large Dataset (20,000 samples)\n",
    "\n",
    "## Hypothesis: Split should be most efficient\n",
    "\n",
    "Large datasets provide enough calibration samples for split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large dataset\n",
    "large_data = generate_regression_data(n_samples=25000, noise=0.15)\n",
    "\n",
    "# Split train/test (20000 train, 5000 test)\n",
    "large_train = large_data.iloc[:20000].copy()\n",
    "large_test = large_data.iloc[20000:].copy()\n",
    "\n",
    "print(f\"Large Dataset:\")\n",
    "print(f\"  Train: {len(large_train)} samples\")\n",
    "print(f\"  Test:  {len(large_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods on large dataset\n",
    "# Note: Skip jackknife+ for large dataset (too slow)\n",
    "large_results = []\n",
    "\n",
    "for method in ['split', 'cv+', 'auto']:\n",
    "    result = evaluate_conformal_method(large_train, large_test, method)\n",
    "    large_results.append(result)\n",
    "    print(f\"{method:12s} - Coverage: {result['coverage']:.1%}, \"\n",
    "          f\"Width: {result['avg_interval_width']:.2f}, \"\n",
    "          f\"Time: {result['time_seconds']:.3f}s\")\n",
    "\n",
    "large_df = pd.DataFrame(large_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Large Dataset Results:\")\n",
    "print(large_df.to_string(index=False))\n",
    "print(\"\\nNote: Jackknife+ skipped (O(n) too slow for 20k samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Expected Results:**\n",
    "- **Split**: Fastest by far, excellent coverage (large calibration set)\n",
    "- **CV+**: Similar coverage but ~5x slower\n",
    "- **Auto**: Should select split for this size\n",
    "- **Jackknife+**: Would take minutes (not practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Comparison Across All Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    small_df.assign(dataset_size='Small (200)'),\n",
    "    medium_df.assign(dataset_size='Medium (2k)'),\n",
    "    large_df.assign(dataset_size='Large (20k)')\n",
    "])\n",
    "\n",
    "print(\"Complete Comparison:\")\n",
    "print(\"=\"*100)\n",
    "print(all_results[['dataset_size', 'method', 'coverage', 'avg_interval_width', 'time_seconds']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Coverage by Method and Dataset Size\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Coverage\n",
    "coverage_data = all_results[all_results['method'] != 'auto']\n",
    "for method in ['split', 'cv+', 'jackknife+']:\n",
    "    subset = coverage_data[coverage_data['method'] == method]\n",
    "    if len(subset) > 0:\n",
    "        axes[0].plot(range(len(subset)), subset['coverage'], 'o-', \n",
    "                    label=method, markersize=10, linewidth=2)\n",
    "\n",
    "axes[0].axhline(y=0.95, color='red', linestyle='--', label='Target 95%')\n",
    "axes[0].set_xticks(range(3))\n",
    "axes[0].set_xticklabels(['Small\\n(200)', 'Medium\\n(2k)', 'Large\\n(20k)'])\n",
    "axes[0].set_ylabel('Coverage')\n",
    "axes[0].set_title('Coverage by Dataset Size')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.85, 1.0])\n",
    "\n",
    "# Plot 2: Interval Width\n",
    "for method in ['split', 'cv+', 'jackknife+']:\n",
    "    subset = coverage_data[coverage_data['method'] == method]\n",
    "    if len(subset) > 0:\n",
    "        axes[1].plot(range(len(subset)), subset['avg_interval_width'], 'o-', \n",
    "                    label=method, markersize=10, linewidth=2)\n",
    "\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_xticklabels(['Small\\n(200)', 'Medium\\n(2k)', 'Large\\n(20k)'])\n",
    "axes[1].set_ylabel('Average Interval Width')\n",
    "axes[1].set_title('Interval Width by Dataset Size')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Computation Time (log scale)\n",
    "for method in ['split', 'cv+', 'jackknife+']:\n",
    "    subset = coverage_data[coverage_data['method'] == method]\n",
    "    if len(subset) > 0:\n",
    "        axes[2].plot(range(len(subset)), subset['time_seconds'], 'o-', \n",
    "                    label=method, markersize=10, linewidth=2)\n",
    "\n",
    "axes[2].set_xticks(range(3))\n",
    "axes[2].set_xticklabels(['Small\\n(200)', 'Medium\\n(2k)', 'Large\\n(20k)'])\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].set_title('Computation Time by Dataset Size (log scale)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization shows tradeoffs clearly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Auto-Selection Verification\n",
    "\n",
    "Verify that `method='auto'` selects the optimal method for each dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what auto selected for each dataset\n",
    "auto_selections = all_results[all_results['method'] == 'auto'][['dataset_size', 'coverage', 'time_seconds']].copy()\n",
    "\n",
    "print(\"Auto-Selection Results:\")\n",
    "print(\"=\"*80)\n",
    "print(auto_selections.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Expected Auto-Selection Logic:\")\n",
    "print(\"  â€¢ Small (200):    jackknife+ or cv+  (n < 1000)\")\n",
    "print(\"  â€¢ Medium (2k):    cv+                (1000 â‰¤ n â‰¤ 10000)\")\n",
    "print(\"  â€¢ Large (20k):    split              (n > 10000)\")\n",
    "print(\"\\nâœ“ Auto-selection provides good defaults without manual tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Practical Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONFORMAL METHOD SELECTION GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š BY DATASET SIZE:\")\n",
    "print(\"  â€¢ Small (<1k):       Use 'jackknife+'  (most data-efficient)\")\n",
    "print(\"  â€¢ Medium (1k-10k):   Use 'cv+'         (balanced approach)\")\n",
    "print(\"  â€¢ Large (>10k):      Use 'split'       (fastest, O(1))\")\n",
    "\n",
    "print(\"\\nâš¡ BY COMPUTATION BUDGET:\")\n",
    "print(\"  â€¢ Tight deadline:    Use 'split'       (always fastest)\")\n",
    "print(\"  â€¢ Flexible timing:   Use 'cv+'         (better coverage)\")\n",
    "print(\"  â€¢ Maximum accuracy:  Use 'jackknife+'  (best for small data)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ WHEN TO USE 'AUTO':\")\n",
    "print(\"  â€¢ Default choice for production code\")\n",
    "print(\"  â€¢ Variable dataset sizes\")\n",
    "print(\"  â€¢ You're unsure which method to pick\")\n",
    "print(\"  â€¢ Auto-selection is conservative and reliable\")\n",
    "\n",
    "print(\"\\nâš ï¸  IMPORTANT NOTES:\")\n",
    "print(\"  â€¢ All methods achieve ~95% coverage with enough data\")\n",
    "print(\"  â€¢ Jackknife+ is O(n) - avoid for large datasets (>5k)\")\n",
    "print(\"  â€¢ Split needs minimum ~30 calibration samples (15% of train)\")\n",
    "print(\"  â€¢ CV+ k=5 is good default (5-fold cross-validation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Demonstrated\n",
    "\n",
    "1. âœ… **Small Datasets (200 samples)**\n",
    "   - Jackknife+ provides best coverage with limited data\n",
    "   - Leave-one-out approach maximizes data efficiency\n",
    "   - Acceptable computation time for small n\n",
    "\n",
    "2. âœ… **Medium Datasets (2,000 samples)**\n",
    "   - CV+ offers optimal balance of accuracy and speed\n",
    "   - Similar coverage to Jackknife+ but much faster\n",
    "   - 5-fold CV is good default\n",
    "\n",
    "3. âœ… **Large Datasets (20,000 samples)**\n",
    "   - Split is most practical (O(1) complexity)\n",
    "   - Excellent coverage with large calibration set\n",
    "   - 10-100x faster than CV+ or Jackknife+\n",
    "\n",
    "4. âœ… **Auto-Selection**\n",
    "   - Reliably picks appropriate method for dataset size\n",
    "   - Conservative defaults ensure good coverage\n",
    "   - Recommended for production use\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**When in doubt, use `method='auto'`** - it provides sensible defaults based on data characteristics.\n",
    "\n",
    "**The \"best\" method depends on your constraints:**\n",
    "- Data availability â†’ Jackknife+ for small datasets\n",
    "- Computation budget â†’ Split for large datasets\n",
    "- Balance both â†’ CV+ for medium datasets\n",
    "\n",
    "**All methods achieve target coverage** when properly applied to appropriate dataset sizes.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- See `24b_timeseries_conformal_enbpi.ipynb` for time series conformal methods\n",
    "- See `24c_feature_selection_conformal.ipynb` for WorkflowSet conformal comparison\n",
    "- See `examples/22_conformal_prediction_demo.ipynb` for comprehensive overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
