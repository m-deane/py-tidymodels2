{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection via Conformal Interval Width\n",
    "\n",
    "This notebook demonstrates using **WorkflowSet.compare_conformal()** to select optimal features based on uncertainty quantification.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "**Traditional Approach:** Select features based on predictive accuracy (RMSE, R\u00b2)\n",
    "\n",
    "**Conformal Approach:** Select features based on:\n",
    "1. **Interval Width** - Tighter intervals = better uncertainty quantification\n",
    "2. **Coverage** - Maintains target coverage (e.g., 95%)\n",
    "3. **Balance** - Good accuracy + tight intervals\n",
    "\n",
    "## Key Method (NEW)\n",
    "\n",
    "```python\n",
    "# Compare conformal intervals across ALL workflows\n",
    "comparison = wf_set.compare_conformal(\n",
    "    data=train_data,\n",
    "    alpha=0.05,\n",
    "    method='split'\n",
    ")\n",
    "```\n",
    "\n",
    "Returns DataFrame sorted by **average interval width** (tightest first).\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "1. Load real-world refinery production data (JODI)\n",
    "2. Create 6 different feature sets (simple \u2192 complex)\n",
    "3. Test with 2 models (linear_reg, rand_forest)\n",
    "4. Compare all 12 workflows via conformal intervals\n",
    "5. Key insight: **More features \u2260 tighter intervals**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_workflowsets import WorkflowSet\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "\n",
    "Using JODI (Joint Organisations Data Initiative) refinery production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "refinery = pd.read_csv('../_md/__data/jodi_refinery_production_data.csv')\n",
    "\n",
    "# Convert date\n",
    "refinery['date'] = pd.to_datetime(refinery['date'])\n",
    "\n",
    "# Filter for Refinery Intake only\n",
    "refinery = refinery[refinery['subcategory'] == 'Refinery Intake'].copy()\n",
    "\n",
    "# Sort\n",
    "refinery = refinery.sort_values(['country', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {refinery.shape}\")\n",
    "print(f\"Date range: {refinery['date'].min()} to {refinery['date'].max()}\")\n",
    "print(f\"Countries: {refinery['country'].nunique()}\")\n",
    "print(f\"\\nColumns: {list(refinery.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 5 countries by average production\n",
    "avg_production = refinery.groupby('country')['value'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Refining Countries:\")\n",
    "print(avg_production.head(10))\n",
    "\n",
    "# Select top 5 for faster demonstration\n",
    "top_countries = avg_production.head(5).index.tolist()\n",
    "refinery_subset = refinery[refinery['country'].isin(top_countries)].copy()\n",
    "\n",
    "print(f\"\\n\u2713 Selected countries: {top_countries}\")\n",
    "print(f\"\u2713 Filtered dataset: {refinery_subset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Feature Engineering\n",
    "\n",
    "Create lagged features and rolling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create lagged and rolling features.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Lagged production (1, 3, 6, 12 months)\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        df[f'prod_lag_{lag}'] = df.groupby('country')['value'].shift(lag)\n",
    "    \n",
    "    # Rolling means (3, 6 months)\n",
    "    df['prod_ma_3'] = df.groupby('country')['value'].transform(\n",
    "        lambda x: x.shift(1).rolling(3, min_periods=1).mean()\n",
    "    )\n",
    "    df['prod_ma_6'] = df.groupby('country')['value'].transform(\n",
    "        lambda x: x.shift(1).rolling(6, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Date features\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "refinery_features = create_features(refinery_subset)\n",
    "\n",
    "# Drop missing values\n",
    "refinery_clean = refinery_features.dropna().copy()\n",
    "\n",
    "print(f\"Dataset with features: {refinery_clean.shape}\")\n",
    "print(f\"\\nFeatures created:\")\n",
    "print(f\"  Lags: {[c for c in refinery_clean.columns if 'lag' in c]}\")\n",
    "print(f\"  MAs:  {[c for c in refinery_clean.columns if 'ma' in c]}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(refinery_clean[['date', 'country', 'value', 'prod_lag_1', 'prod_ma_3']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last 12 months for testing\n",
    "split_date = refinery_clean['date'].max() - pd.DateOffset(months=12)\n",
    "\n",
    "train_data = refinery_clean[refinery_clean['date'] <= split_date].copy()\n",
    "test_data = refinery_clean[refinery_clean['date'] > split_date].copy()\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples (up to {train_data['date'].max().date()})\")\n",
    "print(f\"Test:  {len(test_data)} samples (from {test_data['date'].min().date()})\")\n",
    "print(f\"\\nTrain countries: {train_data['country'].nunique()}\")\n",
    "print(f\"Test countries:  {test_data['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Define Feature Strategies\n",
    "\n",
    "Create 6 preprocessing strategies from simple to complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formulas with different feature combinations\n",
    "strategies = [\n",
    "    # Strategy 1: Minimal (just recent lag)\n",
    "    \"value ~ prod_lag_1\",\n",
    "    \n",
    "    # Strategy 2: Short-term lags\n",
    "    \"value ~ prod_lag_1 + prod_lag_3\",\n",
    "    \n",
    "    # Strategy 3: Short + medium term\n",
    "    \"value ~ prod_lag_1 + prod_lag_3 + prod_lag_6\",\n",
    "    \n",
    "    # Strategy 4: All lags\n",
    "    \"value ~ prod_lag_1 + prod_lag_3 + prod_lag_6 + prod_lag_12\",\n",
    "    \n",
    "    # Strategy 5: Lags + rolling means\n",
    "    \"value ~ prod_lag_1 + prod_lag_3 + prod_lag_6 + prod_ma_3 + prod_ma_6\",\n",
    "    \n",
    "    # Strategy 6: Comprehensive (lags + MA + seasonality)\n",
    "    \"value ~ prod_lag_1 + prod_lag_3 + prod_lag_6 + prod_ma_3 + prod_ma_6 + month + quarter\"\n",
    "]\n",
    "\n",
    "print(f\"Number of preprocessing strategies: {len(strategies)}\")\n",
    "print(\"\\nStrategies:\")\n",
    "for i, s in enumerate(strategies, 1):\n",
    "    # Count features\n",
    "    n_features = len(s.split(' ~ ')[1].split(' + '))\n",
    "    print(f\"  {i}. ({n_features} features) {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Create WorkflowSet\n",
    "\n",
    "Cross product: 6 strategies \u00d7 2 models = 12 workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WorkflowSet\n",
    "wf_set = WorkflowSet.from_cross(\n",
    "    preproc=strategies,\n",
    "    models=[\n",
    "        linear_reg(),\n",
    "        rand_forest(trees=50).set_mode('regression')  # Fewer trees for speed\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Created WorkflowSet with {len(wf_set.workflows)} workflows\")\n",
    "print(f\"\\nWorkflow IDs:\")\n",
    "for wf_id in wf_set.workflows.keys():\n",
    "    print(f\"  {wf_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Compare Conformal Intervals (NEW METHOD)\n",
    "\n",
    "## Key Feature: `WorkflowSet.compare_conformal()`\n",
    "\n",
    "This fits ALL workflows and compares their conformal prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare conformal intervals across all workflows\n",
    "print(\"Comparing conformal intervals across all workflows...\")\n",
    "print(\"(This may take 1-2 minutes...)\\n\")\n",
    "\n",
    "comparison = wf_set.compare_conformal(\n",
    "    data=train_data,\n",
    "    alpha=0.05,\n",
    "    method='split'\n",
    ")\n",
    "\n",
    "print(\"\\nConformal Interval Comparison Results:\")\n",
    "print(\"=\"*100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n\u2713 Results sorted by average interval width (tightest first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Shorten workflow labels\n",
    "comparison['short_label'] = [\n",
    "    f\"S{i+1}_{wf_id.split('_')[-2]}\" \n",
    "    for i, wf_id in enumerate(comparison['wflow_id'])\n",
    "]\n",
    "\n",
    "# Plot 1: Interval width\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(comparison)))\n",
    "axes[0].barh(comparison['short_label'], comparison['avg_interval_width'], color=colors)\n",
    "axes[0].set_xlabel('Average Interval Width (lower = better)')\n",
    "axes[0].set_ylabel('Strategy_Model')\n",
    "axes[0].set_title('Conformal Interval Width Comparison')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Annotate best\n",
    "best_idx = 0\n",
    "axes[0].axhline(y=best_idx, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Plot 2: Coverage\n",
    "axes[1].barh(comparison['short_label'], comparison['coverage'], color=colors)\n",
    "axes[1].axvline(x=0.95, color='red', linestyle='--', linewidth=2, label='Target 95%')\n",
    "axes[1].set_xlabel('Coverage')\n",
    "axes[1].set_ylabel('Strategy_Model')\n",
    "axes[1].set_title('Empirical Coverage')\n",
    "axes[1].set_xlim([0.85, 1.0])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"\u2022 Best workflow: {comparison.iloc[0]['wflow_id']}\")\n",
    "print(f\"  - Tightest intervals: {comparison.iloc[0]['avg_interval_width']:.2f}\")\n",
    "print(f\"  - Coverage: {comparison.iloc[0]['coverage']:.1%}\")\n",
    "print(f\"\\n\u2022 Worst workflow: {comparison.iloc[-1]['wflow_id']}\")\n",
    "print(f\"  - Widest intervals: {comparison.iloc[-1]['avg_interval_width']:.2f}\")\n",
    "print(f\"  - Coverage: {comparison.iloc[-1]['coverage']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Analyze Feature Complexity vs Interval Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract strategy number and model type\n",
    "comparison['strategy_num'] = comparison['wflow_id'].str.extract(r'prep_(\\d+)_')[0].astype(int)\n",
    "comparison['model_type'] = comparison['model'].str.replace('_reg', '').str.replace('_forest', '_forest')\n",
    "\n",
    "# Plot: Strategy complexity vs interval width\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model in comparison['model_type'].unique():\n",
    "    subset = comparison[comparison['model_type'] == model]\n",
    "    ax.plot(subset['strategy_num'], subset['avg_interval_width'], \n",
    "           'o-', label=model, markersize=10, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Strategy Number (1=simple, 6=complex)', fontsize=12)\n",
    "ax.set_ylabel('Average Interval Width', fontsize=12)\n",
    "ax.set_title('Feature Complexity vs Uncertainty Quantification', fontsize=14)\n",
    "ax.set_xticks(range(1, 7))\n",
    "ax.legend(title='Model', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsight: More features \u2260 tighter intervals\")\n",
    "print(\"  \u2022 Simple strategies may provide better uncertainty quantification\")\n",
    "print(\"  \u2022 Overfitting increases interval width (model uncertainty)\")\n",
    "print(\"  \u2022 Optimal strategy balances predictive power and overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Use Best Workflow for Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best workflow (tightest intervals with good coverage)\n",
    "best_wf_id = comparison.iloc[0]['wflow_id']\n",
    "best_workflow = wf_set[best_wf_id]\n",
    "\n",
    "print(f\"Best Workflow: {best_wf_id}\")\n",
    "print(f\"  Interval width: {comparison.iloc[0]['avg_interval_width']:.2f}\")\n",
    "print(f\"  Coverage: {comparison.iloc[0]['coverage']:.1%}\")\n",
    "print(f\"  Model: {comparison.iloc[0]['model']}\")\n",
    "\n",
    "# Fit on full training data\n",
    "print(\"\\nFitting best workflow on training data...\")\n",
    "best_fit = best_workflow.fit(train_data)\n",
    "\n",
    "print(\"\u2713 Model fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get conformal predictions for test data\n",
    "best_predictions = best_fit.conformal_predict(\n",
    "    test_data,\n",
    "    alpha=0.05,\n",
    "    method='split'\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(best_predictions)} predictions\")\n",
    "print(f\"\\nColumns: {list(best_predictions.columns)}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(best_predictions[['.pred', '.pred_lower', '.pred_upper']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecasts for first country\n",
    "first_country = test_data['country'].iloc[0]\n",
    "country_test = test_data[test_data['country'] == first_country].reset_index(drop=True)\n",
    "country_pred = best_predictions[:len(country_test)]\n",
    "\n",
    "n_show = min(50, len(country_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(range(n_show), country_test['value'].values[:n_show],\n",
    "       'o-', label='Actual', markersize=5, linewidth=1.5)\n",
    "ax.plot(range(n_show), country_pred['.pred'].values[:n_show],\n",
    "       'k-', label='Prediction', linewidth=2)\n",
    "ax.fill_between(\n",
    "    range(n_show),\n",
    "    country_pred['.pred_lower'].values[:n_show],\n",
    "    country_pred['.pred_upper'].values[:n_show],\n",
    "    alpha=0.3,\n",
    "    label='95% Conformal Interval'\n",
    ")\n",
    "\n",
    "ax.set_title(f\"{first_country} - Refinery Production Forecast (Best Workflow)\")\n",
    "ax.set_xlabel('Month (Test Period)')\n",
    "ax.set_ylabel('Production (kbd)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2713 Best workflow provides well-calibrated prediction intervals\")\n",
    "print(f\"\u2713 Selected via conformal comparison (not just RMSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Demonstrated\n",
    "\n",
    "1. \u2705 **WorkflowSet.compare_conformal()** - NEW METHOD\n",
    "   - Fits all workflows and compares conformal intervals\n",
    "   - Returns DataFrame sorted by interval width\n",
    "   - Identifies optimal preprocessing for uncertainty quantification\n",
    "\n",
    "2. \u2705 **Feature Selection via Uncertainty**\n",
    "   - Traditional: Select by RMSE or R\u00b2\n",
    "   - Conformal: Select by interval width + coverage\n",
    "   - More features \u2260 better uncertainty quantification\n",
    "\n",
    "3. \u2705 **Multiple Strategies Comparison**\n",
    "   - 6 feature strategies (simple \u2192 complex)\n",
    "   - 2 model types (linear, random forest)\n",
    "   - 12 total workflows evaluated\n",
    "\n",
    "4. \u2705 **Real-World Application**\n",
    "   - JODI refinery production data\n",
    "   - 5 countries, monthly observations\n",
    "   - Practical energy forecasting scenario\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Use conformal intervals for feature selection when:**\n",
    "- Uncertainty quantification matters (risk management, planning)\n",
    "- You need calibrated prediction intervals\n",
    "- Multiple feature sets perform similarly in RMSE\n",
    "- Want to avoid overfitting (wider intervals signal overfitting)\n",
    "\n",
    "**Method Advantages:**\n",
    "- \u2705 Data-driven preprocessing selection\n",
    "- \u2705 Systematic comparison across all workflows\n",
    "- \u2705 Balances accuracy and uncertainty\n",
    "- \u2705 One method call evaluates everything\n",
    "\n",
    "**Practical Workflow:**\n",
    "```python\n",
    "# 1. Define multiple strategies\n",
    "strategies = [formula1, formula2, ...]\n",
    "\n",
    "# 2. Create WorkflowSet\n",
    "wf_set = WorkflowSet.from_cross(preproc=strategies, models=[model1, model2])\n",
    "\n",
    "# 3. Compare via conformal intervals\n",
    "comparison = wf_set.compare_conformal(train_data, alpha=0.05)\n",
    "\n",
    "# 4. Select best\n",
    "best_wf_id = comparison.iloc[0]['wflow_id']\n",
    "best_wf = wf_set[best_wf_id]\n",
    "\n",
    "# 5. Fit and use\n",
    "final_fit = best_wf.fit(train_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- See `24e_per_group_conformal.ipynb` for per-group calibration\n",
    "- See `24h_cv_conformal_integration.ipynb` for CV + conformal dual ranking\n",
    "- See `examples/22_conformal_prediction_demo.ipynb` for comprehensive overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}