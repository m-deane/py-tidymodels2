{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 24: Refinery Margin Prediction with NSGA-II Multi-Objective Optimization\n",
    "\n",
    "This notebook demonstrates using genetic algorithm feature selection with **NSGA-II multi-objective optimization** for refinery margin prediction.\n",
    "\n",
    "**Dataset**: Refinery margins with crude oil prices and production data (2006-2023)\n",
    "\n",
    "**Key Features**:\n",
    "- Panel/grouped data (multiple countries)\n",
    "- Many predictors: crude oil prices (brent, dubai, wti), refinery throughput, various margin metrics\n",
    "- Target: brent_cracking_nw_europe (refinery margin)\n",
    "- **Enhancement demonstrated**: NSGA-II for balancing performance vs sparsity vs cost\n",
    "\n",
    "**NSGA-II Benefits**:\n",
    "- Optimize multiple conflicting objectives simultaneously\n",
    "- Find Pareto-optimal trade-off solutions\n",
    "- Balance model accuracy with feature count and computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom py_recipes import recipe\nfrom py_recipes.steps import step_select_genetic_algorithm, step_normalize\nfrom py_workflows import workflow\nfrom py_parsnip import linear_reg\nfrom py_yardstick import rmse, mae, r_squared\n\n# Load data\ndata = pd.read_csv('../_md/__data/refinery_margins.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\nprint(f\"Data shape: {data.shape}\")\nprint(f\"\\nCountries: {sorted(data['country'].unique())}\")\nprint(f\"\\nDate range: {data['date'].min()} to {data['date'].max()}\")\nprint(f\"\\nColumns: {list(data.columns)}\")\nprint(f\"\\nFirst few rows:\")\ndata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Focus on predicting Brent cracking margin in NW Europe using crude prices and other margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictive features (exclude target and non-predictive columns)\ntarget = 'brent_cracking_nw_europe'\n\n# Features to use as predictors\nfeature_cols = [\n    'refinery_kbd',  # Refinery throughput\n    'brent', 'dubai', 'wti',  # Crude oil prices\n    'brent_hydroskimming_nw_europe',  # Related margin\n    'urals_cracking_nw_europe', 'urals_hydroskimming_nw_europe',  # Urals margins\n    'es_sider_cracking_med', 'es_sider_hydroskimming_med',  # Mediterranean margins\n    'dubai_cracking_singapore', 'dubai_hydroskimming_singapore',  # Singapore margins\n    'tapis_hydroskimming_singapore',  # Tapis margin\n    'x50_50_hls_lls_cracking_usgc'  # US Gulf Coast margin\n]\n\n# Keep only relevant columns\ndata_model = data[['date', 'country', target] + feature_cols].copy()\n\n# Remove rows with missing target\ndata_model = data_model.dropna(subset=[target])\n\nprint(f\"Modeling data shape: {data_model.shape}\")\nprint(f\"\\nTarget: {target}\")\nprint(f\"Features ({len(feature_cols)}): {feature_cols}\")\n\n# Train/test split (80/20 by date)\nsplit_date = data_model['date'].quantile(0.8)\ntrain = data_model[data_model['date'] <= split_date].copy()\ntest = data_model[data_model['date'] > split_date].copy()\n\nprint(f\"\\nTrain: {train.shape[0]} rows, {train['date'].min()} to {train['date'].max()}\")\nprint(f\"Test: {test.shape[0]} rows, {test['date'].min()} to {test['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: All Features (No Selection)\n",
    "\n",
    "Establish baseline performance using all 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one country for demonstration (Germany - large refining capacity)\ncountry = 'Germany'\ntrain_country = train[train['country'] == country].drop(['date', 'country'], axis=1).copy()\ntest_country = test[test['country'] == country].drop(['date', 'country'], axis=1).copy()\n\n# Remove any remaining NaNs\ntrain_country = train_country.dropna()\ntest_country = test_country.dropna()\n\nprint(f\"Training on {country}\")\nprint(f\"Train: {train_country.shape}\")\nprint(f\"Test: {test_country.shape}\")\n\n# Baseline model with all features\nbaseline_wf = workflow().add_formula(f'{target} ~ .').add_model(linear_reg())\nbaseline_fit = baseline_wf.fit(train_country)\n\n# Evaluate\nbaseline_preds = baseline_fit.predict(test_country)\nbaseline_rmse = rmse(test_country[target], baseline_preds['.pred']).iloc[0]['value']\nbaseline_mae = mae(test_country[target], baseline_preds['.pred']).iloc[0]['value']\nbaseline_r2 = r_squared(test_country[target], baseline_preds['.pred']).iloc[0]['value']\n\nprint(f\"\\n=== Baseline (All {len(feature_cols)} Features) ===\")\nprint(f\"RMSE: {baseline_rmse:.4f}\")\nprint(f\"MAE: {baseline_mae:.4f}\")\nprint(f\"R\u00b2: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Costs\n",
    "\n",
    "Assign hypothetical costs to features based on data acquisition difficulty.\n",
    "\n",
    "**Cost Categories**:\n",
    "- High cost: Real-time international margin data (10 units)\n",
    "- Medium cost: Crude oil prices (5 units)\n",
    "- Low cost: Local refinery throughput (1 unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature costs (hypothetical)\nfeature_costs = {\n    'refinery_kbd': 1,  # Low cost - local data\n    'brent': 5, 'dubai': 5, 'wti': 5,  # Medium cost - crude prices\n    'brent_hydroskimming_nw_europe': 7,  # Medium-high cost - regional margin\n    'urals_cracking_nw_europe': 10, 'urals_hydroskimming_nw_europe': 10,  # High cost - international margins\n    'es_sider_cracking_med': 10, 'es_sider_hydroskimming_med': 10,\n    'dubai_cracking_singapore': 10, 'dubai_hydroskimming_singapore': 10,\n    'tapis_hydroskimming_singapore': 10,\n    'x50_50_hls_lls_cracking_usgc': 10\n}\n\nprint(\"Feature costs:\")\nfor feat, cost in sorted(feature_costs.items(), key=lambda x: x[1]):\n    print(f\"  {feat}: {cost}\")\n\ntotal_cost_all_features = sum(feature_costs.values())\nprint(f\"\\nTotal cost of all features: {total_cost_all_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NSGA-II: Two-Objective Optimization (Performance vs Sparsity)\n",
    "\n",
    "First, optimize for performance and sparsity (number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSGA-II with performance and sparsity objectives\nrec_nsga2_2obj = recipe(train_country)\nrec_nsga2_2obj = step_select_genetic_algorithm(\n    rec_nsga2_2obj,\n    outcome=target,\n    model=linear_reg(),\n    metric='rmse',\n    \n    # NSGA-II settings\n    use_nsga2=True,\n    nsga2_objectives=['performance', 'sparsity'],\n    nsga2_selection_method='knee_point',  # Best trade-off\n    \n    # GA settings\n    population_size=50,\n    generations=30,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_nsga2_2obj = rec_nsga2_2obj.prep(train_country)\nselected_2obj = prepped_nsga2_2obj.prepared_steps[0]._selected_features\n\nprint(f\"\\n=== NSGA-II (Performance + Sparsity) ===\")\nprint(f\"Selected features ({len(selected_2obj)}): {selected_2obj}\")\nprint(f\"\\nPareto front size: {len(prepped_nsga2_2obj.prepared_steps[0]._pareto_front)}\")\n\n# Evaluate on test set\nif len(selected_2obj) > 0:\n    train_selected = prepped_nsga2_2obj.bake(train_country)\n    test_selected = prepped_nsga2_2obj.bake(test_country)\n    \n    wf = workflow().add_formula(f'{target} ~ .').add_model(linear_reg())\n    fit = wf.fit(train_selected)\n    preds = fit.predict(test_selected)\n    \n    test_rmse = rmse(test_selected[target], preds['.pred']).iloc[0]['value']\n    test_mae = mae(test_selected[target], preds['.pred']).iloc[0]['value']\n    test_r2 = r_squared(test_selected[target], preds['.pred']).iloc[0]['value']\n    \n    print(f\"\\nTest performance:\")\n    print(f\"  RMSE: {test_rmse:.4f} (baseline: {baseline_rmse:.4f})\")\n    print(f\"  MAE: {test_mae:.4f}\")\n    print(f\"  R\u00b2: {test_r2:.4f}\")\n    print(f\"  Features reduced: {len(feature_cols)} \u2192 {len(selected_2obj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NSGA-II: Three-Objective Optimization (Performance + Sparsity + Cost)\n",
    "\n",
    "Now add cost as third objective to find solutions that balance accuracy, simplicity, AND cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSGA-II with performance, sparsity, and cost objectives\nrec_nsga2_3obj = recipe(train_country)\nrec_nsga2_3obj = step_select_genetic_algorithm(\n    rec_nsga2_3obj,\n    outcome=target,\n    model=linear_reg(),\n    metric='rmse',\n    \n    # Feature costs\n    feature_costs=feature_costs,\n    \n    # NSGA-II settings\n    use_nsga2=True,\n    nsga2_objectives=['performance', 'sparsity', 'cost'],\n    nsga2_selection_method='knee_point',  # Best 3-way trade-off\n    \n    # GA settings\n    population_size=50,\n    generations=30,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_nsga2_3obj = rec_nsga2_3obj.prep(train_country)\nselected_3obj = prepped_nsga2_3obj.prepared_steps[0]._selected_features\n\nprint(f\"\\n=== NSGA-II (Performance + Sparsity + Cost) ===\")\nprint(f\"Selected features ({len(selected_3obj)}): {selected_3obj}\")\n\n# Calculate cost of selected features\nselected_cost = sum(feature_costs.get(f, 0) for f in selected_3obj)\nprint(f\"Total cost: {selected_cost} (baseline: {total_cost_all_features})\")\nprint(f\"Cost reduction: {(1 - selected_cost/total_cost_all_features)*100:.1f}%\")\n\nprint(f\"\\nPareto front size: {len(prepped_nsga2_3obj.prepared_steps[0]._pareto_front)}\")\n\n# Evaluate on test set\nif len(selected_3obj) > 0:\n    train_selected = prepped_nsga2_3obj.bake(train_country)\n    test_selected = prepped_nsga2_3obj.bake(test_country)\n    \n    wf = workflow().add_formula(f'{target} ~ .').add_model(linear_reg())\n    fit = wf.fit(train_selected)\n    preds = fit.predict(test_selected)\n    \n    test_rmse_3obj = rmse(test_selected[target], preds['.pred']).iloc[0]['value']\n    test_mae_3obj = mae(test_selected[target], preds['.pred']).iloc[0]['value']\n    test_r2_3obj = r_squared(test_selected[target], preds['.pred']).iloc[0]['value']\n    \n    print(f\"\\nTest performance:\")\n    print(f\"  RMSE: {test_rmse_3obj:.4f} (baseline: {baseline_rmse:.4f})\")\n    print(f\"  MAE: {test_mae_3obj:.4f}\")\n    print(f\"  R\u00b2: {test_r2_3obj:.4f}\")\n    print(f\"  Features reduced: {len(feature_cols)} \u2192 {len(selected_3obj)}\")\n    print(f\"  Cost reduced: {total_cost_all_features} \u2192 {selected_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Different NSGA-II Selection Methods\n",
    "\n",
    "NSGA-II produces a Pareto front of solutions. Test different selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_methods = ['knee_point', 'min_features', 'best_performance']\nmethod_results = {}\n\nfor method in selection_methods:\n    print(f\"\\nTesting selection method: {method}\")\n    \n    rec = recipe(train_country)\n    rec = step_select_genetic_algorithm(\n        rec,\n        outcome=target,\n        model=linear_reg(),\n        metric='rmse',\n        feature_costs=feature_costs,\n        use_nsga2=True,\n        nsga2_objectives=['performance', 'sparsity', 'cost'],\n        nsga2_selection_method=method,\n        population_size=50,\n        generations=30,\n        cv_folds=3,\n        random_state=42,\n        verbose=False\n    )\n    \n    prepped = rec.prep(train_country)\n    selected = prepped.prepared_steps[0]._selected_features\n    \n    if len(selected) > 0:\n        selected_cost = sum(feature_costs.get(f, 0) for f in selected)\n        \n        train_selected = prepped.bake(train_country)\n        test_selected = prepped.bake(test_country)\n        \n        wf = workflow().add_formula(f'{target} ~ .').add_model(linear_reg())\n        fit = wf.fit(train_selected)\n        preds = fit.predict(test_selected)\n        \n        test_rmse = rmse(test_selected[target], preds['.pred']).iloc[0]['value']\n        test_r2 = r_squared(test_selected[target], preds['.pred']).iloc[0]['value']\n        \n        method_results[method] = {\n            'n_features': len(selected),\n            'features': selected,\n            'cost': selected_cost,\n            'rmse': test_rmse,\n            'r2': test_r2\n        }\n        \n        print(f\"  Selected {len(selected)} features (cost={selected_cost}): {selected}\")\n        print(f\"  RMSE: {test_rmse:.4f}, R\u00b2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\ncomparison_data = []\ncomparison_data.append({\n    'Method': 'Baseline (All Features)',\n    'N Features': len(feature_cols),\n    'Cost': total_cost_all_features,\n    'RMSE': baseline_rmse,\n    'R\u00b2': baseline_r2\n})\n\nfor method, metrics in method_results.items():\n    comparison_data.append({\n        'Method': f'NSGA-II ({method})',\n        'N Features': metrics['n_features'],\n        'Cost': metrics['cost'],\n        'RMSE': metrics['rmse'],\n        'R\u00b2': metrics['r2']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(\"\\n=== Performance Comparison ===\")\nprint(comparison_df.to_string(index=False))\n\n# Calculate improvements\nfor idx in range(1, len(comparison_df)):\n    rmse_improvement = (1 - comparison_df.loc[idx, 'RMSE'] / baseline_rmse) * 100\n    cost_reduction = (1 - comparison_df.loc[idx, 'Cost'] / total_cost_all_features) * 100\n    print(f\"\\n{comparison_df.loc[idx, 'Method']}:\")\n    print(f\"  RMSE change: {rmse_improvement:+.1f}%\")\n    print(f\"  Cost reduction: {cost_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Cost vs RMSE trade-off\nax1 = axes[0]\ncosts = comparison_df['Cost']\nrmses = comparison_df['RMSE']\nmethods = comparison_df['Method']\ncolors = ['red' if 'Baseline' in m else 'blue' for m in methods]\n\nfor i, (c, r, color, m) in enumerate(zip(costs, rmses, colors, methods)):\n    label = 'Baseline' if 'Baseline' in m else 'NSGA-II' if i == 1 else None\n    ax1.scatter(c, r, s=200, c=color, alpha=0.7, label=label, edgecolors='black', linewidths=1.5)\n    ax1.annotate(m.replace('NSGA-II (', '').replace(')', ''), \n                 (c, r), xytext=(5, 5), textcoords='offset points', \n                 fontsize=9, fontweight='bold')\n\nax1.set_xlabel('Total Feature Cost', fontsize=11)\nax1.set_ylabel('RMSE (lower is better)', fontsize=11)\nax1.set_title('Cost vs Performance Trade-off', fontsize=12, fontweight='bold')\nax1.grid(alpha=0.3)\nax1.legend(fontsize=10)\n\n# Plot 2: Multi-objective comparison (3D projected to 2D)\nax2 = axes[1]\nn_features = comparison_df['N Features']\n\n# Create bubble plot: x=features, y=RMSE, size=cost\nfor i, (n, r, c, m) in enumerate(zip(n_features, rmses, costs, methods)):\n    color = 'red' if 'Baseline' in m else 'blue'\n    label = 'Baseline' if 'Baseline' in m else 'NSGA-II' if i == 1 else None\n    ax2.scatter(n, r, s=c*10, c=color, alpha=0.5, label=label, edgecolors='black', linewidths=1.5)\n    ax2.annotate(m.replace('NSGA-II (', '').replace(')', ''), \n                 (n, r), xytext=(5, 5), textcoords='offset points', \n                 fontsize=8)\n\nax2.set_xlabel('Number of Features', fontsize=11)\nax2.set_ylabel('RMSE', fontsize=11)\nax2.set_title('Feature Count vs Performance\\n(bubble size = cost)', fontsize=12, fontweight='bold')\nax2.grid(alpha=0.3)\nax2.legend(fontsize=10)\n\nplt.tight_layout()\nplt.savefig('refinery_margins_nsga2_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nPlot saved as: refinery_margins_nsga2_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pareto Front Visualization\n",
    "\n",
    "Visualize the Pareto front of solutions from the 3-objective optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pareto front objectives\npareto_objectives = prepped_nsga2_3obj.prepared_steps[0]._pareto_objectives\n\nif pareto_objectives is not None and len(pareto_objectives) > 0:\n    # Objectives: [performance, sparsity, cost]\n    performance = pareto_objectives[:, 0]\n    sparsity = pareto_objectives[:, 1]\n    cost_obj = pareto_objectives[:, 2]\n    \n    # Create 2D projections of 3D Pareto front\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n    \n    # Plot 1: Performance vs Sparsity\n    ax1 = axes[0]\n    scatter1 = ax1.scatter(performance, sparsity, c=cost_obj, cmap='viridis', s=100, alpha=0.7)\n    ax1.set_xlabel('Performance (RMSE)', fontsize=11)\n    ax1.set_ylabel('Sparsity (# features)', fontsize=11)\n    ax1.set_title('Performance vs Sparsity', fontsize=12, fontweight='bold')\n    ax1.grid(alpha=0.3)\n    plt.colorbar(scatter1, ax=ax1, label='Cost')\n    \n    # Mark selected solution (knee point)\n    # Calculate knee point index (same method as the step uses internally)\ndef find_knee_point(objectives):\n    \"\"\"Find knee point using maximum distance from line connecting extremes.\"\"\"\n    if len(objectives) == 1:\n        return 0\n    \n    # Find extreme points\n    min_idx = np.argmin(objectives[:, 0])\n    max_idx = np.argmax(objectives[:, 1])\n    \n    if min_idx == max_idx:\n        return min_idx\n    \n    # Calculate distances from line\n    p1 = objectives[min_idx]\n    p2 = objectives[max_idx]\n    \n    max_dist = -1\n    knee_idx = 0\n    \n    for i in range(len(objectives)):\n        point = objectives[i]\n        # Perpendicular distance from point to line\n        dist = np.abs(np.cross(p2 - p1, point - p1)) / np.linalg.norm(p2 - p1)\n        if dist > max_dist:\n            max_dist = dist\n            knee_idx = i\n    \n    return knee_idx\n\nselected_idx = find_knee_point(pareto_objectives)\n    ax1.scatter(performance[selected_idx], sparsity[selected_idx], \n                s=300, c='red', marker='*', edgecolors='black', linewidths=2,\n                label='Selected (knee point)', zorder=10)\n    ax1.legend()\n    \n    # Plot 2: Performance vs Cost\n    ax2 = axes[1]\n    scatter2 = ax2.scatter(performance, cost_obj, c=sparsity, cmap='plasma', s=100, alpha=0.7)\n    ax2.set_xlabel('Performance (RMSE)', fontsize=11)\n    ax2.set_ylabel('Cost', fontsize=11)\n    ax2.set_title('Performance vs Cost', fontsize=12, fontweight='bold')\n    ax2.grid(alpha=0.3)\n    plt.colorbar(scatter2, ax=ax2, label='Sparsity')\n    \n    # Mark selected solution\n    ax2.scatter(performance[selected_idx], cost_obj[selected_idx], \n                s=300, c='red', marker='*', edgecolors='black', linewidths=2,\n                label='Selected (knee point)', zorder=10)\n    ax2.legend()\n    \n    # Plot 3: Sparsity vs Cost\n    ax3 = axes[2]\n    scatter3 = ax3.scatter(sparsity, cost_obj, c=performance, cmap='coolwarm', s=100, alpha=0.7)\n    ax3.set_xlabel('Sparsity (# features)', fontsize=11)\n    ax3.set_ylabel('Cost', fontsize=11)\n    ax3.set_title('Sparsity vs Cost', fontsize=12, fontweight='bold')\n    ax3.grid(alpha=0.3)\n    plt.colorbar(scatter3, ax=ax3, label='Performance')\n    \n    # Mark selected solution\n    ax3.scatter(sparsity[selected_idx], cost_obj[selected_idx], \n                s=300, c='red', marker='*', edgecolors='black', linewidths=2,\n                label='Selected (knee point)', zorder=10)\n    ax3.legend()\n    \n    plt.tight_layout()\n    plt.savefig('refinery_margins_pareto_front.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"\\nPlot saved as: refinery_margins_pareto_front.png\")\n    print(f\"\\nPareto front statistics:\")\n    print(f\"  Number of solutions: {len(performance)}\")\n    print(f\"  Performance range: [{performance.min():.4f}, {performance.max():.4f}]\")\n    print(f\"  Sparsity range: [{int(sparsity.min())}, {int(sparsity.max())}] features\")\n    print(f\"  Cost range: [{cost_obj.min():.0f}, {cost_obj.max():.0f}]\")\n    print(f\"\\nSelected solution (knee point):\")\n    print(f\"  Performance: {performance[selected_idx]:.4f}\")\n    print(f\"  Sparsity: {int(sparsity[selected_idx])} features\")\n    print(f\"  Cost: {cost_obj[selected_idx]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **NSGA-II Enables Multi-Objective Optimization**: Simultaneously optimize for performance, model simplicity, and data acquisition cost\n",
    "\n",
    "2. **Pareto Front Reveals Trade-offs**: No single \"best\" solution - choose based on priorities:\n",
    "   - `best_performance`: Highest accuracy (may use more features/cost)\n",
    "   - `min_features`: Simplest model (may sacrifice some accuracy)\n",
    "   - `knee_point`: Best overall trade-off (recommended)\n",
    "\n",
    "3. **Cost-Aware Feature Selection**: Incorporating feature costs helps identify economical feature subsets\n",
    "\n",
    "4. **Three-Objective Balance**: Adding cost objective finds solutions that are accurate, simple, AND economical\n",
    "\n",
    "5. **Real-World Applicability**: Refinery margin prediction benefits from balancing predictive power with operational constraints (data acquisition costs, model complexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}