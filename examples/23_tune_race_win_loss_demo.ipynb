{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tuning: Racing with Bradley-Terry Models (tune_race_win_loss)\n",
    "\n",
    "This notebook demonstrates **Bradley-Terry racing** for hyperparameter optimization using pairwise win/loss comparisons.\n",
    "\n",
    "## Key Benefits:\n",
    "- **Robust to outliers**: Uses win/loss counts instead of mean performance\n",
    "- **Pairwise comparisons**: Models all head-to-head matchups\n",
    "- **Probabilistic**: Estimates win probability for each config\n",
    "- **Handles ties**: Gracefully deals with equal performance\n",
    "\n",
    "## Bradley-Terry Algorithm:\n",
    "1. Evaluate all configs on first resample\n",
    "2. After each resample, fit Bradley-Terry model\n",
    "3. Estimate win probabilities for each config\n",
    "4. Eliminate configs with low win probability\n",
    "5. Continue with survivors only\n",
    "\n",
    "## When to use:\n",
    "- ✓ Data with **outliers** or **skewed distributions**\n",
    "- ✓ Prefer **rank-based** selection over mean-based\n",
    "- ✓ Want **probabilistic** interpretation of results\n",
    "- ✓ More **conservative** than ANOVA (fewer false eliminations)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import rand_forest, boost_tree, svm_rbf\n",
    "from py_rsample import vfold_cv\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_tune import (\n",
    "    tune, grid_regular, tune_grid,\n",
    "    tune_race_anova, tune_race_win_loss,\n",
    "    control_race\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../_md/__data/preem.csv')\n",
    "# Convert and save date range before dropping\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "date_min, date_max = df['date'].min(), df['date'].max()\n",
    "df = df.drop(columns=['date'])  # Drop date to avoid patsy categorical issues\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {date_min} to {date_max}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formula\n",
    "FORMULA = \"target ~ totaltar + mean_med_diesel_crack_input1_trade_month_lag2 + mean_nwe_hsfo_crack_trade_month_lag1 + mean_nwe_lsfo_crack_trade_month\"\n",
    "\n",
    "print(f\"Formula: {FORMULA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison: ANOVA vs Bradley-Terry Racing\n",
    "\n",
    "### 1.1 Setup Workflow and Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest workflow\n",
    "wf_rf = (\n",
    "    workflow()\n",
    "    .add_formula(FORMULA)\n",
    "    .add_model(\n",
    "        rand_forest(\n",
    "            mtry=tune(),\n",
    "            trees=tune(),\n",
    "            min_n=tune()\n",
    "        ).set_mode(\"regression\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Large grid\n",
    "rf_grid = grid_regular(\n",
    "    {\n",
    "        \"mtry\": {\"range\": (1, 4)},\n",
    "        \"trees\": {\"range\": (50, 500)},\n",
    "        \"min_n\": {\"range\": (2, 40)}\n",
    "    },\n",
    "    levels=5  # 125 combinations\n",
    ")\n",
    "\n",
    "print(f\"Grid size: {len(rf_grid)} configurations\")\n",
    "\n",
    "# CV folds\n",
    "cv_folds = vfold_cv(df, v=10)\n",
    "print(f\"CV folds: {len(cv_folds)}\")\n",
    "print(f\"Full grid would require: {len(rf_grid) * len(cv_folds)} model fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run ANOVA Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA racing control\n",
    "anova_ctrl = control_race(\n",
    "    burn_in=3,\n",
    "    num_ties=5,\n",
    "    alpha=0.05,\n",
    "    verbose_elim=True\n",
    ")\n",
    "\n",
    "print(\"Running ANOVA racing...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "anova_results = tune_race_anova(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse, mae),\n",
    "    control=anova_ctrl\n",
    ")\n",
    "\n",
    "anova_time = time.time() - start_time\n",
    "print(f\"\\n✓ ANOVA racing: {anova_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run Bradley-Terry Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bradley-Terry racing control\n",
    "bt_ctrl = control_race(\n",
    "    burn_in=3,\n",
    "    num_ties=5,\n",
    "    alpha=0.05,  # Win probability threshold\n",
    "    verbose_elim=True\n",
    ")\n",
    "\n",
    "print(\"Running Bradley-Terry racing...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "bt_results = tune_race_win_loss(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds,\n",
    "    grid=rf_grid,\n",
    "    metrics=metric_set(rmse, mae),\n",
    "    control=bt_ctrl\n",
    ")\n",
    "\n",
    "bt_time = time.time() - start_time\n",
    "print(f\"\\n✓ Bradley-Terry racing: {bt_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count evaluations\n",
    "anova_evals = len(anova_results.metrics[anova_results.metrics['metric'] == 'rmse'])\n",
    "bt_evals = len(bt_results.metrics[bt_results.metrics['metric'] == 'rmse'])\n",
    "full_evals = len(rf_grid) * len(cv_folds)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RACING METHOD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFull grid search:      {full_evals} evaluations\")\n",
    "print(f\"ANOVA racing:          {anova_evals} evaluations ({(1-anova_evals/full_evals)*100:.1f}% reduction)\")\n",
    "print(f\"Bradley-Terry racing:  {bt_evals} evaluations ({(1-bt_evals/full_evals)*100:.1f}% reduction)\")\n",
    "print(f\"\\nTiming:\")\n",
    "print(f\"ANOVA:                 {anova_time:.1f} seconds\")\n",
    "print(f\"Bradley-Terry:         {bt_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best configurations\n",
    "anova_best = anova_results.select_best(metric=\"rmse\", maximize=False)\n",
    "bt_best = bt_results.select_best(metric=\"rmse\", maximize=False)\n",
    "\n",
    "print(\"Best configurations found:\\n\")\n",
    "print(\"ANOVA racing:\")\n",
    "for param, value in anova_best.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nBradley-Terry racing:\")\n",
    "for param, value in bt_best.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "if anova_best == bt_best:\n",
    "    print(\"\\n✓ Both methods found the SAME winner!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Different winners found (both should be close in performance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 from each method\n",
    "print(\"Top 10 from ANOVA racing:\")\n",
    "print(anova_results.show_best(metric=\"rmse\", n=10, maximize=False))\n",
    "\n",
    "print(\"\\nTop 10 from Bradley-Terry racing:\")\n",
    "print(bt_results.show_best(metric=\"rmse\", n=10, maximize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bradley-Terry with Noisy Data\n",
    "\n",
    "Bradley-Terry racing is more robust to outliers. Let's test with noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to target variable\n",
    "df_noisy = df.copy()\n",
    "np.random.seed(42)\n",
    "# Add 10% random outliers (extreme values)\n",
    "n_outliers = int(0.1 * len(df_noisy))\n",
    "outlier_idx = np.random.choice(len(df_noisy), n_outliers, replace=False)\n",
    "df_noisy.loc[outlier_idx, 'target'] = df_noisy.loc[outlier_idx, 'target'] * np.random.uniform(2, 5, n_outliers)\n",
    "\n",
    "print(f\"Created noisy dataset with {n_outliers} outliers ({n_outliers/len(df_noisy)*100:.0f}%)\")\n",
    "print(f\"\\nOriginal target stats:\")\n",
    "print(df['target'].describe())\n",
    "print(f\"\\nNoisy target stats:\")\n",
    "print(df_noisy['target'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CV folds for noisy data\n",
    "cv_folds_noisy = vfold_cv(df_noisy, v=10)\n",
    "\n",
    "# Smaller grid for faster demo\n",
    "small_grid = grid_regular(\n",
    "    {\n",
    "        \"mtry\": {\"range\": (1, 4)},\n",
    "        \"trees\": {\"range\": (50, 300)},\n",
    "        \"min_n\": {\"range\": (5, 20)}\n",
    "    },\n",
    "    levels=3  # 27 combinations\n",
    ")\n",
    "\n",
    "print(f\"Testing with {len(small_grid)} configurations on noisy data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA on noisy data\n",
    "print(\"ANOVA racing on noisy data...\\n\")\n",
    "anova_noisy = tune_race_anova(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds_noisy,\n",
    "    grid=small_grid,\n",
    "    metrics=metric_set(rmse),\n",
    "    control=control_race(burn_in=2, alpha=0.05, verbose=False)\n",
    ")\n",
    "print(\"✓ ANOVA complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bradley-Terry on noisy data\n",
    "print(\"Bradley-Terry racing on noisy data...\\n\")\n",
    "bt_noisy = tune_race_win_loss(\n",
    "    wf_rf,\n",
    "    resamples=cv_folds_noisy,\n",
    "    grid=small_grid,\n",
    "    metrics=metric_set(rmse),\n",
    "    control=control_race(burn_in=2, alpha=0.05, verbose=False)\n",
    ")\n",
    "print(\"✓ Bradley-Terry complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare robustness\n",
    "anova_noisy_evals = len(anova_noisy.metrics)\n",
    "bt_noisy_evals = len(bt_noisy.metrics)\n",
    "\n",
    "print(\"\\nRobustness to outliers:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ANOVA kept evaluating:        {anova_noisy_evals} configs\")\n",
    "print(f\"Bradley-Terry kept evaluating: {bt_noisy_evals} configs\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if bt_noisy_evals > anova_noisy_evals:\n",
    "    print(\"✓ Bradley-Terry is more conservative (keeps more configs)\")\n",
    "    print(\"  → Better when means are affected by outliers\")\n",
    "    print(\"  → Pairwise comparisons more robust than mean-based tests\")\n",
    "else:\n",
    "    print(\"✓ ANOVA and Bradley-Terry showed similar behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost with Bradley-Terry Racing\n",
    "\n",
    "Test on a different model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost workflow\n",
    "wf_xgb = (\n",
    "    workflow()\n",
    "    .add_formula(FORMULA)\n",
    "    .add_model(\n",
    "        boost_tree(\n",
    "            trees=tune(),\n",
    "            tree_depth=tune(),\n",
    "            learn_rate=tune()\n",
    "        ).set_mode(\"regression\").set_engine(\"xgboost\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3D grid\n",
    "xgb_grid = grid_regular(\n",
    "    {\n",
    "        \"trees\": {\"range\": (50, 300)},\n",
    "        \"tree_depth\": {\"range\": (3, 10)},\n",
    "        \"learn_rate\": {\"range\": (0.01, 0.3), \"trans\": \"log\"}\n",
    "    },\n",
    "    levels=4  # 64 combinations\n",
    ")\n",
    "\n",
    "print(f\"XGBoost grid: {len(xgb_grid)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bradley-Terry racing on XGBoost\n",
    "print(\"Running Bradley-Terry racing on XGBoost...\\n\")\n",
    "xgb_bt_results = tune_race_win_loss(\n",
    "    wf_xgb,\n",
    "    resamples=cv_folds,\n",
    "    grid=xgb_grid,\n",
    "    metrics=metric_set(rmse, mae, r_squared),\n",
    "    control=control_race(burn_in=3, alpha=0.05, verbose_elim=True)\n",
    ")\n",
    "\n",
    "print(\"\\n✓ XGBoost Bradley-Terry racing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "print(\"Top 10 XGBoost configurations:\")\n",
    "xgb_bt_results.show_best(metric=\"rmse\", n=10, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency metrics\n",
    "xgb_evals = len(xgb_bt_results.metrics[xgb_bt_results.metrics['metric'] == 'rmse'])\n",
    "xgb_full = len(xgb_grid) * len(cv_folds)\n",
    "\n",
    "print(f\"\\nXGBoost efficiency:\")\n",
    "print(f\"  Full grid: {xgb_full} evaluations\")\n",
    "print(f\"  Racing: {xgb_evals} evaluations\")\n",
    "print(f\"  Reduction: {(1 - xgb_evals/xgb_full)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced: Custom Win Probability Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values (win probability thresholds)\n",
    "alphas = [0.01, 0.05, 0.10, 0.20]\n",
    "alpha_results = {}\n",
    "\n",
    "print(\"Testing different win probability thresholds...\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Alpha = {alpha}...\")\n",
    "    ctrl = control_race(burn_in=2, alpha=alpha, verbose=False)\n",
    "    \n",
    "    results = tune_race_win_loss(\n",
    "        wf_rf,\n",
    "        resamples=cv_folds,\n",
    "        grid=small_grid,\n",
    "        metrics=metric_set(rmse),\n",
    "        control=ctrl\n",
    "    )\n",
    "    \n",
    "    n_evals = len(results.metrics)\n",
    "    alpha_results[alpha] = n_evals\n",
    "    print(f\"  → {n_evals} evaluations\\n\")\n",
    "\n",
    "print(\"\\nAlpha comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for alpha, n in alpha_results.items():\n",
    "    print(f\"α = {alpha:5.2f}:  {n:4d} evaluations\")\n",
    "\n",
    "print(\"\\n✓ Lower alpha = more stringent = keep fewer configs\")\n",
    "print(\"✓ Higher alpha = more lenient = keep more configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary: When to Use Each Racing Method\n",
    "\n",
    "### ANOVA Racing (`tune_race_anova`):\n",
    "- ✓ **Clean data** with normal distributions\n",
    "- ✓ **Mean-based** performance important\n",
    "- ✓ **Faster** in most cases\n",
    "- ✓ **Well-established** statistical test\n",
    "\n",
    "### Bradley-Terry Racing (`tune_race_win_loss`):\n",
    "- ✓ **Noisy data** or **outliers** present\n",
    "- ✓ **Rank-based** selection preferred\n",
    "- ✓ **Probabilistic** interpretation needed\n",
    "- ✓ **More conservative** elimination\n",
    "- ✓ **Handles ties** naturally\n",
    "\n",
    "### Configuration Guidelines:\n",
    "\n",
    "**burn_in**:\n",
    "- 2-3: Aggressive (faster, riskier)\n",
    "- 4-5: Conservative (slower, safer)\n",
    "\n",
    "**alpha** (for Bradley-Terry = win probability threshold):\n",
    "- 0.01: Very stringent (keep many configs)\n",
    "- 0.05: Standard (balanced)\n",
    "- 0.10-0.20: Lenient (eliminate more)\n",
    "\n",
    "**num_ties**:\n",
    "- 3-5: Aggressive\n",
    "- 5-10: Standard\n",
    "- 10+: Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY: tune_race_win_loss()\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: {df.shape[0]} observations, {df.shape[1]} features\")\n",
    "print(f\"\\nRandom Forest comparison:\")\n",
    "print(f\"  ANOVA racing:        {anova_evals} evaluations in {anova_time:.1f}s\")\n",
    "print(f\"  Bradley-Terry racing: {bt_evals} evaluations in {bt_time:.1f}s\")\n",
    "print(f\"\\nKey advantages of Bradley-Terry:\")\n",
    "print(\"  ✓ Robust to outliers (uses win/loss, not means)\")\n",
    "print(\"  ✓ Probabilistic interpretation (win probabilities)\")\n",
    "print(\"  ✓ Handles ties naturally\")\n",
    "print(\"  ✓ More conservative (fewer false eliminations)\")\n",
    "print(f\"\\n✓ Both methods significantly faster than full grid search\")\n",
    "print(\"✓ Choose based on data characteristics and preferences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
