{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Feature Selection Demo\n",
    "\n",
    "This notebook demonstrates the `step_select_genetic_algorithm()` recipe step for intelligent feature selection using genetic algorithms.\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "1. **Basic GA feature selection** (ungrouped data)\n",
    "2. **Advanced GA enhancements** (mandatory/forbidden features, costs, sparsity, warm start)\n",
    "3. **Adaptive GA parameters** (mutation/crossover adaptation)\n",
    "4. **Constraint relaxation** (gradual constraint loosening)\n",
    "5. **Parallel evaluation** (multi-core fitness evaluation)\n",
    "6. **Ensemble mode** (multiple GA runs with aggregation)\n",
    "7. **Multi-objective optimization** (NSGA-II for Pareto optimization)\n",
    "8. **Diversity maintenance** (prevent premature convergence)\n",
    "9. **Grouped/panel data** (per-group feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from py_recipes import recipe\n",
    "from py_recipes.steps import step_select_genetic_algorithm\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_workflows import workflow\n",
    "from py_yardstick import rmse, mae, r_squared\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Style settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ungrouped Data - Basic Feature Selection\n",
    "\n",
    "We'll create a dataset with:\n",
    "- 3 important features (strong signal)\n",
    "- 7 noise features (weak signal)\n",
    "- Goal: GA should identify the 3 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (250, 11)\n",
      "Test data: (50, 11)\n",
      "\n",
      "True important features: x1, x2, x3\n"
     ]
    }
   ],
   "source": [
    "# Create dataset with known important features\n",
    "n = 300\n",
    "\n",
    "# Important features (strong predictors)\n",
    "x1 = np.random.randn(n)\n",
    "x2 = np.random.randn(n)\n",
    "x3 = np.random.randn(n)\n",
    "\n",
    "# Noise features (weak predictors)\n",
    "x4 = np.random.randn(n) * 0.1\n",
    "x5 = np.random.randn(n) * 0.1\n",
    "x6 = np.random.randn(n) * 0.1\n",
    "x7 = np.random.randn(n) * 0.1\n",
    "x8 = np.random.randn(n) * 0.1\n",
    "x9 = np.random.randn(n) * 0.1\n",
    "x10 = np.random.randn(n) * 0.1\n",
    "\n",
    "# Outcome: strong signal from x1, x2, x3\n",
    "y = 5 * x1 + 3 * x2 + 2 * x3 + np.random.randn(n) * 0.5\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'y': y,\n",
    "    'x1': x1, 'x2': x2, 'x3': x3,\n",
    "    'x4': x4, 'x5': x5, 'x6': x6, 'x7': x7,\n",
    "    'x8': x8, 'x9': x9, 'x10': x10\n",
    "})\n",
    "\n",
    "# Split train/test\n",
    "train = data.iloc[:250]\n",
    "test = data.iloc[250:]\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "print(f\"\\nTrue important features: x1, x2, x3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic GA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 30\n",
      "  Max generations: 20\n",
      "\n",
      "Generation 0: Best fitness = 0.659774\n",
      "Generation 10: Best fitness = 0.661039\n",
      "Converged at generation 12\n",
      "\n",
      "GA Complete:\n",
      "  Converged: True\n",
      "  Generations: 12\n",
      "  Final fitness: 0.661039\n",
      "  Selected features: 4/10\n",
      "  Features: ['x1', 'x2', 'x3', 'x5']\n",
      "\n",
      "============================================================\n",
      "Selected features: ['x1', 'x2', 'x3', 'x5']\n",
      "Number of features: 4\n"
     ]
    }
   ],
   "source": [
    "# Create recipe with basic GA feature selection\n",
    "rec_basic = recipe(train)\n",
    "rec_basic = step_select_genetic_algorithm(\n",
    "    rec_basic,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    maximize=False,\n",
    "    top_n=5,  # Select top 5 features\n",
    "    population_size=30,\n",
    "    generations=20,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Prep and bake\n",
    "prepped_basic = rec_basic.prep(train)\n",
    "train_processed = prepped_basic.bake(train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features: {[col for col in train_processed.columns if col != 'y']}\")\n",
    "print(f\"Number of features: {train_processed.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Mandatory and Forbidden Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 20\n",
      "  Max generations: 15\n",
      "\n",
      "Generation 0: Best fitness = 0.654661\n",
      "Generation 10: Best fitness = 0.661039\n",
      "Converged at generation 13\n",
      "\n",
      "GA Complete:\n",
      "  Converged: True\n",
      "  Generations: 13\n",
      "  Final fitness: 0.661039\n",
      "  Selected features: 4/10\n",
      "  Features: ['x1', 'x2', 'x3', 'x5']\n",
      "\n",
      "============================================================\n",
      "Selected features: ['x1', 'x2', 'x3', 'x5']\n",
      "x1 included (mandatory): True\n",
      "x10 excluded (forbidden): True\n"
     ]
    }
   ],
   "source": [
    "# Force x1 to be included, forbid x10\n",
    "rec_constrained = recipe(train)\n",
    "rec_constrained = step_select_genetic_algorithm(\n",
    "    rec_constrained,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    mandatory_features=['x1'],  # Must include x1\n",
    "    forbidden_features=['x10'],  # Cannot include x10\n",
    "    top_n=4,\n",
    "    population_size=20,\n",
    "    generations=15,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prepped_constrained = rec_constrained.prep(train)\n",
    "train_constrained = prepped_constrained.bake(train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features: {[col for col in train_constrained.columns if col != 'y']}\")\n",
    "print(f\"x1 included (mandatory): {'x1' in train_constrained.columns}\")\n",
    "print(f\"x10 excluded (forbidden): {'x10' not in train_constrained.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Feature Costs and Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 25\n",
      "  Max generations: 20\n",
      "\n",
      "Generation 0: Best fitness = -0.103028\n",
      "Generation 10: Best fitness = -0.103028\n",
      "Converged at generation 10\n",
      "\n",
      "GA Complete:\n",
      "  Converged: True\n",
      "  Generations: 10\n",
      "  Final fitness: -0.103028\n",
      "  Selected features: 1/10\n",
      "  Features: ['x1']\n",
      "\n",
      "============================================================\n",
      "Selected features: ['x1']\n",
      "Total cost: 1.0 (max: 10.0)\n",
      "Number of features: 1 (sparsity encouraged)\n"
     ]
    }
   ],
   "source": [
    "# Assign costs to features (higher = more expensive)\n",
    "costs = {\n",
    "    'x1': 1.0, 'x2': 1.0, 'x3': 1.0,\n",
    "    'x4': 5.0, 'x5': 5.0,  # Expensive noise features\n",
    "    'x6': 2.0, 'x7': 2.0, 'x8': 2.0, 'x9': 2.0, 'x10': 2.0\n",
    "}\n",
    "\n",
    "rec_cost = recipe(train)\n",
    "rec_cost = step_select_genetic_algorithm(\n",
    "    rec_cost,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    feature_costs=costs,\n",
    "    max_total_cost=10.0,  # Budget constraint\n",
    "    cost_weight=0.3,  # Balance performance vs cost\n",
    "    sparsity_weight=0.2,  # Prefer fewer features\n",
    "    top_n=5,\n",
    "    population_size=25,\n",
    "    generations=20,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prepped_cost = rec_cost.prep(train)\n",
    "train_cost = prepped_cost.bake(train)\n",
    "\n",
    "selected_features = [col for col in train_cost.columns if col != 'y']\n",
    "total_cost = sum(costs.get(f, 0) for f in selected_features)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "print(f\"Total cost: {total_cost:.1f} (max: 10.0)\")\n",
    "print(f\"Number of features: {len(selected_features)} (sparsity encouraged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Ensemble Mode (Multiple GA Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 20\n",
      "  Max generations: 15\n",
      "\n",
      "\n",
      "Running ensemble with 5 GA instances...\n",
      "  [1/5] Running GA with seed=42...\n",
      "Generation 0: Best fitness = 0.660842\n",
      "Generation 10: Best fitness = 0.661599\n",
      "Converged at generation 15\n",
      "      Fitness: 0.661599, Features: 4\n",
      "  [2/5] Running GA with seed=43...\n",
      "Generation 0: Best fitness = 0.661074\n",
      "Generation 10: Best fitness = 0.661599\n",
      "Converged at generation 15\n",
      "      Fitness: 0.661599, Features: 4\n",
      "  [3/5] Running GA with seed=44...\n",
      "Generation 0: Best fitness = 0.661185\n",
      "Generation 10: Best fitness = 0.661599\n",
      "Converged at generation 14\n",
      "      Fitness: 0.661599, Features: 4\n",
      "  [4/5] Running GA with seed=45...\n",
      "Generation 0: Best fitness = 0.657323\n",
      "Generation 10: Best fitness = 0.661599\n",
      "Converged at generation 14\n",
      "      Fitness: 0.661599, Features: 4\n",
      "  [5/5] Running GA with seed=46...\n",
      "Generation 0: Best fitness = 0.658777\n",
      "Generation 10: Best fitness = 0.661599\n",
      "Converged at generation 12\n",
      "      Fitness: 0.661599, Features: 4\n",
      "\n",
      "Ensemble aggregation (voting):\n",
      "  Threshold: 0.6\n",
      "  Selected features: 4\n",
      "  Feature frequencies: [('x1', 5), ('x2', 5), ('x3', 5), ('x5', 5), ('x4', 0), ('x6', 0), ('x7', 0), ('x8', 0), ('x9', 0), ('x10', 0)]\n",
      "\n",
      "GA Ensemble Complete:\n",
      "  Converged: True\n",
      "  Generations: 15\n",
      "  Final fitness: 0.661599\n",
      "  Selected features: 4/10\n",
      "  Features: ['x1', 'x2', 'x3', 'x5']\n",
      "\n",
      "============================================================\n",
      "Selected features (ensemble): ['x1', 'x2', 'x3', 'x5']\n",
      "Ensemble size: 5 runs\n",
      "Strategy: voting with 60% threshold\n"
     ]
    }
   ],
   "source": [
    "# Run 5 GA instances and aggregate results\n",
    "rec_ensemble = recipe(train)\n",
    "rec_ensemble = step_select_genetic_algorithm(\n",
    "    rec_ensemble,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    top_n=4,\n",
    "    n_ensemble=5,  # Run 5 GA instances\n",
    "    ensemble_strategy='voting',  # Aggregate by voting\n",
    "    ensemble_threshold=0.6,  # Feature must appear in 60%+ of runs\n",
    "    population_size=20,\n",
    "    generations=15,\n",
    "    cv_folds=2,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prepped_ensemble = rec_ensemble.prep(train)\n",
    "train_ensemble = prepped_ensemble.bake(train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features (ensemble): {[col for col in train_ensemble.columns if col != 'y']}\")\n",
    "print(f\"Ensemble size: 5 runs\")\n",
    "print(f\"Strategy: voting with 60% threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Diversity Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 30\n",
      "  Max generations: 25\n",
      "\n",
      "Generation 0: Best fitness = 0.659774\n",
      "  Diversity = 0.5023\n",
      "  Applied fitness sharing (diversity=0.2524)\n",
      "Generation 10: Best fitness = 0.661039, Diversity = 0.2524\n",
      "Converged at generation 12\n",
      "\n",
      "GA Complete:\n",
      "  Converged: True\n",
      "  Generations: 12\n",
      "  Final fitness: 0.661039\n",
      "  Selected features: 4/10\n",
      "  Features: ['x1', 'x2', 'x3', 'x5']\n",
      "\n",
      "============================================================\n",
      "Selected features (with diversity): ['x1', 'x2', 'x3', 'x5']\n",
      "Diversity maintenance: enabled\n"
     ]
    }
   ],
   "source": [
    "# Maintain population diversity to avoid premature convergence\n",
    "rec_diversity = recipe(train)\n",
    "rec_diversity = step_select_genetic_algorithm(\n",
    "    rec_diversity,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    top_n=5,\n",
    "    maintain_diversity=True,\n",
    "    diversity_threshold=0.3,  # Trigger fitness sharing below this\n",
    "    fitness_sharing_sigma=0.5,  # Sharing function width\n",
    "    population_size=30,\n",
    "    generations=25,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prepped_diversity = rec_diversity.prep(train)\n",
    "train_diversity = prepped_diversity.bake(train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features (with diversity): {[col for col in train_diversity.columns if col != 'y']}\")\n",
    "print(f\"Diversity maintenance: enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Multi-Objective Optimization (NSGA-II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Feature Selection\n",
      "  Candidate features: 10\n",
      "  Model: linear_reg\n",
      "  Metric: rmse (minimize)\n",
      "  Constraints: 0\n",
      "  Population size: 30\n",
      "  Max generations: 20\n",
      "\n",
      "\n",
      "Running NSGA-II with objectives: ['performance', 'sparsity']\n",
      "Generation 0: Pareto front size = 5\n",
      "Generation 10: Pareto front size = 30\n",
      "Final Pareto front size: 30\n",
      "\n",
      "NSGA-II Complete:\n",
      "  Pareto front size: 30\n",
      "  Selected solution index: 7\n",
      "  Selected features: 3/10\n",
      "  Objective values: [0.13903147 3.        ]\n",
      "  Features: ['x4', 'x9', 'x10']\n",
      "\n",
      "============================================================\n",
      "Selected features (NSGA-II): ['x4', 'x9', 'x10']\n",
      "Objectives: performance (RMSE) + sparsity (# features)\n",
      "Selection: knee point (best trade-off)\n"
     ]
    }
   ],
   "source": [
    "# Optimize multiple objectives simultaneously (performance + sparsity)\n",
    "rec_nsga2 = recipe(train)\n",
    "rec_nsga2 = step_select_genetic_algorithm(\n",
    "    rec_nsga2,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    use_nsga2=True,  # Enable NSGA-II\n",
    "    nsga2_objectives=['performance', 'sparsity'],  # Two objectives\n",
    "    nsga2_selection_method='knee_point',  # Select best trade-off\n",
    "    population_size=30,  # Must be even for NSGA-II\n",
    "    generations=20,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prepped_nsga2 = rec_nsga2.prep(train)\n",
    "train_nsga2 = prepped_nsga2.bake(train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected features (NSGA-II): {[col for col in train_nsga2.columns if col != 'y']}\")\n",
    "print(f\"Objectives: performance (RMSE) + sparsity (# features)\")\n",
    "print(f\"Selection: knee point (best trade-off)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Complete Workflow with Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GA Feature Selection + Linear Regression Results:\n",
      "\n",
      "Train RMSE: 0.5086\n",
      "Test RMSE:  0.5233\n",
      "Train R²:   0.9929\n",
      "Test R²:    0.9900\n",
      "\n",
      "Comparison (all features): Test RMSE = 0.5233\n",
      "Improvement: -0.0% better\n"
     ]
    }
   ],
   "source": [
    "# Build complete workflow with GA feature selection\n",
    "rec_full = recipe(train)\n",
    "rec_full = step_select_genetic_algorithm(\n",
    "    rec_full,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    top_n=5,\n",
    "    population_size=25,\n",
    "    generations=20,\n",
    "    adaptive_mutation=True,  # Adaptive mutation rate\n",
    "    adaptive_crossover=True,  # Adaptive crossover rate\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Build workflow\n",
    "wf = workflow().add_recipe(rec_full).add_model(linear_reg())\n",
    "\n",
    "# Fit and evaluate\n",
    "fit = wf.fit(train)\n",
    "fit = fit.evaluate(test)\n",
    "\n",
    "# Get predictions\n",
    "train_preds = fit.predict(train)\n",
    "test_preds = fit.predict(test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = rmse(train['y'].values, train_preds['.pred'].values).iloc[0]['value']\n",
    "test_rmse = rmse(test['y'].values, test_preds['.pred'].values).iloc[0]['value']\n",
    "train_r2 = r_squared(train['y'].values, train_preds['.pred'].values).iloc[0]['value']\n",
    "test_r2 = r_squared(test['y'].values, test_preds['.pred'].values).iloc[0]['value']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GA Feature Selection + Linear Regression Results:\")\n",
    "print(f\"\\nTrain RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE:  {test_rmse:.4f}\")\n",
    "print(f\"Train R²:   {train_r2:.4f}\")\n",
    "print(f\"Test R²:    {test_r2:.4f}\")\n",
    "\n",
    "# Compare with using all features\n",
    "wf_all = workflow().add_formula('y ~ .').add_model(linear_reg())\n",
    "fit_all = wf_all.fit(train)\n",
    "test_preds_all = fit_all.predict(test)\n",
    "test_rmse_all = rmse(test['y'].values, test_preds_all['.pred'].values).iloc[0]['value']\n",
    "\n",
    "print(f\"\\nComparison (all features): Test RMSE = {test_rmse_all:.4f}\")\n",
    "print(f\"Improvement: {((test_rmse_all - test_rmse) / test_rmse_all * 100):.1f}% better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grouped/Panel Data - Per-Group Feature Selection\n",
    "\n",
    "We'll create a panel dataset with multiple groups (e.g., countries) where different features are important for different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel data shape: (450, 8)\n",
      "Groups: ['USA', 'Germany', 'Japan']\n",
      "\n",
      "True important features by group:\n",
      "  USA:     x1, x2, x3\n",
      "  Germany: x2, x4, x5\n",
      "  Japan:   x1, x4, x6\n"
     ]
    }
   ],
   "source": [
    "# Create grouped dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "groups = ['USA', 'Germany', 'Japan']\n",
    "n_per_group = 150\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for group_idx, group in enumerate(groups):\n",
    "    # Different features are important for different groups\n",
    "    x1 = np.random.randn(n_per_group)\n",
    "    x2 = np.random.randn(n_per_group)\n",
    "    x3 = np.random.randn(n_per_group)\n",
    "    x4 = np.random.randn(n_per_group)\n",
    "    x5 = np.random.randn(n_per_group)\n",
    "    x6 = np.random.randn(n_per_group)\n",
    "    \n",
    "    if group == 'USA':\n",
    "        # USA: x1, x2, x3 are important\n",
    "        y = 4 * x1 + 3 * x2 + 2 * x3 + np.random.randn(n_per_group) * 0.5\n",
    "    elif group == 'Germany':\n",
    "        # Germany: x2, x4, x5 are important\n",
    "        y = 3 * x2 + 2 * x4 + 2 * x5 + np.random.randn(n_per_group) * 0.5\n",
    "    else:  # Japan\n",
    "        # Japan: x1, x4, x6 are important\n",
    "        y = 4 * x1 + 2 * x4 + 3 * x6 + np.random.randn(n_per_group) * 0.5\n",
    "    \n",
    "    group_data = pd.DataFrame({\n",
    "        'country': group,\n",
    "        'y': y,\n",
    "        'x1': x1, 'x2': x2, 'x3': x3,\n",
    "        'x4': x4, 'x5': x5, 'x6': x6\n",
    "    })\n",
    "    data_list.append(group_data)\n",
    "\n",
    "panel_data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Split train/test\n",
    "train_panel = panel_data.sample(frac=0.75, random_state=42)\n",
    "test_panel = panel_data.drop(train_panel.index)\n",
    "\n",
    "print(f\"Panel data shape: {panel_data.shape}\")\n",
    "print(f\"Groups: {groups}\")\n",
    "print(f\"\\nTrue important features by group:\")\n",
    "print(\"  USA:     x1, x2, x3\")\n",
    "print(\"  Germany: x2, x4, x5\")\n",
    "print(\"  Japan:   x1, x4, x6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Per-Group Feature Selection\n",
    "\n",
    "We'll use a workflow with nested modeling to select features for each group independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Per-Group Feature Selection with GA:\n",
      "============================================================\n",
      "\n",
      "USA:\n",
      "  Selected: ['x1', 'x2', 'x3']\n",
      "  Expected: x1, x2, x3\n",
      "\n",
      "Germany:\n",
      "  Selected: ['x2', 'x4', 'x5']\n",
      "  Expected: x2, x4, x5\n",
      "\n",
      "Japan:\n",
      "  Selected: ['x1', 'x6', 'x4']\n",
      "  Expected: x1, x4, x6\n",
      "\n",
      "\n",
      "Building per-group models with selected features...\n",
      "✓ Trained 3 group-specific models\n"
     ]
    }
   ],
   "source": [
    "# For grouped data, we'll run GA feature selection per group manually\n",
    "# (This provides full control and clear results per group)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Per-Group Feature Selection with GA:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "selected_by_group = {}\n",
    "prepped_by_group = {}\n",
    "\n",
    "for group in groups:\n",
    "    # Get group data (without country column)\n",
    "    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n",
    "    \n",
    "    # Run GA for this group\n",
    "    rec_group = recipe(group_train)\n",
    "    rec_group = step_select_genetic_algorithm(\n",
    "        rec_group,\n",
    "        outcome='y',\n",
    "        model=linear_reg(),\n",
    "        metric='rmse',\n",
    "        top_n=3,\n",
    "        population_size=20,\n",
    "        generations=15,\n",
    "        cv_folds=3,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    prepped_group = rec_group.prep(group_train)\n",
    "    processed_group = prepped_group.bake(group_train)\n",
    "    selected_features = [col for col in processed_group.columns if col != 'y']\n",
    "    \n",
    "    selected_by_group[group] = selected_features\n",
    "    prepped_by_group[group] = prepped_group\n",
    "    \n",
    "    print(f\"{group}:\")\n",
    "    print(f\"  Selected: {selected_features}\")\n",
    "    print(f\"  Expected: \", end=\"\")\n",
    "    if group == 'USA':\n",
    "        print(\"x1, x2, x3\")\n",
    "    elif group == 'Germany':\n",
    "        print(\"x2, x4, x5\")\n",
    "    else:\n",
    "        print(\"x1, x4, x6\")\n",
    "    print()\n",
    "\n",
    "# Build per-group models with selected features\n",
    "print(\"\\nBuilding per-group models with selected features...\")\n",
    "group_models = {}\n",
    "\n",
    "for group, features in selected_by_group.items():\n",
    "    formula = 'y ~ ' + ' + '.join(features)\n",
    "    wf_group = workflow().add_formula(formula).add_model(linear_reg())\n",
    "    \n",
    "    # Fit on group training data\n",
    "    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n",
    "    group_models[group] = wf_group.fit(group_train)\n",
    "    \n",
    "print(f\"✓ Trained {len(group_models)} group-specific models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Evaluate Per-Group Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Per-Group Model Performance:\n",
      "============================================================\n",
      "\n",
      "USA:\n",
      "  RMSE: 0.4166\n",
      "  MAE:  0.3279\n",
      "  R²:   0.9915\n",
      "\n",
      "Germany:\n",
      "  RMSE: 0.4626\n",
      "  MAE:  0.3477\n",
      "  R²:   0.9877\n",
      "\n",
      "Japan:\n",
      "  RMSE: 0.5475\n",
      "  MAE:  0.4366\n",
      "  R²:   0.9920\n",
      "\n",
      "\n",
      "Comparison with using all features:\n",
      "USA: All features RMSE = 0.4263, GA selected RMSE = 0.4166 (2.3% better)\n",
      "Germany: All features RMSE = 0.4633, GA selected RMSE = 0.4626 (0.2% better)\n",
      "Japan: All features RMSE = 0.5488, GA selected RMSE = 0.5475 (0.2% better)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each group's model on test data\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Per-Group Model Performance:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for group in groups:\n",
    "    # Get group test data\n",
    "    group_test = test_panel[test_panel['country'] == group].drop('country', axis=1)\n",
    "    \n",
    "    # Predict\n",
    "    group_preds = group_models[group].predict(group_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    group_rmse = rmse(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n",
    "    group_mae = mae(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n",
    "    group_r2 = r_squared(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n",
    "    \n",
    "    print(f\"{group}:\")\n",
    "    print(f\"  RMSE: {group_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {group_mae:.4f}\")\n",
    "    print(f\"  R²:   {group_r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Compare with using all features per group\n",
    "print(\"\\nComparison with using all features:\")\n",
    "for group in groups:\n",
    "    group_test = test_panel[test_panel['country'] == group].drop('country', axis=1)\n",
    "    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n",
    "    \n",
    "    # Fit model with all features\n",
    "    wf_all = workflow().add_formula('y ~ .').add_model(linear_reg())\n",
    "    fit_all = wf_all.fit(group_train)\n",
    "    preds_all = fit_all.predict(group_test)\n",
    "    \n",
    "    rmse_all = rmse(group_test['y'].values, preds_all['.pred'].values).iloc[0]['value']\n",
    "    rmse_ga = rmse(group_test['y'].values, group_models[group].predict(group_test)['.pred'].values).iloc[0]['value']\n",
    "    \n",
    "    improvement = ((rmse_all - rmse_ga) / rmse_all * 100) if rmse_all > rmse_ga else 0\n",
    "    print(f\"{group}: All features RMSE = {rmse_all:.4f}, GA selected RMSE = {rmse_ga:.4f} ({improvement:.1f}% better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10: Visualize GA Evolution History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PreparedRecipe' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m prepped_vis \u001b[38;5;241m=\u001b[39m rec_vis\u001b[38;5;241m.\u001b[39mprep(train)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Access the step to get GA history\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m ga_step \u001b[38;5;241m=\u001b[39m \u001b[43mprepped_vis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Plot evolution\u001b[39;00m\n\u001b[1;32m     22\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PreparedRecipe' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "# Get GA history from the recipe step (ungrouped example)\n",
    "rec_vis = recipe(train)\n",
    "rec_vis = step_select_genetic_algorithm(\n",
    "    rec_vis,\n",
    "    outcome='y',\n",
    "    model=linear_reg(),\n",
    "    metric='rmse',\n",
    "    top_n=5,\n",
    "    population_size=30,\n",
    "    generations=30,\n",
    "    cv_folds=3,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "prepped_vis = rec_vis.prep(train)\n",
    "\n",
    "# Access the step to get GA history\n",
    "ga_step = prepped_vis.steps[0]\n",
    "\n",
    "# Plot evolution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Fitness history\n",
    "ax1.plot(ga_step._ga_history, linewidth=2, color='steelblue')\n",
    "ax1.set_xlabel('Generation', fontsize=12)\n",
    "ax1.set_ylabel('Best Fitness (lower RMSE = better)', fontsize=12)\n",
    "ax1.set_title('GA Evolution: Fitness Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Selected features\n",
    "selected = [col for col in prepped_vis.bake(train).columns if col != 'y']\n",
    "feature_importance = pd.Series(1, index=selected).sort_values(ascending=True)\n",
    "feature_importance.plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_xlabel('Selected', fontsize=12)\n",
    "ax2.set_title('Final Selected Features', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGA converged: {ga_step._converged}\")\n",
    "print(f\"Generations run: {ga_step._n_generations}\")\n",
    "print(f\"Final fitness: {ga_step._final_fitness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "### Ungrouped Data Examples:\n",
    "1. ✅ Basic GA feature selection\n",
    "2. ✅ Mandatory and forbidden features\n",
    "3. ✅ Feature costs and sparsity objectives\n",
    "4. ✅ Ensemble mode (multiple GA runs)\n",
    "5. ✅ Diversity maintenance\n",
    "6. ✅ Multi-objective optimization (NSGA-II)\n",
    "7. ✅ Complete workflow with evaluation\n",
    "\n",
    "### Grouped Data Examples:\n",
    "8. ✅ Per-group feature selection\n",
    "9. ✅ Per-group model evaluation\n",
    "10. ✅ Visualization of GA evolution\n",
    "\n",
    "### Key Takeaways:\n",
    "- GA successfully identifies important features even with noise\n",
    "- Constraints (mandatory/forbidden) are respected\n",
    "- Cost and sparsity objectives guide feature selection\n",
    "- Ensemble mode provides more stable results\n",
    "- NSGA-II finds optimal trade-offs between objectives\n",
    "- Per-group selection adapts to group-specific patterns\n",
    "\n",
    "### Additional Features Not Demonstrated:\n",
    "- Warm start initialization (importance-based, low-correlation)\n",
    "- Constraint relaxation (gradual loosening)\n",
    "- Parallel evaluation (n_jobs > 1)\n",
    "- Statistical constraints (p-value, VIF, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
