{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Feature Selection Demo\n",
    "\n",
    "This notebook demonstrates the `step_select_genetic_algorithm()` recipe step for intelligent feature selection using genetic algorithms.\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "1. **Basic GA feature selection** (ungrouped data)\n",
    "2. **Advanced GA enhancements** (mandatory/forbidden features, costs, sparsity, warm start)\n",
    "3. **Adaptive GA parameters** (mutation/crossover adaptation)\n",
    "4. **Constraint relaxation** (gradual constraint loosening)\n",
    "5. **Parallel evaluation** (multi-core fitness evaluation)\n",
    "6. **Ensemble mode** (multiple GA runs with aggregation)\n",
    "7. **Multi-objective optimization** (NSGA-II for Pareto optimization)\n",
    "8. **Diversity maintenance** (prevent premature convergence)\n",
    "9. **Grouped/panel data** (per-group feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from py_recipes import recipe\n",
    "from py_recipes.steps import step_select_genetic_algorithm\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_workflows import workflow\n",
    "from py_yardstick import rmse, mae, r_squared\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Style settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ungrouped Data - Basic Feature Selection\n",
    "\n",
    "We'll create a dataset with:\n",
    "- 3 important features (strong signal)\n",
    "- 7 noise features (weak signal)\n",
    "- Goal: GA should identify the 3 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with known important features\n",
    "n = 300\n",
    "\n",
    "# Important features (strong predictors)\n",
    "x1 = np.random.randn(n)\n",
    "x2 = np.random.randn(n)\n",
    "x3 = np.random.randn(n)\n",
    "\n",
    "# Noise features (weak predictors)\n",
    "x4 = np.random.randn(n) * 0.1\n",
    "x5 = np.random.randn(n) * 0.1\n",
    "x6 = np.random.randn(n) * 0.1\n",
    "x7 = np.random.randn(n) * 0.1\n",
    "x8 = np.random.randn(n) * 0.1\n",
    "x9 = np.random.randn(n) * 0.1\n",
    "x10 = np.random.randn(n) * 0.1\n",
    "\n",
    "# Outcome: strong signal from x1, x2, x3\n",
    "y = 5 * x1 + 3 * x2 + 2 * x3 + np.random.randn(n) * 0.5\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'y': y,\n",
    "    'x1': x1, 'x2': x2, 'x3': x3,\n",
    "    'x4': x4, 'x5': x5, 'x6': x6, 'x7': x7,\n",
    "    'x8': x8, 'x9': x9, 'x10': x10\n",
    "})\n",
    "\n",
    "# Split train/test\n",
    "train = data.iloc[:250]\n",
    "test = data.iloc[250:]\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "print(f\"\\nTrue important features: x1, x2, x3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic GA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create recipe with basic GA feature selection\nrec_basic = recipe(train)\nrec_basic = step_select_genetic_algorithm(\n    rec_basic,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    maximize=False,\n    top_n=5,  # Select top 5 features\n    population_size=30,\n    generations=20,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\n# Prep and bake\nprepped_basic = rec_basic.prep(train)\ntrain_processed = prepped_basic.bake(train)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features: {[col for col in train_processed.columns if col != 'y']}\")\nprint(f\"Number of features: {train_processed.shape[1] - 1}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Mandatory and Forbidden Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Force x1 to be included, forbid x10\nrec_constrained = recipe(train)\nrec_constrained = step_select_genetic_algorithm(\n    rec_constrained,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    mandatory_features=['x1'],  # Must include x1\n    forbidden_features=['x10'],  # Cannot include x10\n    top_n=4,\n    population_size=20,\n    generations=15,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_constrained = rec_constrained.prep(train)\ntrain_constrained = prepped_constrained.bake(train)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features: {[col for col in train_constrained.columns if col != 'y']}\")\nprint(f\"x1 included (mandatory): {'x1' in train_constrained.columns}\")\nprint(f\"x10 excluded (forbidden): {'x10' not in train_constrained.columns}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Feature Costs and Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Assign costs to features (higher = more expensive)\ncosts = {\n    'x1': 1.0, 'x2': 1.0, 'x3': 1.0,\n    'x4': 5.0, 'x5': 5.0,  # Expensive noise features\n    'x6': 2.0, 'x7': 2.0, 'x8': 2.0, 'x9': 2.0, 'x10': 2.0\n}\n\nrec_cost = recipe(train)\nrec_cost = step_select_genetic_algorithm(\n    rec_cost,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    feature_costs=costs,\n    max_total_cost=10.0,  # Budget constraint\n    cost_weight=0.3,  # Balance performance vs cost\n    sparsity_weight=0.2,  # Prefer fewer features\n    top_n=5,\n    population_size=25,\n    generations=20,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_cost = rec_cost.prep(train)\ntrain_cost = prepped_cost.bake(train)\n\nselected_features = [col for col in train_cost.columns if col != 'y']\ntotal_cost = sum(costs.get(f, 0) for f in selected_features)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features: {selected_features}\")\nprint(f\"Total cost: {total_cost:.1f} (max: 10.0)\")\nprint(f\"Number of features: {len(selected_features)} (sparsity encouraged)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Ensemble Mode (Multiple GA Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run 5 GA instances and aggregate results\nrec_ensemble = recipe(train)\nrec_ensemble = step_select_genetic_algorithm(\n    rec_ensemble,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    top_n=4,\n    n_ensemble=5,  # Run 5 GA instances\n    ensemble_strategy='voting',  # Aggregate by voting\n    ensemble_threshold=0.6,  # Feature must appear in 60%+ of runs\n    population_size=20,\n    generations=15,\n    cv_folds=2,\n    random_state=42,\n    verbose=True\n)\n\nprepped_ensemble = rec_ensemble.prep(train)\ntrain_ensemble = prepped_ensemble.bake(train)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features (ensemble): {[col for col in train_ensemble.columns if col != 'y']}\")\nprint(f\"Ensemble size: 5 runs\")\nprint(f\"Strategy: voting with 60% threshold\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Diversity Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Maintain population diversity to avoid premature convergence\nrec_diversity = recipe(train)\nrec_diversity = step_select_genetic_algorithm(\n    rec_diversity,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    top_n=5,\n    maintain_diversity=True,\n    diversity_threshold=0.3,  # Trigger fitness sharing below this\n    fitness_sharing_sigma=0.5,  # Sharing function width\n    population_size=30,\n    generations=25,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_diversity = rec_diversity.prep(train)\ntrain_diversity = prepped_diversity.bake(train)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features (with diversity): {[col for col in train_diversity.columns if col != 'y']}\")\nprint(f\"Diversity maintenance: enabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Multi-Objective Optimization (NSGA-II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimize multiple objectives simultaneously (performance + sparsity)\nrec_nsga2 = recipe(train)\nrec_nsga2 = step_select_genetic_algorithm(\n    rec_nsga2,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    use_nsga2=True,  # Enable NSGA-II\n    nsga2_objectives=['performance', 'sparsity'],  # Two objectives\n    nsga2_selection_method='knee_point',  # Select best trade-off\n    population_size=30,  # Must be even for NSGA-II\n    generations=20,\n    cv_folds=3,\n    random_state=42,\n    verbose=True\n)\n\nprepped_nsga2 = rec_nsga2.prep(train)\ntrain_nsga2 = prepped_nsga2.bake(train)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Selected features (NSGA-II): {[col for col in train_nsga2.columns if col != 'y']}\")\nprint(f\"Objectives: performance (RMSE) + sparsity (# features)\")\nprint(f\"Selection: knee point (best trade-off)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Complete Workflow with Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build complete workflow with GA feature selection\nrec_full = recipe(train)\nrec_full = step_select_genetic_algorithm(\n    rec_full,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    top_n=5,\n    population_size=25,\n    generations=20,\n    adaptive_mutation=True,  # Adaptive mutation rate\n    adaptive_crossover=True,  # Adaptive crossover rate\n    cv_folds=3,\n    random_state=42,\n    verbose=False\n)\n\n# Build workflow\nwf = workflow().add_recipe(rec_full).add_model(linear_reg())\n\n# Fit and evaluate\nfit = wf.fit(train)\nfit = fit.evaluate(test)\n\n# Get predictions\ntrain_preds = fit.predict(train)\ntest_preds = fit.predict(test)\n\n# Calculate metrics\ntrain_rmse = rmse(train['y'].values, train_preds['.pred'].values).iloc[0]['value']\ntest_rmse = rmse(test['y'].values, test_preds['.pred'].values).iloc[0]['value']\ntrain_r2 = r_squared(train['y'].values, train_preds['.pred'].values).iloc[0]['value']\ntest_r2 = r_squared(test['y'].values, test_preds['.pred'].values).iloc[0]['value']\n\nprint(f\"\\n{'='*60}\")\nprint(\"GA Feature Selection + Linear Regression Results:\")\nprint(f\"\\nTrain RMSE: {train_rmse:.4f}\")\nprint(f\"Test RMSE:  {test_rmse:.4f}\")\nprint(f\"Train R²:   {train_r2:.4f}\")\nprint(f\"Test R²:    {test_r2:.4f}\")\n\n# Compare with using all features\nwf_all = workflow().add_formula('y ~ .').add_model(linear_reg())\nfit_all = wf_all.fit(train)\ntest_preds_all = fit_all.predict(test)\ntest_rmse_all = rmse(test['y'].values, test_preds_all['.pred'].values).iloc[0]['value']\n\nprint(f\"\\nComparison (all features): Test RMSE = {test_rmse_all:.4f}\")\nprint(f\"Improvement: {((test_rmse_all - test_rmse) / test_rmse_all * 100):.1f}% better\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grouped/Panel Data - Per-Group Feature Selection\n",
    "\n",
    "We'll create a panel dataset with multiple groups (e.g., countries) where different features are important for different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "groups = ['USA', 'Germany', 'Japan']\n",
    "n_per_group = 150\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for group_idx, group in enumerate(groups):\n",
    "    # Different features are important for different groups\n",
    "    x1 = np.random.randn(n_per_group)\n",
    "    x2 = np.random.randn(n_per_group)\n",
    "    x3 = np.random.randn(n_per_group)\n",
    "    x4 = np.random.randn(n_per_group)\n",
    "    x5 = np.random.randn(n_per_group)\n",
    "    x6 = np.random.randn(n_per_group)\n",
    "    \n",
    "    if group == 'USA':\n",
    "        # USA: x1, x2, x3 are important\n",
    "        y = 4 * x1 + 3 * x2 + 2 * x3 + np.random.randn(n_per_group) * 0.5\n",
    "    elif group == 'Germany':\n",
    "        # Germany: x2, x4, x5 are important\n",
    "        y = 3 * x2 + 2 * x4 + 2 * x5 + np.random.randn(n_per_group) * 0.5\n",
    "    else:  # Japan\n",
    "        # Japan: x1, x4, x6 are important\n",
    "        y = 4 * x1 + 2 * x4 + 3 * x6 + np.random.randn(n_per_group) * 0.5\n",
    "    \n",
    "    group_data = pd.DataFrame({\n",
    "        'country': group,\n",
    "        'y': y,\n",
    "        'x1': x1, 'x2': x2, 'x3': x3,\n",
    "        'x4': x4, 'x5': x5, 'x6': x6\n",
    "    })\n",
    "    data_list.append(group_data)\n",
    "\n",
    "panel_data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Split train/test\n",
    "train_panel = panel_data.sample(frac=0.75, random_state=42)\n",
    "test_panel = panel_data.drop(train_panel.index)\n",
    "\n",
    "print(f\"Panel data shape: {panel_data.shape}\")\n",
    "print(f\"Groups: {groups}\")\n",
    "print(f\"\\nTrue important features by group:\")\n",
    "print(\"  USA:     x1, x2, x3\")\n",
    "print(\"  Germany: x2, x4, x5\")\n",
    "print(\"  Japan:   x1, x4, x6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Per-Group Feature Selection\n",
    "\n",
    "We'll use a workflow with nested modeling to select features for each group independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# For grouped data, we'll run GA feature selection per group manually\n# (This provides full control and clear results per group)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Per-Group Feature Selection with GA:\")\nprint(f\"{'='*60}\\n\")\n\nselected_by_group = {}\nprepped_by_group = {}\n\nfor group in groups:\n    # Get group data (without country column)\n    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n    \n    # Run GA for this group\n    rec_group = recipe(group_train)\n    rec_group = step_select_genetic_algorithm(\n        rec_group,\n        outcome='y',\n        model=linear_reg(),\n        metric='rmse',\n        top_n=3,\n        population_size=20,\n        generations=15,\n        cv_folds=3,\n        random_state=42,\n        verbose=False\n    )\n    \n    prepped_group = rec_group.prep(group_train)\n    processed_group = prepped_group.bake(group_train)\n    selected_features = [col for col in processed_group.columns if col != 'y']\n    \n    selected_by_group[group] = selected_features\n    prepped_by_group[group] = prepped_group\n    \n    print(f\"{group}:\")\n    print(f\"  Selected: {selected_features}\")\n    print(f\"  Expected: \", end=\"\")\n    if group == 'USA':\n        print(\"x1, x2, x3\")\n    elif group == 'Germany':\n        print(\"x2, x4, x5\")\n    else:\n        print(\"x1, x4, x6\")\n    print()\n\n# Build per-group models with selected features\nprint(\"\\nBuilding per-group models with selected features...\")\ngroup_models = {}\n\nfor group, features in selected_by_group.items():\n    formula = 'y ~ ' + ' + '.join(features)\n    wf_group = workflow().add_formula(formula).add_model(linear_reg())\n    \n    # Fit on group training data\n    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n    group_models[group] = wf_group.fit(group_train)\n    \nprint(f\"✓ Trained {len(group_models)} group-specific models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Evaluate Per-Group Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate each group's model on test data\nprint(f\"\\n{'='*60}\")\nprint(\"Per-Group Model Performance:\")\nprint(f\"{'='*60}\\n\")\n\nfor group in groups:\n    # Get group test data\n    group_test = test_panel[test_panel['country'] == group].drop('country', axis=1)\n    \n    # Predict\n    group_preds = group_models[group].predict(group_test)\n    \n    # Calculate metrics\n    group_rmse = rmse(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n    group_mae = mae(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n    group_r2 = r_squared(group_test['y'].values, group_preds['.pred'].values).iloc[0]['value']\n    \n    print(f\"{group}:\")\n    print(f\"  RMSE: {group_rmse:.4f}\")\n    print(f\"  MAE:  {group_mae:.4f}\")\n    print(f\"  R²:   {group_r2:.4f}\")\n    print()\n\n# Compare with using all features per group\nprint(\"\\nComparison with using all features:\")\nfor group in groups:\n    group_test = test_panel[test_panel['country'] == group].drop('country', axis=1)\n    group_train = train_panel[train_panel['country'] == group].drop('country', axis=1)\n    \n    # Fit model with all features\n    wf_all = workflow().add_formula('y ~ .').add_model(linear_reg())\n    fit_all = wf_all.fit(group_train)\n    preds_all = fit_all.predict(group_test)\n    \n    rmse_all = rmse(group_test['y'].values, preds_all['.pred'].values).iloc[0]['value']\n    rmse_ga = rmse(group_test['y'].values, group_models[group].predict(group_test)['.pred'].values).iloc[0]['value']\n    \n    improvement = ((rmse_all - rmse_ga) / rmse_all * 100) if rmse_all > rmse_ga else 0\n    print(f\"{group}: All features RMSE = {rmse_all:.4f}, GA selected RMSE = {rmse_ga:.4f} ({improvement:.1f}% better)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10: Visualize GA Evolution History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get GA history from the recipe step (ungrouped example)\nrec_vis = recipe(train)\nrec_vis = step_select_genetic_algorithm(\n    rec_vis,\n    outcome='y',\n    model=linear_reg(),\n    metric='rmse',\n    top_n=5,\n    population_size=30,\n    generations=30,\n    cv_folds=3,\n    random_state=42,\n    verbose=False\n)\n\nprepped_vis = rec_vis.prep(train)\n\n# Access the step to get GA history\nga_step = prepped_vis.steps[0]\n\n# Plot evolution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Fitness history\nax1.plot(ga_step._ga_history, linewidth=2, color='steelblue')\nax1.set_xlabel('Generation', fontsize=12)\nax1.set_ylabel('Best Fitness (lower RMSE = better)', fontsize=12)\nax1.set_title('GA Evolution: Fitness Over Time', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Selected features\nselected = [col for col in prepped_vis.bake(train).columns if col != 'y']\nfeature_importance = pd.Series(1, index=selected).sort_values(ascending=True)\nfeature_importance.plot(kind='barh', ax=ax2, color='coral')\nax2.set_xlabel('Selected', fontsize=12)\nax2.set_title('Final Selected Features', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nGA converged: {ga_step._converged}\")\nprint(f\"Generations run: {ga_step._n_generations}\")\nprint(f\"Final fitness: {ga_step._final_fitness:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "### Ungrouped Data Examples:\n",
    "1. ✅ Basic GA feature selection\n",
    "2. ✅ Mandatory and forbidden features\n",
    "3. ✅ Feature costs and sparsity objectives\n",
    "4. ✅ Ensemble mode (multiple GA runs)\n",
    "5. ✅ Diversity maintenance\n",
    "6. ✅ Multi-objective optimization (NSGA-II)\n",
    "7. ✅ Complete workflow with evaluation\n",
    "\n",
    "### Grouped Data Examples:\n",
    "8. ✅ Per-group feature selection\n",
    "9. ✅ Per-group model evaluation\n",
    "10. ✅ Visualization of GA evolution\n",
    "\n",
    "### Key Takeaways:\n",
    "- GA successfully identifies important features even with noise\n",
    "- Constraints (mandatory/forbidden) are respected\n",
    "- Cost and sparsity objectives guide feature selection\n",
    "- Ensemble mode provides more stable results\n",
    "- NSGA-II finds optimal trade-offs between objectives\n",
    "- Per-group selection adapts to group-specific patterns\n",
    "\n",
    "### Additional Features Not Demonstrated:\n",
    "- Warm start initialization (importance-based, low-correlation)\n",
    "- Constraint relaxation (gradual loosening)\n",
    "- Parallel evaluation (n_jobs > 1)\n",
    "- Statistical constraints (p-value, VIF, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}