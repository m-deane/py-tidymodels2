{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py_agent: Advanced Features Tutorial\n",
    "\n",
    "This notebook demonstrates **advanced py_agent capabilities** for power users and production deployments:\n",
    "\n",
    "1. **Custom Preprocessing Strategies**: Advanced recipe engineering\n",
    "2. **Ensemble Methods**: Combining multiple models for better accuracy\n",
    "3. **Performance Debugging**: Diagnosing and fixing poor forecasts\n",
    "4. **Grouped/Panel Modeling**: Multi-entity forecasting at scale\n",
    "5. **Production Best Practices**: Deployment, monitoring, error handling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial assumes you've completed:\n",
    "- **22_agent_complete_tutorial.ipynb** (basics)\n",
    "- **23_agent_llm_mode_tutorial.ipynb** (LLM mode)\n",
    "- **24_agent_domain_specific_examples.ipynb** (domain applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import py_agent\n",
    "from py_agent import ForecastAgent\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Complex Dataset\n",
    "\n",
    "We'll create a challenging forecasting scenario to showcase advanced features:\n",
    "- **Non-linear relationships**\n",
    "- **Multiple seasonality**\n",
    "- **Structural breaks** (regime changes)\n",
    "- **Heteroscedasticity** (changing volatility)\n",
    "- **Outliers and missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(789)\n",
    "n_days = 730  # 2 years\n",
    "dates = pd.date_range('2022-01-01', periods=n_days, freq='D')\n",
    "\n",
    "# Base trend with structural break at day 400\n",
    "trend_early = np.linspace(100, 150, 400)\n",
    "trend_late = np.linspace(150, 220, n_days - 400)\n",
    "trend = np.concatenate([trend_early, trend_late])\n",
    "\n",
    "# Multiple seasonality\n",
    "day_of_week = dates.dayofweek\n",
    "day_of_year = dates.dayofyear\n",
    "weekly_season = 20 * np.sin(2 * np.pi * day_of_week / 7)\n",
    "yearly_season = 30 * np.sin(2 * np.pi * (day_of_year - 80) / 365)\n",
    "\n",
    "# External variables with non-linear effects\n",
    "x1 = np.random.normal(10, 2, n_days)\n",
    "x2 = np.random.uniform(5, 15, n_days)\n",
    "x3 = np.random.exponential(3, n_days)\n",
    "\n",
    "# Non-linear relationships\n",
    "x1_effect = 2 * x1 + 0.3 * x1**2  # Quadratic\n",
    "x2_effect = 5 * np.log(x2)  # Logarithmic\n",
    "x3_effect = 10 / (1 + np.exp(-0.5 * (x3 - 3)))  # Sigmoid\n",
    "\n",
    "# Interaction effects\n",
    "interaction = 0.5 * x1 * x2\n",
    "\n",
    "# Heteroscedastic noise (increasing volatility)\n",
    "volatility = np.linspace(5, 20, n_days)\n",
    "noise = np.random.normal(0, volatility)\n",
    "\n",
    "# Combine\n",
    "y = trend + weekly_season + yearly_season + x1_effect + x2_effect + x3_effect + interaction + noise\n",
    "\n",
    "# Add outliers (5% of data)\n",
    "outlier_mask = np.random.choice([False, True], size=n_days, p=[0.95, 0.05])\n",
    "y[outlier_mask] += np.random.choice([-1, 1], size=outlier_mask.sum()) * np.random.uniform(50, 100, outlier_mask.sum())\n",
    "\n",
    "# Add missing data (3% of data)\n",
    "missing_mask = np.random.choice([False, True], size=n_days, p=[0.97, 0.03])\n",
    "y_with_missing = y.copy()\n",
    "y_with_missing[missing_mask] = np.nan\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'target': y_with_missing,\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x3': x3\n",
    "})\n",
    "\n",
    "# Split\n",
    "train_data = data.iloc[:int(0.75 * len(data))].copy()\n",
    "test_data = data.iloc[int(0.75 * len(data)):].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š COMPLEX DATASET\")\n",
    "print(f\"  - Total: {len(data)} days\")\n",
    "print(f\"  - Train: {len(train_data)} days\")\n",
    "print(f\"  - Test: {len(test_data)} days\")\n",
    "print(f\"  - Missing values: {data['target'].isna().sum()} ({data['target'].isna().mean()*100:.1f}%)\")\n",
    "print(f\"  - Outliers (estimated): {outlier_mask.sum()} ({outlier_mask.mean()*100:.1f}%)\")\n",
    "print(f\"  - Structural break: Day 400\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Target with structural break\n",
    "axes[0].plot(data['date'], data['target'], alpha=0.7, label='Target')\n",
    "axes[0].axvline(dates[400], color='red', linestyle='--', linewidth=2, label='Structural Break')\n",
    "axes[0].axvline(train_data['date'].iloc[-1], color='gray', linestyle='--', label='Train/Test')\n",
    "axes[0].set_title('Target Variable (with missing data, outliers, structural break)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Target')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# External variables\n",
    "axes[1].plot(data['date'], data['x1'], alpha=0.7, label='x1 (quadratic effect)')\n",
    "axes[1].plot(data['date'], data['x2'], alpha=0.7, label='x2 (log effect)')\n",
    "axes[1].plot(data['date'], data['x3'], alpha=0.7, label='x3 (sigmoid effect)')\n",
    "axes[1].set_title('External Variables', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Non-linear effects\n",
    "axes[2].scatter(data['x1'], data['target'], alpha=0.3, s=10)\n",
    "axes[2].set_title('Non-Linear Relationship: Target vs x1', fontweight='bold', fontsize=12)\n",
    "axes[2].set_xlabel('x1')\n",
    "axes[2].set_ylabel('Target')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility over time\n",
    "rolling_std = data['target'].rolling(30).std()\n",
    "axes[3].plot(data['date'], rolling_std, color='purple', alpha=0.7)\n",
    "axes[3].set_title('Rolling Volatility (30-day)', fontweight='bold', fontsize=12)\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_ylabel('Std Dev')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Performance Debugging with Autonomous Iteration\n",
    "\n",
    "When a simple workflow doesn't work well, use **Phase 3.5 Autonomous Iteration** to automatically try different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent_debug = ForecastAgent(verbose=True, use_rag=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE DEBUGGING WITH AUTONOMOUS ITERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try to achieve RMSE < 15\n",
    "best_workflow, history = agent_debug.iterate(\n",
    "    data=train_data,\n",
    "    request=\"Forecast target with complex non-linear patterns\",\n",
    "    target_metric='rmse',\n",
    "    target_value=15.0,  # Stop when RMSE < 15\n",
    "    max_iterations=5,\n",
    "    test_data=test_data\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Iteration complete\")\n",
    "print(f\"  Iterations run: {len(history)}\")\n",
    "print(f\"  Best RMSE: {history[-1].performance_metrics['rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze iteration history\n",
    "print(\"\\nIteration History:\")\n",
    "iteration_summary = pd.DataFrame([\n",
    "    {\n",
    "        'iteration': i+1,\n",
    "        'approach': result.approach_taken,\n",
    "        'model': result.workflow.extract_spec_parsnip().model_type,\n",
    "        'rmse': result.performance_metrics['rmse'],\n",
    "        'mae': result.performance_metrics.get('mae', np.nan),\n",
    "        'r2': result.performance_metrics.get('r_squared', np.nan),\n",
    "        'issues_detected': ', '.join(result.issues_detected) if result.issues_detected else 'None'\n",
    "    }\n",
    "    for i, result in enumerate(history)\n",
    "])\n",
    "\n",
    "print(iteration_summary.to_string(index=False))\n",
    "\n",
    "# Visualize improvement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE over iterations\n",
    "axes[0].plot(range(1, len(history)+1), iteration_summary['rmse'], \n",
    "             marker='o', linewidth=2, markersize=8)\n",
    "axes[0].axhline(15.0, color='red', linestyle='--', label='Target RMSE')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE Improvement Over Iterations', fontweight='bold', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RÂ² over iterations\n",
    "axes[1].plot(range(1, len(history)+1), iteration_summary['r2'],\n",
    "             marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('RÂ²', fontsize=12)\n",
    "axes[1].set_title('RÂ² Improvement Over Iterations', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Multi-Model Ensemble with Diversity\n",
    "\n",
    "Combine multiple models for better robustness and accuracy using **Phase 3.3 Multi-Model Comparison**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent for ensemble\n",
    "agent_ensemble = ForecastAgent(verbose=True, use_rag=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-MODEL ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare 5 models and get ensemble recommendation\n",
    "results_ensemble = agent_ensemble.compare_models(\n",
    "    data=train_data,\n",
    "    request=\"Forecast target with complex patterns\",\n",
    "    n_models=5,\n",
    "    cv_strategy='time_series',\n",
    "    date_column='date',\n",
    "    initial='12 months',\n",
    "    assess='2 months',\n",
    "    skip='1 month',\n",
    "    return_ensemble=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Multi-model comparison complete\")\n",
    "print(f\"  Models compared: {len(results_ensemble['model_ids'])}\")\n",
    "print(f\"  Ensemble recommended: {results_ensemble['ensemble_recommended']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rankings\n",
    "print(\"\\nModel Rankings:\")\n",
    "print(results_ensemble['rankings'].head(10).to_string(index=False))\n",
    "\n",
    "if results_ensemble['ensemble_recommended']:\n",
    "    print(f\"\\nâœ“ Ensemble Recommendation:\")\n",
    "    print(f\"  Ensemble models: {results_ensemble['ensemble_models']}\")\n",
    "    print(f\"  Diversity score: {results_ensemble['ensemble_diversity']:.2f}\")\n",
    "    print(f\"  Reason: {results_ensemble['ensemble_reasoning']}\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "rankings = results_ensemble['rankings'].head(5)\n",
    "models = rankings['model_id'].values\n",
    "rmse_vals = rankings['mean_rmse'].values\n",
    "rmse_std = rankings['std_rmse'].values\n",
    "\n",
    "x = np.arange(len(models))\n",
    "bars = ax.bar(x, rmse_vals, yerr=rmse_std, capsize=5, alpha=0.7)\n",
    "\n",
    "# Highlight ensemble models\n",
    "if results_ensemble['ensemble_recommended']:\n",
    "    for i, model in enumerate(models):\n",
    "        if model in results_ensemble['ensemble_models']:\n",
    "            bars[i].set_color('green')\n",
    "            bars[i].set_alpha(0.9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylabel('RMSE (Mean Â± Std)', fontsize=12)\n",
    "ax.set_title('Top 5 Models Comparison (Green = Ensemble Members)', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Grouped/Panel Modeling at Scale\n",
    "\n",
    "Forecast multiple entities (stores, products, regions) efficiently with **nested modeling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-entity dataset (3 stores with different patterns)\n",
    "np.random.seed(999)\n",
    "\n",
    "stores = ['Store_A', 'Store_B', 'Store_C']\n",
    "grouped_data = []\n",
    "\n",
    "for i, store in enumerate(stores):\n",
    "    n = 365  # 1 year per store\n",
    "    dates_store = pd.date_range('2023-01-01', periods=n, freq='D')\n",
    "    \n",
    "    # Different patterns per store\n",
    "    base = 50 + i * 20  # Different base levels\n",
    "    trend = np.linspace(base, base + 30, n)\n",
    "    \n",
    "    # Store A: Strong weekly seasonality\n",
    "    # Store B: Strong yearly seasonality\n",
    "    # Store C: Mixed\n",
    "    dow = dates_store.dayofweek\n",
    "    doy = dates_store.dayofyear\n",
    "    \n",
    "    if store == 'Store_A':\n",
    "        pattern = 15 * np.sin(2 * np.pi * dow / 7)\n",
    "    elif store == 'Store_B':\n",
    "        pattern = 20 * np.sin(2 * np.pi * (doy - 80) / 365)\n",
    "    else:\n",
    "        pattern = 10 * np.sin(2 * np.pi * dow / 7) + 10 * np.sin(2 * np.pi * doy / 365)\n",
    "    \n",
    "    sales = trend + pattern + np.random.normal(0, 5, n)\n",
    "    \n",
    "    store_df = pd.DataFrame({\n",
    "        'date': dates_store,\n",
    "        'store': store,\n",
    "        'sales': sales\n",
    "    })\n",
    "    grouped_data.append(store_df)\n",
    "\n",
    "grouped_df = pd.concat(grouped_data, ignore_index=True)\n",
    "\n",
    "# Split\n",
    "train_grouped = grouped_df[grouped_df['date'] < '2023-10-01'].copy()\n",
    "test_grouped = grouped_df[grouped_df['date'] >= '2023-10-01'].copy()\n",
    "\n",
    "print(f\"\\nðŸª GROUPED MODELING DATA\")\n",
    "print(f\"  - Stores: {len(stores)}\")\n",
    "print(f\"  - Total observations: {len(grouped_df)}\")\n",
    "print(f\"  - Train: {len(train_grouped)} ({train_grouped['date'].min()} to {train_grouped['date'].max()})\")\n",
    "print(f\"  - Test: {len(test_grouped)} ({test_grouped['date'].min()} to {test_grouped['date'].max()})\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "for i, store in enumerate(stores):\n",
    "    store_data = grouped_df[grouped_df['store'] == store]\n",
    "    train_store = train_grouped[train_grouped['store'] == store]\n",
    "    \n",
    "    axes[i].plot(store_data['date'], store_data['sales'], alpha=0.7, label='Sales')\n",
    "    axes[i].axvline(train_store['date'].iloc[-1], color='gray', linestyle='--', label='Train/Test')\n",
    "    axes[i].set_title(f'{store} Sales', fontweight='bold', fontsize=12)\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grouped modeling\n",
    "from py_workflows import Workflow\n",
    "from py_parsnip import linear_reg\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GROUPED/PANEL MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create workflow\n",
    "wf_grouped = Workflow().add_formula(\"sales ~ date\").add_model(linear_reg())\n",
    "\n",
    "# Fit nested (separate model per store)\n",
    "fit_grouped = wf_grouped.fit_nested(train_grouped, group_col='store')\n",
    "\n",
    "print(f\"\\nâœ“ Nested models fitted\")\n",
    "print(f\"  Groups: {len(fit_grouped.group_fits)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions_grouped = fit_grouped.predict(test_grouped)\n",
    "\n",
    "# Extract outputs\n",
    "outputs_grouped, coeffs_grouped, stats_grouped = fit_grouped.extract_outputs()\n",
    "\n",
    "print(f\"\\nPer-Store Performance:\")\n",
    "test_stats_grouped = stats_grouped[stats_grouped['split'] == 'test']\n",
    "print(test_stats_grouped[['group', 'rmse', 'mae', 'r_squared']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-store predictions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "test_outputs_grouped = outputs_grouped[outputs_grouped['split'] == 'test']\n",
    "\n",
    "for i, store in enumerate(stores):\n",
    "    store_outputs = test_outputs_grouped[test_outputs_grouped['group'] == store]\n",
    "    store_test = test_grouped[test_grouped['store'] == store]\n",
    "    \n",
    "    axes[i].plot(store_test['date'].values, store_outputs['actuals'].values,\n",
    "                 label='Actual', linewidth=2, alpha=0.7)\n",
    "    axes[i].plot(store_test['date'].values, store_outputs['fitted'].values,\n",
    "                 label='Forecast', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    store_stats = test_stats_grouped[test_stats_grouped['group'] == store]\n",
    "    rmse = store_stats['rmse'].iloc[0]\n",
    "    r2 = store_stats['r_squared'].iloc[0]\n",
    "    \n",
    "    axes[i].set_title(f'{store} Forecast (RMSE={rmse:.2f}, RÂ²={r2:.4f})', \n",
    "                      fontweight='bold', fontsize=12)\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 4: Production Best Practices\n",
    "\n",
    "### Error Handling and Graceful Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION ERROR HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: Handling missing data\n",
    "data_with_gaps = train_data.copy()\n",
    "data_with_gaps.loc[50:60, 'target'] = np.nan  # Large gap\n",
    "\n",
    "try:\n",
    "    agent_prod = ForecastAgent(verbose=True)\n",
    "    workflow_prod = agent_prod.generate_workflow(\n",
    "        data=data_with_gaps,\n",
    "        request=\"Forecast target\"\n",
    "    )\n",
    "    print(\"âœ“ Handled missing data gracefully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"   Recommendation: Use imputation in preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Budget controls for LLM mode\n",
    "import os\n",
    "\n",
    "if 'ANTHROPIC_API_KEY' in os.environ:\n",
    "    try:\n",
    "        agent_budget = ForecastAgent(\n",
    "            verbose=False,  # Reduce verbosity in production\n",
    "            use_llm=True,\n",
    "            budget_per_day=10.0  # Strict budget\n",
    "        )\n",
    "        \n",
    "        # Generate multiple workflows\n",
    "        for i in range(3):\n",
    "            workflow = agent_budget.generate_workflow(\n",
    "                data=train_data.head(100),  # Small subset for testing\n",
    "                request=f\"Forecast target (iteration {i+1})\"\n",
    "            )\n",
    "            print(f\"  Iteration {i+1}: Cost=${agent_budget.llm_client.total_cost:.4f}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Budget controls working\")\n",
    "        print(f\"  Total cost: ${agent_budget.llm_client.total_cost:.4f}\")\n",
    "        print(f\"  Budget limit: ${agent_budget.llm_client.budget_per_day:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ“ Budget exceeded: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Skipping LLM budget test (no API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE MONITORING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create monitoring dashboard function\n",
    "def create_monitoring_dashboard(fit_workflow, test_data):\n",
    "    \"\"\"\n",
    "    Create monitoring dashboard for production forecasts.\n",
    "    \"\"\"\n",
    "    # Evaluate\n",
    "    eval_result = fit_workflow.evaluate(test_data)\n",
    "    outputs, coeffs, stats = eval_result.extract_outputs()\n",
    "    \n",
    "    # Get test metrics\n",
    "    test_stats = stats[stats['split'] == 'test']\n",
    "    \n",
    "    # Dashboard data\n",
    "    dashboard = {\n",
    "        'timestamp': datetime.now(),\n",
    "        'model_type': fit_workflow.extract_spec_parsnip().model_type,\n",
    "        'n_test_samples': len(test_data),\n",
    "        'rmse': test_stats['rmse'].iloc[0],\n",
    "        'mae': test_stats['mae'].iloc[0],\n",
    "        'mape': test_stats['mape'].iloc[0],\n",
    "        'r_squared': test_stats['r_squared'].iloc[0],\n",
    "        'mean_residual': test_stats['mean_residual'].iloc[0],\n",
    "        'max_error': outputs[outputs['split']=='test']['residuals'].abs().max(),\n",
    "        'status': 'healthy' if test_stats['r_squared'].iloc[0] > 0.7 else 'degraded'\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([dashboard])\n",
    "\n",
    "# Generate monitoring data\n",
    "agent_monitor = ForecastAgent(verbose=False)\n",
    "workflow_monitor = agent_monitor.generate_workflow(data=train_data, request=\"Forecast target\")\n",
    "fit_monitor = workflow_monitor.fit(train_data)\n",
    "\n",
    "dashboard = create_monitoring_dashboard(fit_monitor, test_data)\n",
    "print(\"\\nMonitoring Dashboard:\")\n",
    "print(dashboard.T.to_string())\n",
    "\n",
    "# Alert logic\n",
    "if dashboard['status'].iloc[0] == 'degraded':\n",
    "    print(\"\\nâš ï¸  ALERT: Model performance degraded!\")\n",
    "    print(\"   Action: Retrain model or investigate data quality issues\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Model performance healthy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility and Version Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REPRODUCIBILITY & VERSION CONTROL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract model configuration for versioning\n",
    "model_config = {\n",
    "    'model_type': fit_monitor.extract_spec_parsnip().model_type,\n",
    "    'model_args': fit_monitor.extract_spec_parsnip().args,\n",
    "    'formula': fit_monitor.extract_formula(),\n",
    "    'engine': fit_monitor.extract_spec_parsnip().engine,\n",
    "    'train_start': train_data['date'].min(),\n",
    "    'train_end': train_data['date'].max(),\n",
    "    'n_train_samples': len(train_data),\n",
    "    'features': list(train_data.columns)\n",
    "}\n",
    "\n",
    "print(\"\\nModel Configuration (for version control):\")\n",
    "for key, val in model_config.items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "\n",
    "# Save configuration to JSON\n",
    "import json\n",
    "config_json = json.dumps(model_config, indent=2, default=str)\n",
    "print(\"\\nJSON Configuration:\")\n",
    "print(config_json[:500] + \"...\" if len(config_json) > 500 else config_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Advanced Features Summary\n",
    "\n",
    "1. **Autonomous Iteration (Phase 3.5)**\n",
    "   - Automatically tries different approaches until target performance reached\n",
    "   - Self-debugging: detects overfitting, underfitting, etc.\n",
    "   - Use for complex forecasting scenarios\n",
    "\n",
    "2. **Multi-Model Ensembles (Phase 3.3)**\n",
    "   - Compare 5+ models with cross-validation\n",
    "   - Automatic ensemble recommendation with diversity scoring\n",
    "   - Better robustness and accuracy\n",
    "\n",
    "3. **Grouped/Panel Modeling**\n",
    "   - Forecast multiple entities efficiently\n",
    "   - Separate models per group (`fit_nested()`)\n",
    "   - Global models with group features (`fit_global()`)\n",
    "\n",
    "4. **Production Best Practices**\n",
    "   - Error handling and graceful degradation\n",
    "   - Budget controls for LLM mode\n",
    "   - Performance monitoring dashboards\n",
    "   - Model configuration versioning\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "**Before Deployment**:\n",
    "- âœ… Test on historical data\n",
    "- âœ… Set up error handling\n",
    "- âœ… Configure budget limits (if using LLM)\n",
    "- âœ… Create monitoring dashboard\n",
    "- âœ… Version control model configurations\n",
    "\n",
    "**During Operation**:\n",
    "- âœ… Monitor performance metrics\n",
    "- âœ… Track API costs (if using LLM)\n",
    "- âœ… Log errors and edge cases\n",
    "- âœ… Retrain periodically (concept drift)\n",
    "\n",
    "**Performance Optimization**:\n",
    "- Use rule-based mode for batch processing ($0 cost)\n",
    "- Use LLM mode for critical forecasts ($4-10)\n",
    "- Use RAG for domain-specific recommendations\n",
    "- Use iteration for complex scenarios\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Apply to your data**: Start with simple workflows, add complexity as needed\n",
    "2. **Combine features**: Use multiple phases together for best results\n",
    "3. **Monitor in production**: Set up dashboards and alerts\n",
    "4. **Iterate and improve**: Use feedback to refine models\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Documentation**: `py_agent/README.md` for full API reference\n",
    "- **Examples**: See other notebooks (22-24) for basics and domain applications\n",
    "- **Tests**: `tests/test_agent/` for comprehensive test suite\n",
    "- **Community**: GitHub issues for questions and bug reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "py-tidymodels2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
