{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-recipes Demo: Feature Engineering Pipelines\n",
    "\n",
    "This notebook demonstrates the **py-recipes** package for creating reproducible feature engineering pipelines.\n",
    "\n",
    "## What is py-recipes?\n",
    "\n",
    "**py-recipes** provides a consistent interface for:\n",
    "- **Preprocessing**: Normalization, imputation, encoding\n",
    "- **Feature engineering**: Custom transformations\n",
    "- **Pipeline composition**: Chain multiple steps\n",
    "- **Train/test consistency**: No data leakage\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Recipe**: Specification of preprocessing steps (immutable)\n",
    "2. **prep()**: Fit recipe to training data\n",
    "3. **bake()**: Apply fitted recipe to new data\n",
    "4. **PreparedRecipe**: Fitted recipe ready to transform\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary packages and create sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from py_recipes import recipe\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import linear_reg\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with various issues\n",
    "n = 200\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"sales\": np.random.randn(n) * 1000 + 5000,\n",
    "    \"price\": np.random.randn(n) * 10 + 50,\n",
    "    \"advertising\": np.random.randn(n) * 500 + 1000,\n",
    "    \"temperature\": np.random.randn(n) * 15 + 20,\n",
    "    \"store_type\": np.random.choice([\"Mall\", \"Street\", \"Online\"], n),\n",
    "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "missing_idx = np.random.choice(n, size=20, replace=False)\n",
    "data.loc[missing_idx, \"advertising\"] = np.nan\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"\\nMissing values:\\n{data.isna().sum()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train_idx = int(0.8 * n)\n",
    "train = data[:train_idx].copy()\n",
    "test = data[train_idx:].copy()\n",
    "\n",
    "print(f\"Train: {len(train)} | Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Recipe Usage\n",
    "\n",
    "A recipe follows the **prep/bake** pattern:\n",
    "1. **Create** a recipe specification\n",
    "2. **prep()** fits the recipe to training data\n",
    "3. **bake()** applies the fitted recipe to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple recipe\n",
    "rec = recipe()\n",
    "print(f\"Recipe: {rec}\")\n",
    "print(f\"Steps: {rec.steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a normalization step\n",
    "rec = recipe().step_normalize()\n",
    "print(f\"Recipe with 1 step: {len(rec.steps)} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (prep) the recipe to training data\n",
    "rec_fit = rec.prep(train)\n",
    "print(f\"Prepared recipe: {rec_fit}\")\n",
    "print(f\"Prepared steps: {len(rec_fit.prepared_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply (bake) to new data\n",
    "train_transformed = rec_fit.bake(train)\n",
    "\n",
    "print(\"\\nOriginal train data (first 3 rows):\")\n",
    "print(train[[\"sales\", \"price\", \"advertising\"]].head(3))\n",
    "\n",
    "print(\"\\nTransformed train data (first 3 rows):\")\n",
    "print(train_transformed[[\"sales\", \"price\", \"advertising\"]].head(3))\n",
    "\n",
    "print(\"\\nMeans after normalization:\")\n",
    "print(train_transformed[[\"sales\", \"price\", \"advertising\", \"temperature\"]].mean())\n",
    "\n",
    "print(\"\\nStd devs after normalization:\")\n",
    "print(train_transformed[[\"sales\", \"price\", \"advertising\", \"temperature\"]].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Point**: The recipe learns from **training data** and applies the same transformation to **test data**.\n",
    "\n",
    "This prevents **data leakage**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the SAME fitted recipe to test data\n",
    "test_transformed = rec_fit.bake(test)\n",
    "\n",
    "print(\"Test data means (NOT zero because fitted on train):\")\n",
    "print(test_transformed[[\"sales\", \"price\", \"advertising\", \"temperature\"]].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Normalization (step_normalize)\n",
    "\n",
    "Normalize numeric columns using:\n",
    "- **zscore** (default): standardization (mean=0, std=1)\n",
    "- **minmax**: scaling to [0, 1] range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize specific columns\n",
    "rec_zscore = (\n",
    "    recipe()\n",
    "    .step_normalize(columns=[\"price\", \"advertising\"], method=\"zscore\")\n",
    ")\n",
    "\n",
    "rec_zscore_fit = rec_zscore.prep(train)\n",
    "train_zscore = rec_zscore_fit.bake(train)\n",
    "\n",
    "print(\"Z-score normalized columns:\")\n",
    "print(train_zscore[[\"price\", \"advertising\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling [0, 1]\n",
    "rec_minmax = (\n",
    "    recipe()\n",
    "    .step_normalize(columns=[\"price\", \"advertising\"], method=\"minmax\")\n",
    ")\n",
    "\n",
    "rec_minmax_fit = rec_minmax.prep(train)\n",
    "train_minmax = rec_minmax_fit.bake(train)\n",
    "\n",
    "print(\"MinMax normalized columns:\")\n",
    "print(train_minmax[[\"price\", \"advertising\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dummy Variables (step_dummy)\n",
    "\n",
    "Convert categorical variables to one-hot encoded dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original categorical columns\n",
    "print(\"Original data:\")\n",
    "print(train[[\"store_type\", \"region\"]].head())\n",
    "print(f\"\\nUnique store types: {train['store_type'].unique()}\")\n",
    "print(f\"Unique regions: {train['region'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "rec_dummy = (\n",
    "    recipe()\n",
    "    .step_dummy([\"store_type\", \"region\"])\n",
    ")\n",
    "\n",
    "rec_dummy_fit = rec_dummy.prep(train)\n",
    "train_dummy = rec_dummy_fit.bake(train)\n",
    "\n",
    "print(\"After dummy encoding:\")\n",
    "print(\"\\nNew columns:\")\n",
    "dummy_cols = [col for col in train_dummy.columns if \"store_type\" in col or \"region\" in col]\n",
    "print(dummy_cols)\n",
    "\n",
    "print(\"\\nFirst few rows of dummy variables:\")\n",
    "print(train_dummy[dummy_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Original categorical columns are removed after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that original columns are gone\n",
    "print(\"'store_type' in columns:\", \"store_type\" in train_dummy.columns)\n",
    "print(\"'region' in columns:\", \"region\" in train_dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Missing Value Imputation\n",
    "\n",
    "Handle missing values with:\n",
    "- **step_impute_mean()**: Replace NA with column mean\n",
    "- **step_impute_median()**: Replace NA with column median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train.isna().sum())\n",
    "\n",
    "print(f\"\\nMissing in 'advertising': {train['advertising'].isna().sum()} out of {len(train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with mean\n",
    "rec_impute_mean = (\n",
    "    recipe()\n",
    "    .step_impute_mean(columns=[\"advertising\"])\n",
    ")\n",
    "\n",
    "rec_impute_mean_fit = rec_impute_mean.prep(train)\n",
    "train_imputed = rec_impute_mean_fit.bake(train)\n",
    "\n",
    "print(\"After mean imputation:\")\n",
    "print(f\"Missing values: {train_imputed['advertising'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nOriginal mean: {train['advertising'].mean():.2f}\")\n",
    "print(f\"After imputation mean: {train_imputed['advertising'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with median\n",
    "rec_impute_median = (\n",
    "    recipe()\n",
    "    .step_impute_median(columns=[\"advertising\"])\n",
    ")\n",
    "\n",
    "rec_impute_median_fit = rec_impute_median.prep(train)\n",
    "train_imputed_median = rec_impute_median_fit.bake(train)\n",
    "\n",
    "print(\"After median imputation:\")\n",
    "print(f\"Missing values: {train_imputed_median['advertising'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nOriginal median: {train['advertising'].median():.2f}\")\n",
    "print(f\"After imputation median: {train_imputed_median['advertising'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Custom Transformations (step_mutate)\n",
    "\n",
    "Create new features using custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "rec_mutate = (\n",
    "    recipe()\n",
    "    .step_mutate({\n",
    "        \"price_squared\": lambda df: df[\"price\"] ** 2,\n",
    "        \"price_x_advertising\": lambda df: df[\"price\"] * df[\"advertising\"],\n",
    "        \"log_sales\": lambda df: np.log(df[\"sales\"])\n",
    "    })\n",
    ")\n",
    "\n",
    "rec_mutate_fit = rec_mutate.prep(train)\n",
    "train_mutated = rec_mutate_fit.bake(train)\n",
    "\n",
    "print(\"New columns created:\")\n",
    "new_cols = [\"price_squared\", \"price_x_advertising\", \"log_sales\"]\n",
    "print(train_mutated[new_cols].head())\n",
    "\n",
    "print(\"\\nOriginal columns preserved:\")\n",
    "print(train_mutated[[\"price\", \"advertising\", \"sales\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Multi-Step Pipelines\n",
    "\n",
    "Chain multiple preprocessing steps together.\n",
    "\n",
    "**Order matters!** Steps are applied sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive preprocessing pipeline\n",
    "rec_pipeline = (\n",
    "    recipe()\n",
    "    # 1. Handle missing values first\n",
    "    .step_impute_mean()\n",
    "    # 2. Create new features\n",
    "    .step_mutate({\n",
    "        \"price_squared\": lambda df: df[\"price\"] ** 2,\n",
    "        \"log_advertising\": lambda df: np.log(df[\"advertising\"] + 1)\n",
    "    })\n",
    "    # 3. Normalize numeric columns\n",
    "    .step_normalize()\n",
    "    # 4. Encode categorical variables\n",
    "    .step_dummy([\"store_type\", \"region\"])\n",
    ")\n",
    "\n",
    "print(f\"Pipeline has {len(rec_pipeline.steps)} steps:\")\n",
    "for i, step in enumerate(rec_pipeline.steps, 1):\n",
    "    print(f\"  {i}. {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the entire pipeline\n",
    "rec_pipeline_fit = rec_pipeline.prep(train)\n",
    "\n",
    "# Apply to train\n",
    "train_final = rec_pipeline_fit.bake(train)\n",
    "\n",
    "# Apply to test\n",
    "test_final = rec_pipeline_fit.bake(test)\n",
    "\n",
    "print(f\"Original train shape: {train.shape}\")\n",
    "print(f\"Final train shape: {train_final.shape}\")\n",
    "print(f\"\\nFinal columns: {list(train_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing values and standardization\n",
    "print(\"Missing values after pipeline:\")\n",
    "print(train_final.isna().sum().sum())\n",
    "\n",
    "print(\"\\nNumeric column statistics:\")\n",
    "numeric_cols = [\"sales\", \"price\", \"advertising\", \"temperature\", \"price_squared\", \"log_advertising\"]\n",
    "print(train_final[numeric_cols].describe().loc[[\"mean\", \"std\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Integration with Workflows\n",
    "\n",
    "Recipes integrate seamlessly with **py-workflows** for complete modeling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recipe\n",
    "rec_for_model = (\n",
    "    recipe()\n",
    "    .step_impute_mean()\n",
    "    .step_normalize(columns=[\"price\", \"advertising\", \"temperature\"])\n",
    "    .step_dummy([\"store_type\", \"region\"])\n",
    ")\n",
    "\n",
    "# Create workflow\n",
    "wf = (\n",
    "    workflow()\n",
    "    .add_recipe(rec_for_model)\n",
    "    .add_model(linear_reg().set_engine(\"sklearn\"))\n",
    ")\n",
    "\n",
    "print(\"Workflow created with recipe preprocessing\")\n",
    "print(f\"Preprocessor: {type(wf.preprocessor).__name__}\")\n",
    "print(f\"Model: {wf.spec.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit workflow (recipe is automatically prepped)\n",
    "wf_fit = wf.fit(train)\n",
    "\n",
    "print(\"Workflow fitted successfully\")\n",
    "print(f\"Preprocessor type: {type(wf_fit.pre).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data (recipe is automatically applied)\n",
    "predictions = wf_fit.predict(test)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predictions.head())\n",
    "\n",
    "print(f\"\\nPrediction shape: {predictions.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive outputs\n",
    "outputs, coefficients, stats = wf_fit.extract_outputs()\n",
    "\n",
    "print(\"Outputs DataFrame (observation-level):\")\n",
    "print(outputs.head())\n",
    "\n",
    "print(\"\\nCoefficients DataFrame (variable-level):\")\n",
    "print(coefficients[[\"variable\", \"coefficient\"]].head())\n",
    "\n",
    "print(\"\\nStats DataFrame (model-level):\")\n",
    "print(stats[stats[\"metric\"].isin([\"rmse\", \"mae\", \"r_squared\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Complete Example: Train/Test Evaluation\n",
    "\n",
    "A full example showing recipe → workflow → evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive recipe\n",
    "final_recipe = (\n",
    "    recipe()\n",
    "    # 1. Impute missing values\n",
    "    .step_impute_mean()\n",
    "    # 2. Engineer features\n",
    "    .step_mutate({\n",
    "        \"price_squared\": lambda df: df[\"price\"] ** 2,\n",
    "        \"temp_x_price\": lambda df: df[\"temperature\"] * df[\"price\"]\n",
    "    })\n",
    "    # 3. Normalize all numeric columns\n",
    "    .step_normalize()\n",
    "    # 4. Encode categories\n",
    "    .step_dummy([\"store_type\", \"region\"])\n",
    ")\n",
    "\n",
    "# Create workflow\n",
    "final_wf = (\n",
    "    workflow()\n",
    "    .add_recipe(final_recipe)\n",
    "    .add_model(linear_reg().set_engine(\"sklearn\"))\n",
    ")\n",
    "\n",
    "# Fit on train\n",
    "final_wf_fit = final_wf.fit(train)\n",
    "\n",
    "# Evaluate on test\n",
    "final_wf_fit = final_wf_fit.evaluate(test)\n",
    "\n",
    "print(\"Complete workflow fitted and evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all outputs\n",
    "outputs, coefficients, stats = final_wf_fit.extract_outputs()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OUTPUTS DATAFRAME (observation-level)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total observations: {len(outputs)}\")\n",
    "print(f\"Train: {len(outputs[outputs['split']=='train'])}\")\n",
    "print(f\"Test: {len(outputs[outputs['split']=='test'])}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(outputs[[\"actuals\", \"forecast\", \"residuals\", \"split\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COEFFICIENTS DATAFRAME (variable-level)\")\n",
    "print(\"=\"*60)\n",
    "print(coefficients[[\"variable\", \"coefficient\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STATS DATAFRAME (model-level metrics)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance metrics\n",
    "perf_metrics = stats[stats[\"metric\"].isin([\"rmse\", \"mae\", \"r_squared\"])]\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    split_metrics = perf_metrics[perf_metrics[\"split\"] == split]\n",
    "    for _, row in split_metrics.iterrows():\n",
    "        print(f\"  {row['metric']}: {row['value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the fitted recipe for inspection\n",
    "fitted_recipe = final_wf_fit.extract_preprocessor()\n",
    "\n",
    "print(\"Fitted Recipe:\")\n",
    "print(f\"Type: {type(fitted_recipe).__name__}\")\n",
    "print(f\"Number of prepared steps: {len(fitted_recipe.prepared_steps)}\")\n",
    "\n",
    "# Can use the fitted recipe independently\n",
    "new_data_transformed = fitted_recipe.bake(test)\n",
    "print(f\"\\nTransformed shape: {new_data_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**py-recipes** provides a powerful, composable system for feature engineering:\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **No Data Leakage**: Preprocessing fitted on train, applied to test\n",
    "2. **Reproducible**: Same transformations applied consistently\n",
    "3. **Composable**: Chain multiple steps together\n",
    "4. **Integrated**: Works seamlessly with py-workflows\n",
    "5. **Method Chaining**: Clean, readable syntax\n",
    "\n",
    "### Available Steps\n",
    "\n",
    "- `step_normalize()`: zscore or minmax normalization\n",
    "- `step_dummy()`: one-hot encoding\n",
    "- `step_impute_mean()`: mean imputation\n",
    "- `step_impute_median()`: median imputation\n",
    "- `step_mutate()`: custom transformations\n",
    "\n",
    "### Pattern\n",
    "\n",
    "```python\n",
    "# 1. Create recipe\n",
    "rec = recipe().step_normalize().step_dummy([\"category\"])\n",
    "\n",
    "# 2. Fit to training data\n",
    "rec_fit = rec.prep(train)\n",
    "\n",
    "# 3. Apply to any data\n",
    "train_transformed = rec_fit.bake(train)\n",
    "test_transformed = rec_fit.bake(test)\n",
    "```\n",
    "\n",
    "### Integration with Workflows\n",
    "\n",
    "```python\n",
    "wf = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(linear_reg().set_engine(\"sklearn\"))\n",
    ")\n",
    "wf_fit = wf.fit(train).evaluate(test)\n",
    "outputs, coefficients, stats = wf_fit.extract_outputs()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore `08_workflows_demo.ipynb` for more workflow examples\n",
    "- Check `02_parsnip_demo.ipynb` for model specifications\n",
    "- See `07_rsample_demo.ipynb` for time series cross-validation\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
