{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-yardstick Demo: Performance Metrics\n",
    "\n",
    "This notebook demonstrates the py-yardstick package for model evaluation metrics.\n",
    "\n",
    "py-yardstick provides:\n",
    "- Time series metrics (RMSE, MAE, MAPE, SMAPE, MASE, R²)\n",
    "- Residual diagnostic tests (Durbin-Watson, Ljung-Box, Shapiro-Wilk, ADF)\n",
    "- Classification metrics (Accuracy, Precision, Recall, F-measure, ROC AUC)\n",
    "- Metric set composition for batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py_yardstick import (\n",
    "    # Time Series Metrics\n",
    "    rmse, mae, mape, smape, mase, r_squared, rsq_trad, mda,\n",
    "    # Residual Diagnostics\n",
    "    durbin_watson, ljung_box, shapiro_wilk, adf_test,\n",
    "    # Classification Metrics\n",
    "    accuracy, precision, recall, f_meas, roc_auc,\n",
    "    # Metric Set\n",
    "    metric_set\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Regression Metrics\n",
    "\n",
    "Let's create some example predictions and calculate various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "n = 100\n",
    "truth = pd.Series(np.linspace(10, 100, n) + np.random.normal(0, 5, n))\n",
    "estimate = truth + np.random.normal(0, 3, n)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(truth.values, label='Truth', marker='o', markersize=3)\n",
    "plt.plot(estimate.values, label='Estimate', marker='x', markersize=3, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Truth vs Estimate')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE - Root Mean Squared Error\n",
    "print(\"RMSE:\")\n",
    "print(rmse(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean Absolute Error\n",
    "print(\"MAE:\")\n",
    "print(mae(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE - Mean Absolute Percentage Error\n",
    "print(\"MAPE (percentage):\")\n",
    "print(mape(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE - Symmetric Mean Absolute Percentage Error\n",
    "print(\"SMAPE (percentage, bounded 0-200):\")\n",
    "print(smape(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared\n",
    "print(\"R-squared:\")\n",
    "print(r_squared(truth, estimate))\n",
    "print()\n",
    "\n",
    "print(\"R-squared Traditional (squared correlation):\")\n",
    "print(rsq_trad(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASE - Mean Absolute Scaled Error\n",
    "\n",
    "MASE requires training data to compute the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_truth = truth[:70]\n",
    "test_truth = truth[70:]\n",
    "test_estimate = estimate[70:]\n",
    "\n",
    "print(\"MASE (scaled by naive forecast on training data):\")\n",
    "print(mase(test_truth, test_estimate, train=train_truth, m=1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDA - Mean Directional Accuracy\n",
    "\n",
    "MDA measures how often the predicted direction matches the actual direction (useful for time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MDA (proportion of correct directional predictions):\")\n",
    "print(mda(truth, estimate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metric Set - Compute Multiple Metrics at Once\n",
    "\n",
    "Use `metric_set()` to create a custom collection of metrics that can be computed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metric set\n",
    "my_metrics = metric_set(rmse, mae, mape, r_squared, mda)\n",
    "\n",
    "# Compute all metrics at once\n",
    "results = my_metrics(truth, estimate)\n",
    "print(\"All metrics computed together:\")\n",
    "print(results)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Diagnostic Tests\n",
    "\n",
    "These tests help evaluate model assumptions and residual properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = truth - estimate\n",
    "\n",
    "# Visualize residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Residuals plot\n",
    "axes[0].scatter(estimate, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Fitted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residuals vs Fitted')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson Test\n",
    "\n",
    "Tests for autocorrelation in residuals. Values range from 0 to 4:\n",
    "- 2 = no autocorrelation\n",
    "- < 2 = positive autocorrelation\n",
    "- > 2 = negative autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Durbin-Watson Test:\")\n",
    "dw_result = durbin_watson(residuals)\n",
    "print(dw_result)\n",
    "print(f\"\\nInterpretation: DW = {dw_result['value'].iloc[0]:.3f}\")\n",
    "if dw_result['value'].iloc[0] < 1.5:\n",
    "    print(\"→ Positive autocorrelation detected\")\n",
    "elif dw_result['value'].iloc[0] > 2.5:\n",
    "    print(\"→ Negative autocorrelation detected\")\n",
    "else:\n",
    "    print(\"→ Little or no autocorrelation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ljung-Box Test\n",
    "\n",
    "Tests for autocorrelation up to a specified lag. Returns both test statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ljung-Box Test (10 lags):\")\n",
    "lb_result = ljung_box(residuals, lags=10)\n",
    "print(lb_result)\n",
    "p_value = lb_result[lb_result['metric'] == 'ljung_box_p']['value'].iloc[0]\n",
    "print(f\"\\nInterpretation: p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"→ Reject null hypothesis: autocorrelation present\")\n",
    "else:\n",
    "    print(\"→ Fail to reject null hypothesis: no significant autocorrelation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk Test\n",
    "\n",
    "Tests for normality of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapiro-Wilk Normality Test:\")\n",
    "sw_result = shapiro_wilk(residuals)\n",
    "print(sw_result)\n",
    "p_value = sw_result[sw_result['metric'] == 'shapiro_wilk_p']['value'].iloc[0]\n",
    "print(f\"\\nInterpretation: p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"→ Reject null hypothesis: residuals are not normally distributed\")\n",
    "else:\n",
    "    print(\"→ Fail to reject null hypothesis: residuals appear normally distributed\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller Test\n",
    "\n",
    "Tests for stationarity in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmented Dickey-Fuller Test (testing stationarity of truth series):\")\n",
    "adf_result = adf_test(truth)\n",
    "print(adf_result)\n",
    "p_value = adf_result[adf_result['metric'] == 'adf_p']['value'].iloc[0]\n",
    "print(f\"\\nInterpretation: p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"→ Reject null hypothesis: series is stationary\")\n",
    "else:\n",
    "    print(\"→ Fail to reject null hypothesis: unit root present (non-stationary)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Metrics\n",
    "\n",
    "Let's create binary classification data and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification data\n",
    "n_samples = 200\n",
    "truth_class = pd.Series(np.random.binomial(1, 0.5, n_samples))\n",
    "\n",
    "# Generate predictions (probabilities)\n",
    "# Good classifier: higher prob for class 1, lower for class 0\n",
    "probs = truth_class * np.random.beta(8, 2, n_samples) + (1 - truth_class) * np.random.beta(2, 8, n_samples)\n",
    "estimate_probs = pd.Series(probs)\n",
    "\n",
    "# Convert to class labels\n",
    "estimate_class = pd.Series((estimate_probs > 0.5).astype(int))\n",
    "\n",
    "print(f\"True class distribution: {truth_class.value_counts().to_dict()}\")\n",
    "print(f\"Predicted class distribution: {estimate_class.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(truth_class, estimate_class)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy(truth_class, estimate_class))\n",
    "print()\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\")\n",
    "print(precision(truth_class, estimate_class))\n",
    "print()\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\")\n",
    "print(recall(truth_class, estimate_class))\n",
    "print()\n",
    "\n",
    "# F1-Score\n",
    "print(\"F1-Score:\")\n",
    "print(f_meas(truth_class, estimate_class))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC (requires probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC:\")\n",
    "print(roc_auc(truth_class, estimate_probs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(truth_class, estimate_probs)\n",
    "roc_auc_val = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_val:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metric Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification metric set\n",
    "clf_metrics = metric_set(accuracy, precision, recall, f_meas)\n",
    "\n",
    "# Compute all at once\n",
    "clf_results = clf_metrics(truth_class, estimate_class)\n",
    "print(\"Classification Metrics Summary:\")\n",
    "print(clf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with py-parsnip Models\n",
    "\n",
    "Let's use yardstick metrics with actual model predictions from py-parsnip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_parsnip import linear_reg\n",
    "from py_rsample import initial_time_split, training, testing\n",
    "\n",
    "# Create sample time series data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "dates = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'price': np.random.uniform(10, 50, n),\n",
    "    'promotion': np.random.binomial(1, 0.3, n),\n",
    "    'sales': 100 + np.random.uniform(0, 20, n) * np.sin(np.linspace(0, 4*np.pi, n)) + np.random.normal(0, 5, n)\n",
    "})\n",
    "\n",
    "# Add effects\n",
    "sales_data['sales'] = sales_data['sales'] - 0.5 * sales_data['price'] + 10 * sales_data['promotion']\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(sales_data.head())\n",
    "print(f\"\\nShape: {sales_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "split = initial_time_split(sales_data, prop=0.75)\n",
    "train_data = training(split)\n",
    "test_data = testing(split)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model = linear_reg().set_engine('sklearn')\n",
    "fit = model.fit(train_data, 'sales ~ price + promotion')\n",
    "\n",
    "# Get predictions\n",
    "train_preds = fit.predict(train_data)\n",
    "test_preds = fit.predict(test_data)\n",
    "\n",
    "print(\"Training predictions:\")\n",
    "print(train_preds.head())\n",
    "print(\"\\nTest predictions:\")\n",
    "print(test_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with yardstick metrics\n",
    "train_truth = train_data['sales']\n",
    "train_estimate = train_preds['.pred']\n",
    "\n",
    "test_truth = test_data['sales']\n",
    "test_estimate = test_preds['.pred']\n",
    "\n",
    "# Define metric set\n",
    "regression_metrics = metric_set(rmse, mae, mape, r_squared)\n",
    "\n",
    "# Compute metrics for both train and test\n",
    "train_metrics = regression_metrics(train_truth, train_estimate)\n",
    "train_metrics['split'] = 'train'\n",
    "\n",
    "test_metrics = regression_metrics(test_truth, test_estimate)\n",
    "test_metrics['split'] = 'test'\n",
    "\n",
    "# Combine\n",
    "all_metrics = pd.concat([train_metrics, test_metrics], ignore_index=True)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(all_metrics.pivot(index='metric', columns='split', values='value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training\n",
    "axes[0].scatter(train_truth, train_estimate, alpha=0.5)\n",
    "axes[0].plot([train_truth.min(), train_truth.max()], \n",
    "             [train_truth.min(), train_truth.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Sales')\n",
    "axes[0].set_ylabel('Predicted Sales')\n",
    "axes[0].set_title('Training Set Predictions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Testing\n",
    "axes[1].scatter(test_truth, test_estimate, alpha=0.5, color='orange')\n",
    "axes[1].plot([test_truth.min(), test_truth.max()], \n",
    "             [test_truth.min(), test_truth.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Sales')\n",
    "axes[1].set_ylabel('Predicted Sales')\n",
    "axes[1].set_title('Test Set Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Multiple Models\n",
    "\n",
    "Use yardstick to compare different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit multiple models\n",
    "model1 = linear_reg().set_engine('sklearn')  # OLS\n",
    "model2 = linear_reg(penalty=0.1).set_engine('sklearn')  # Ridge\n",
    "model3 = linear_reg(penalty=1.0).set_engine('sklearn')  # Ridge with more regularization\n",
    "\n",
    "fit1 = model1.fit(train_data, 'sales ~ price + promotion')\n",
    "fit2 = model2.fit(train_data, 'sales ~ price + promotion')\n",
    "fit3 = model3.fit(train_data, 'sales ~ price + promotion')\n",
    "\n",
    "# Get test predictions\n",
    "pred1 = fit1.predict(test_data)['.pred']\n",
    "pred2 = fit2.predict(test_data)['.pred']\n",
    "pred3 = fit3.predict(test_data)['.pred']\n",
    "\n",
    "# Compute metrics\n",
    "metrics1 = regression_metrics(test_truth, pred1)\n",
    "metrics1['model'] = 'OLS'\n",
    "\n",
    "metrics2 = regression_metrics(test_truth, pred2)\n",
    "metrics2['model'] = 'Ridge (0.1)'\n",
    "\n",
    "metrics3 = regression_metrics(test_truth, pred3)\n",
    "metrics3['model'] = 'Ridge (1.0)'\n",
    "\n",
    "# Combine and pivot\n",
    "comparison = pd.concat([metrics1, metrics2, metrics3], ignore_index=True)\n",
    "comparison_wide = comparison.pivot(index='metric', columns='model', values='value')\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "comparison_plot = comparison[comparison['metric'].isin(['rmse', 'mae', 'r_squared'])]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(comparison_plot['metric'].unique()))\n",
    "width = 0.25\n",
    "models = comparison_plot['model'].unique()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = comparison_plot[comparison_plot['model'] == model]\n",
    "    ax.bar(x + i*width, model_data['value'], width, label=model)\n",
    "\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(comparison_plot['metric'].unique())\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Time Series Metrics**: RMSE, MAE, MAPE, SMAPE, MASE, R², MDA\n",
    "2. **Metric Sets**: Composing multiple metrics for batch evaluation\n",
    "3. **Residual Diagnostics**: Durbin-Watson, Ljung-Box, Shapiro-Wilk, ADF tests\n",
    "4. **Classification Metrics**: Accuracy, Precision, Recall, F-measure, ROC AUC\n",
    "5. **Integration**: Using yardstick with py-parsnip models\n",
    "6. **Model Comparison**: Evaluating multiple models systematically\n",
    "\n",
    "All metrics follow a consistent API:\n",
    "- Accept truth and estimate as primary arguments\n",
    "- Return standardized DataFrames with `metric` and `value` columns\n",
    "- Can be composed using `metric_set()` for batch evaluation\n",
    "- Handle edge cases gracefully (NaN values, empty data, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
