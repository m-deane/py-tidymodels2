{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models: null_model() and naive_reg()\n",
    "\n",
    "This notebook demonstrates **baseline models** in py-tidymodels, which are essential for benchmarking any modeling project.\n",
    "\n",
    "## Why Baselines are Critical\n",
    "\n",
    "**Rule #1 of Machine Learning:** If your model can't beat a simple baseline, it's not adding value.\n",
    "\n",
    "Baseline models provide:\n",
    "- **Reality check**: Is your complex model actually learning something useful?\n",
    "- **Minimum bar**: The simplest possible prediction strategy\n",
    "- **Quick implementation**: Minutes to fit, not hours\n",
    "- **Interpretability**: Everyone understands \"predict the mean\" or \"use last value\"\n",
    "\n",
    "## Models Covered\n",
    "\n",
    "1. **null_model()** - Predicts a constant (mean, median, or mode)\n",
    "   - Regression: mean or median of training target\n",
    "   - Classification: most frequent class\n",
    "   - Use case: General baseline for any problem\n",
    "\n",
    "2. **naive_reg()** - Time series naive forecasting methods\n",
    "   - **naive**: Last observed value (random walk)\n",
    "   - **seasonal_naive**: Last value from same season\n",
    "   - **drift**: Linear trend extrapolation\n",
    "   - Use case: Essential baseline for time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import py-tidymodels packages\n",
    "from py_parsnip import null_model, naive_reg, linear_reg, rand_forest\n",
    "from py_recipes import recipe, step_date, step_lag, step_normalize\n",
    "from py_workflows import workflow\n",
    "from py_rsample import initial_time_split\n",
    "from py_visualize import plot_model_comparison\n",
    "\n",
    "print(\"âœ“ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: null_model() - The Simplest Baseline\n",
    "\n",
    "The null model is the most basic baseline: predict a constant for all observations.\n",
    "\n",
    "### For Regression:\n",
    "- Predicts the **mean** (or median) of the training target\n",
    "- Represents \"what if I just guessed the average every time?\"\n",
    "- RMSE of null model = standard deviation of target\n",
    "\n",
    "### For Classification:\n",
    "- Predicts the **mode** (most frequent class)\n",
    "- Represents \"what if I just guessed the majority class every time?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple regression data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Features\n",
    "x1 = np.random.randn(n)\n",
    "x2 = np.random.randn(n)\n",
    "x3 = np.random.randn(n)\n",
    "\n",
    "# Target: linear relationship + noise\n",
    "y = 10 + 2 * x1 + 1.5 * x2 - 0.8 * x3 + np.random.randn(n) * 2\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x3': x3,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# Train/test split\n",
    "train_data = data.iloc[:150]\n",
    "test_data = data.iloc[150:]\n",
    "\n",
    "print(f\"Training observations: {len(train_data)}\")\n",
    "print(f\"Testing observations: {len(test_data)}\")\n",
    "print(f\"\\nTarget statistics (training):\")\n",
    "print(f\"  Mean: {train_data['y'].mean():.4f}\")\n",
    "print(f\"  Std:  {train_data['y'].std():.4f}\")\n",
    "print(f\"  Min:  {train_data['y'].min():.4f}\")\n",
    "print(f\"  Max:  {train_data['y'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit null_model() - Predicts Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create null model specification\n",
    "spec_null = null_model()\n",
    "\n",
    "print(\"Null Model Specification:\")\n",
    "print(spec_null)\n",
    "print(\"\\nThis model will predict the mean of the training target for all observations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit null model (no features needed, but we'll use the formula for consistency)\n",
    "fit_null = spec_null.fit(formula='y ~ 1', data=train_data)\n",
    "\n",
    "# Generate predictions\n",
    "pred_null = fit_null.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Null model fitted\")\n",
    "print(f\"\\nPredicted value (constant): {pred_null['.pred'].iloc[0]:.4f}\")\n",
    "print(f\"Training mean:              {train_data['y'].mean():.4f}\")\n",
    "print(\"\\nNote: All predictions are identical (the training mean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Linear Regression and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression\n",
    "spec_linear = linear_reg()\n",
    "fit_linear = spec_linear.fit(formula='y ~ x1 + x2 + x3', data=train_data)\n",
    "pred_linear = fit_linear.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Linear regression fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit random forest\n",
    "spec_rf = rand_forest(trees=100, mode='regression')\n",
    "fit_rf = spec_rf.fit(formula='y ~ x1 + x2 + x3', data=train_data)\n",
    "pred_rf = fit_rf.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Random forest fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metrics and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stats DataFrames\n",
    "_, _, stats_null = fit_null.extract_outputs()\n",
    "_, _, stats_linear = fit_linear.extract_outputs()\n",
    "_, _, stats_rf = fit_rf.extract_outputs()\n",
    "\n",
    "print(\"Null Model Performance (Test Set):\")\n",
    "print(\"=\" * 50)\n",
    "test_stats_null = stats_null[stats_null['split'] == 'test']\n",
    "for _, row in test_stats_null.iterrows():\n",
    "    print(f\"{row['metric']:12s}: {row['value']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig = plot_model_comparison(\n",
    "    stats_list=[stats_null, stats_linear, stats_rf],\n",
    "    model_names=[\"Null Model (Mean)\", \"Linear Regression\", \"Random Forest\"],\n",
    "    metrics=[\"rmse\", \"mae\", \"r_squared\"],\n",
    "    split=\"test\",\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Model Performance vs Null Baseline\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"  â€¢ Null model: RMSE â‰ˆ Std(y) - no learning, just predicts mean\")\n",
    "print(\"  â€¢ Linear & RF: RMSE < Std(y) - they've learned something!\")\n",
    "print(\"  â€¢ RÂ² = 0 for null model (baseline)\")\n",
    "print(\"  â€¢ If RÂ² < 0, your model is WORSE than null model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights: null_model()\n",
    "\n",
    "1. **RMSE of null model â‰ˆ Standard deviation of target**\n",
    "   - This is the \"no learning\" baseline\n",
    "   - Any model with RMSE > Ïƒ(y) is worse than predicting the mean\n",
    "\n",
    "2. **RÂ² is defined relative to null model**\n",
    "   - RÂ² = 1 - (SSE / SST) where SST = variance explained by null model\n",
    "   - RÂ² = 0 means you're as good as null model\n",
    "   - RÂ² < 0 means you're WORSE than null model\n",
    "\n",
    "3. **Use cases:**\n",
    "   - First baseline for ANY regression problem\n",
    "   - Sanity check before complex modeling\n",
    "   - Quick benchmark for model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: naive_reg() - Time Series Baselines\n",
    "\n",
    "For time series forecasting, we need different baselines that account for temporal structure.\n",
    "\n",
    "### Three Naive Methods:\n",
    "\n",
    "1. **Naive (Random Walk)**\n",
    "   - Formula: Å·_t = y_{t-1}\n",
    "   - Prediction: \"Tomorrow will be like today\"\n",
    "   - Best for: Random walks, stock prices\n",
    "\n",
    "2. **Seasonal Naive**\n",
    "   - Formula: Å·_t = y_{t-seasonal_period}\n",
    "   - Prediction: \"This Monday will be like last Monday\"\n",
    "   - Best for: Strong seasonal patterns\n",
    "\n",
    "3. **Drift Method**\n",
    "   - Formula: Å·_t = y_{t-1} + (y_T - y_1) / (T - 1)\n",
    "   - Prediction: \"Continue the linear trend\"\n",
    "   - Best for: Data with consistent trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Create Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series with trend + seasonality\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2020-01-01', periods=365, freq='D')\n",
    "time_index = np.arange(len(dates))\n",
    "\n",
    "# Components\n",
    "trend = time_index * 0.3  # Upward trend\n",
    "weekly_season = 10 * np.sin(2 * np.pi * time_index / 7)  # Weekly pattern\n",
    "noise = np.random.randn(len(dates)) * 3\n",
    "\n",
    "y = trend + weekly_season + noise + 100\n",
    "\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': y\n",
    "})\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "split = initial_time_split(ts_data, prop=0.8)\n",
    "train_ts = split.training()\n",
    "test_ts = split.testing()\n",
    "\n",
    "print(f\"Training: {len(train_ts)} observations ({train_ts['date'].min()} to {train_ts['date'].max()})\")\n",
    "print(f\"Testing:  {len(test_ts)} observations ({test_ts['date'].min()} to {test_ts['date'].max()})\")\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  - Trend: +0.3 per day\")\n",
    "print(f\"  - Seasonality: Weekly (period=7)\")\n",
    "print(f\"  - Noise: Ïƒ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Naive (Last Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create naive model (method=\"naive\" is default)\n",
    "spec_naive = naive_reg(method=\"naive\")\n",
    "\n",
    "print(\"Naive Model (Random Walk):\")\n",
    "print(spec_naive)\n",
    "print(\"\\nPrediction strategy: Å·_t = y_{t-1}\")\n",
    "print(\"Use case: Random walks, efficient markets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit naive model\n",
    "fit_naive = spec_naive.fit(formula='value ~ date', data=train_ts)\n",
    "pred_naive = fit_naive.predict(test_ts)\n",
    "\n",
    "print(\"âœ“ Naive model fitted\")\n",
    "print(f\"\\nLast training value: {train_ts['value'].iloc[-1]:.4f}\")\n",
    "print(f\"First prediction:    {pred_naive['.pred'].iloc[0]:.4f}\")\n",
    "print(\"\\nNote: First prediction = last training value (random walk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Seasonal Naive (Last Seasonal Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seasonal naive model (weekly seasonality = 7 days)\n",
    "spec_snaive = naive_reg(seasonal_period=7, method=\"seasonal_naive\")\n",
    "\n",
    "print(\"Seasonal Naive Model:\")\n",
    "print(spec_snaive)\n",
    "print(\"\\nPrediction strategy: Å·_t = y_{t-7}\")\n",
    "print(\"Use case: Strong seasonal patterns (e.g., Monday like last Monday)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit seasonal naive model\n",
    "fit_snaive = spec_snaive.fit(formula='value ~ date', data=train_ts)\n",
    "pred_snaive = fit_snaive.predict(test_ts)\n",
    "\n",
    "print(\"âœ“ Seasonal naive model fitted\")\n",
    "print(\"\\nPrediction logic:\")\n",
    "print(\"  - First test date (Thursday): uses last Thursday's value\")\n",
    "print(\"  - Each day: uses same weekday from 7 days ago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Drift (Linear Trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drift model\n",
    "spec_drift = naive_reg(method=\"drift\")\n",
    "\n",
    "print(\"Drift Model:\")\n",
    "print(spec_drift)\n",
    "print(\"\\nPrediction strategy: Å·_t = y_{t-1} + (y_T - y_1) / (T - 1)\")\n",
    "print(\"Use case: Data with consistent linear trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit drift model\n",
    "fit_drift = spec_drift.fit(formula='value ~ date', data=train_ts)\n",
    "pred_drift = fit_drift.predict(test_ts)\n",
    "\n",
    "# Calculate drift rate\n",
    "first_val = train_ts['value'].iloc[0]\n",
    "last_val = train_ts['value'].iloc[-1]\n",
    "n_obs = len(train_ts)\n",
    "drift_rate = (last_val - first_val) / (n_obs - 1)\n",
    "\n",
    "print(\"âœ“ Drift model fitted\")\n",
    "print(f\"\\nDrift rate: {drift_rate:.4f} per day\")\n",
    "print(f\"First training value: {first_val:.4f}\")\n",
    "print(f\"Last training value:  {last_val:.4f}\")\n",
    "print(f\"\\nPredictions extrapolate this linear trend forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare All Three Naive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stats\n",
    "_, _, stats_naive = fit_naive.extract_outputs()\n",
    "_, _, stats_snaive = fit_snaive.extract_outputs()\n",
    "_, _, stats_drift = fit_drift.extract_outputs()\n",
    "\n",
    "# Visual comparison\n",
    "fig = plot_model_comparison(\n",
    "    stats_list=[stats_naive, stats_snaive, stats_drift],\n",
    "    model_names=[\"Naive (Last Value)\", \"Seasonal Naive (Weekly)\", \"Drift (Trend)\"],\n",
    "    metrics=[\"rmse\", \"mae\", \"r_squared\"],\n",
    "    split=\"test\",\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Naive Methods Comparison\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Which naive method performs best?\")\n",
    "test_rmses = {\n",
    "    \"Naive\": stats_naive[stats_naive['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Seasonal Naive\": stats_snaive[stats_snaive['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Drift\": stats_drift[stats_drift['metric'] == 'rmse']['value'].values[1]\n",
    "}\n",
    "\n",
    "best_method = min(test_rmses, key=test_rmses.get)\n",
    "print(f\"\\nBest naive method: {best_method}\")\n",
    "print(f\"  RMSE: {test_rmses[best_method]:.4f}\")\n",
    "print(\"\\nExpected: Seasonal Naive should win (data has weekly seasonality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Naive Methods to ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ML models with feature engineering\n",
    "rec = (\n",
    "    recipe(value ~ date, data=train_ts)\n",
    "    .step_date('date', features=['dow', 'week'])\n",
    "    .step_lag('value', lags=[1, 7, 14])\n",
    "    .step_normalize(['value_lag_1', 'value_lag_7', 'value_lag_14'])\n",
    ")\n",
    "\n",
    "# Linear regression with features\n",
    "wf_linear = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(linear_reg())\n",
    ")\n",
    "fit_linear_ts = wf_linear.fit(train_ts)\n",
    "\n",
    "# Random forest with features\n",
    "wf_rf = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(rand_forest(trees=100, mode='regression'))\n",
    ")\n",
    "fit_rf_ts = wf_rf.fit(train_ts)\n",
    "\n",
    "print(\"âœ“ ML models fitted with date features and lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stats\n",
    "_, _, stats_linear_ts = fit_linear_ts.extract_outputs()\n",
    "_, _, stats_rf_ts = fit_rf_ts.extract_outputs()\n",
    "\n",
    "# Compare all models\n",
    "fig = plot_model_comparison(\n",
    "    stats_list=[\n",
    "        stats_naive, stats_snaive, stats_drift,\n",
    "        stats_linear_ts, stats_rf_ts\n",
    "    ],\n",
    "    model_names=[\n",
    "        \"Naive\", \"Seasonal Naive\", \"Drift\",\n",
    "        \"Linear Regression\", \"Random Forest\"\n",
    "    ],\n",
    "    metrics=[\"rmse\", \"mae\", \"r_squared\"],\n",
    "    split=\"test\",\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Naive Baselines vs ML Models\",\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Question: Do ML models beat the baselines?\")\n",
    "print(\"\\nTest RMSE:\")\n",
    "all_rmses = {\n",
    "    \"Naive\": stats_naive[stats_naive['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Seasonal Naive\": stats_snaive[stats_snaive['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Drift\": stats_drift[stats_drift['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Linear Regression\": stats_linear_ts[stats_linear_ts['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Random Forest\": stats_rf_ts[stats_rf_ts['metric'] == 'rmse']['value'].values[1]\n",
    "}\n",
    "\n",
    "for model, rmse in sorted(all_rmses.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {model:20s}: {rmse:.4f}\")\n",
    "\n",
    "best_overall = min(all_rmses, key=all_rmses.get)\n",
    "print(f\"\\nðŸ† Best model: {best_overall}\")\n",
    "\n",
    "# Check if ML beat best naive\n",
    "best_naive_rmse = min([test_rmses[k] for k in test_rmses.keys()])\n",
    "ml_beat_naive = min(all_rmses['Linear Regression'], all_rmses['Random Forest']) < best_naive_rmse\n",
    "\n",
    "if ml_beat_naive:\n",
    "    print(\"\\nâœ“ ML models beat naive baselines - they're adding value!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ML models didn't beat naive baselines - reconsider your approach!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Naive Method\n",
    "\n",
    "| Method | Best For | Example Use Cases |\n",
    "|--------|----------|------------------|\n",
    "| **Naive** | Random walks, no pattern | Stock prices, exchange rates |\n",
    "| **Seasonal Naive** | Strong seasonality | Retail sales, website traffic, temperature |\n",
    "| **Drift** | Consistent linear trend | Population growth, cumulative metrics |\n",
    "\n",
    "### Rule of Thumb:\n",
    "1. **Always** try all three naive methods\n",
    "2. The best naive method is your **minimum acceptable baseline**\n",
    "3. If your ML model can't beat it, **don't deploy the ML model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Benchmarking Workflow\n",
    "\n",
    "Here's a recommended workflow for any modeling project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit baseline(s)\n",
    "print(\"Step 1: Establish Baseline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For regression: null_model\n",
    "baseline = null_model()\n",
    "fit_baseline = baseline.fit(formula='y ~ 1', data=train_data)\n",
    "_, _, stats_baseline = fit_baseline.extract_outputs()\n",
    "baseline_rmse = stats_baseline[stats_baseline['metric'] == 'rmse']['value'].values[1]\n",
    "\n",
    "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n",
    "print(\"This is the minimum bar. Any model must beat this.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try simple model\n",
    "print(\"Step 2: Try Simple Model (Linear Regression)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "simple_model = linear_reg()\n",
    "fit_simple = simple_model.fit(formula='y ~ x1 + x2 + x3', data=train_data)\n",
    "_, _, stats_simple = fit_simple.extract_outputs()\n",
    "simple_rmse = stats_simple[stats_simple['metric'] == 'rmse']['value'].values[1]\n",
    "\n",
    "print(f\"Simple Model RMSE: {simple_rmse:.4f}\")\n",
    "\n",
    "improvement_pct = ((baseline_rmse - simple_rmse) / baseline_rmse) * 100\n",
    "print(f\"Improvement: {improvement_pct:.1f}% over baseline\")\n",
    "\n",
    "if simple_rmse < baseline_rmse:\n",
    "    print(\"âœ“ Simple model beats baseline - proceed to complex models\\n\")\n",
    "else:\n",
    "    print(\"âš ï¸  Simple model doesn't beat baseline - check data quality!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Try complex model (only if simple model worked)\n",
    "print(\"Step 3: Try Complex Model (Random Forest)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "complex_model = rand_forest(trees=100, mode='regression')\n",
    "fit_complex = complex_model.fit(formula='y ~ x1 + x2 + x3', data=train_data)\n",
    "_, _, stats_complex = fit_complex.extract_outputs()\n",
    "complex_rmse = stats_complex[stats_complex['metric'] == 'rmse']['value'].values[1]\n",
    "\n",
    "print(f\"Complex Model RMSE: {complex_rmse:.4f}\")\n",
    "\n",
    "improvement_vs_simple = ((simple_rmse - complex_rmse) / simple_rmse) * 100\n",
    "improvement_vs_baseline = ((baseline_rmse - complex_rmse) / baseline_rmse) * 100\n",
    "\n",
    "print(f\"Improvement vs simple model: {improvement_vs_simple:.1f}%\")\n",
    "print(f\"Improvement vs baseline:     {improvement_vs_baseline:.1f}%\")\n",
    "\n",
    "if complex_rmse < simple_rmse * 0.95:  # At least 5% better\n",
    "    print(\"âœ“ Complex model significantly better - use it!\")\n",
    "elif complex_rmse < simple_rmse:\n",
    "    print(\"âš ï¸  Complex model slightly better - maybe not worth the complexity\")\n",
    "else:\n",
    "    print(\"âš ï¸  Complex model not better - stick with simple model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Summarize\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nPerformance Ladder:\")\n",
    "results = [\n",
    "    (\"Baseline (null_model)\", baseline_rmse),\n",
    "    (\"Simple (linear_reg)\", simple_rmse),\n",
    "    (\"Complex (rand_forest)\", complex_rmse)\n",
    "]\n",
    "\n",
    "for i, (name, rmse) in enumerate(sorted(results, key=lambda x: x[1]), 1):\n",
    "    print(f\"  {i}. {name:25s} RMSE = {rmse:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Decision Framework:\")\n",
    "print(\"  1. If NOTHING beats baseline â†’ Don't use ML, predict mean\")\n",
    "print(\"  2. If only simple beats baseline â†’ Use simple model\")\n",
    "print(\"  3. If complex beats simple by >5% â†’ Use complex model\")\n",
    "print(\"  4. If complex beats simple by <5% â†’ Stick with simple (interpretability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extract Outputs - Three-DataFrame Structure\n",
    "\n",
    "All fitted models in py-tidymodels return three DataFrames via `extract_outputs()`:\n",
    "1. **outputs**: Predictions with actuals\n",
    "2. **residuals**: Residual diagnostics\n",
    "3. **stats**: Performance metrics\n",
    "\n",
    "Let's see this for baseline models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from null model\n",
    "outputs_null, residuals_null, stats_null_full = fit_null.extract_outputs()\n",
    "\n",
    "print(\"DataFrame 1: outputs (predictions + actuals)\")\n",
    "print(\"=\" * 50)\n",
    "print(outputs_null.head(10))\n",
    "print(f\"\\nShape: {outputs_null.shape}\")\n",
    "print(f\"Columns: {list(outputs_null.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataFrame 2: residuals (diagnostic info)\")\n",
    "print(\"=\" * 50)\n",
    "print(residuals_null.head(10))\n",
    "print(f\"\\nShape: {residuals_null.shape}\")\n",
    "print(f\"Columns: {list(residuals_null.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataFrame 3: stats (performance metrics)\")\n",
    "print(\"=\" * 50)\n",
    "print(stats_null_full)\n",
    "print(f\"\\nShape: {stats_null_full.shape}\")\n",
    "print(f\"\\nMetrics available: {stats_null_full['metric'].unique()}\")\n",
    "print(f\"Splits: {stats_null_full['split'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same structure for naive_reg\n",
    "outputs_naive, residuals_naive, stats_naive_full = fit_naive.extract_outputs()\n",
    "\n",
    "print(\"naive_reg() also returns three DataFrames:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"outputs:   {outputs_naive.shape}\")\n",
    "print(f\"residuals: {residuals_naive.shape}\")\n",
    "print(f\"stats:     {stats_naive_full.shape}\")\n",
    "print(\"\\nConsistent structure across ALL models in py-tidymodels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Baseline Models\n",
    "\n",
    "### null_model() - Universal Baseline\n",
    "\n",
    "**What it does:**\n",
    "- Regression: Predicts mean (or median) of training target\n",
    "- Classification: Predicts mode (most frequent class)\n",
    "\n",
    "**When to use:**\n",
    "- First baseline for ANY regression/classification problem\n",
    "- Reality check before complex modeling\n",
    "- Quick benchmark for model selection\n",
    "\n",
    "**Key insight:**\n",
    "- RMSE of null model â‰ˆ Ïƒ(y)\n",
    "- RÂ² = 0 by definition\n",
    "- If your model has RÂ² < 0, it's worse than null model!\n",
    "\n",
    "### naive_reg() - Time Series Baselines\n",
    "\n",
    "**Three methods:**\n",
    "\n",
    "1. **Naive (Last Value)**\n",
    "   - Best for: Random walks, no pattern\n",
    "   - Formula: Å·_t = y_{t-1}\n",
    "\n",
    "2. **Seasonal Naive**\n",
    "   - Best for: Strong seasonality\n",
    "   - Formula: Å·_t = y_{t-seasonal_period}\n",
    "\n",
    "3. **Drift (Linear Trend)**\n",
    "   - Best for: Consistent linear trend\n",
    "   - Formula: Å·_t = y_{t-1} + trend\n",
    "\n",
    "**When to use:**\n",
    "- Essential baseline for ANY time series forecasting\n",
    "- Try all three, use the best as minimum bar\n",
    "- If ML can't beat them, don't use ML!\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "**If your model can't beat a simple baseline, it's not adding value.**\n",
    "\n",
    "- Baselines are fast to implement (minutes, not hours)\n",
    "- Baselines are easy to explain (everyone understands \"predict the mean\")\n",
    "- Baselines provide minimum acceptable performance\n",
    "- Complex models must justify their complexity by beating baselines\n",
    "\n",
    "### Best Practice Workflow\n",
    "\n",
    "1. Fit appropriate baseline (null_model or naive_reg)\n",
    "2. Try simple model (e.g., linear regression)\n",
    "3. Only try complex models if simple model beats baseline\n",
    "4. Complex model must beat simple model by significant margin (>5%)\n",
    "5. Always report performance relative to baseline\n",
    "\n",
    "**Remember:** The best model is often the simplest one that beats the baseline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
