{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 32: Baseline Forecasting Models\n",
    "\n",
    "**New in v1.0.0**: `null_model()` and `naive_reg()` for baseline forecasting\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the NEW baseline forecasting models added in py-tidymodels v1.0.0:\n",
    "\n",
    "### `null_model()` - Statistical Baselines\n",
    "- **mean**: Average of historical values\n",
    "- **median**: Median of historical values\n",
    "- **last**: Last observed value (persistence)\n",
    "\n",
    "### `naive_reg()` - Time Series Baselines\n",
    "- **naive**: Last observation (same as null_model last)\n",
    "- **seasonal_naive**: Value from same period last season\n",
    "- **drift**: Linear trend from first to last observation\n",
    "- **window**: Moving average over recent window\n",
    "\n",
    "## When to Use Baseline Models\n",
    "\n",
    "**Critical for model evaluation**:\n",
    "1. Establish performance floor - any advanced model should beat baselines\n",
    "2. Quick sanity checks - if complex model loses to mean baseline, investigate!\n",
    "3. Production fallbacks - use baselines when advanced models fail\n",
    "4. Benchmark comparisons - report \"% improvement over naive forecast\"\n",
    "\n",
    "**Sometimes baselines are sufficient**:\n",
    "- Stable time series with no trend/seasonality\n",
    "- Limited historical data (< 50 observations)\n",
    "- High forecast uncertainty makes complex models overkill\n",
    "- Interpretability is paramount\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**JODI Crude Oil Production** (Saudi Arabia):\n",
    "- Monthly production from 2002-2024\n",
    "- Units: Thousands of barrels per day (KBD)\n",
    "- Major oil producer with stable production patterns\n",
    "- Source: Joint Organisations Data Initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_parsnip import null_model, naive_reg, linear_reg, prophet_reg\n",
    "from py_workflows import Workflow\n",
    "from py_rsample import initial_time_split\n",
    "from py_yardstick import rmse, mae, mape\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_yardstick import metric_set\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JODI crude production data\n",
    "df = pd.read_csv('../_md/__data/jodi_crude_production_data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Filter to Saudi Arabia only\n",
    "saudi = df[df['country'] == 'Saudi Arabia'].copy()\n",
    "saudi = saudi[['date', 'value']].rename(columns={'value': 'production'})\n",
    "saudi = saudi.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Remove zeros (missing months)\n",
    "saudi = saudi[saudi['production'] > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saudi Arabia crude oil production:\")\n",
    "print(f\"  Records: {len(saudi):,}\")\n",
    "print(f\"  Date range: {saudi['date'].min()} to {saudi['date'].max()}\")\n",
    "print(f\"  Mean production: {saudi['production'].mean():.0f} KBD\")\n",
    "print(f\"  Std dev: {saudi['production'].std():.0f} KBD\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(saudi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (hold out last 12 months)\n",
    "split = initial_time_split(saudi, date_column='date', prop=0.90)\n",
    "train = split.training()\n",
    "test = split.testing()\n",
    "\n",
    "print(f\"Train: {len(train)} months ({train['date'].min()} to {train['date'].max()})\")\n",
    "print(f\"Test:  {len(test)} months ({test['date'].min()} to {test['date'].max()})\")\n",
    "print(f\"\\nTest period: {len(test)} months for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. null_model() - Statistical Baselines\n",
    "\n",
    "These models use simple statistics from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Baseline\n",
    "\n",
    "Forecast = mean of all training values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline: predicts average of training data\n",
    "mean_spec = null_model(strategy='mean')\n",
    "mean_fit = mean_spec.fit(train, 'production ~ date')\n",
    "\n",
    "# Predict on test\n",
    "mean_eval = mean_fit.evaluate(test)\n",
    "outputs, coeffs, stats = mean_eval.extract_outputs()\n",
    "\n",
    "print(\"Mean Baseline:\")\n",
    "print(f\"  Training mean: {train['production'].mean():.2f} KBD\")\n",
    "print(f\"  Test RMSE: {stats[stats['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats[stats['split']=='test']['mae'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {stats[stats['split']=='test']['mape'].values[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Median Baseline\n",
    "\n",
    "Forecast = median of all training values (robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median baseline: more robust to outliers\n",
    "median_spec = null_model(strategy='median')\n",
    "median_fit = median_spec.fit(train, 'production ~ date')\n",
    "median_eval = median_fit.evaluate(test)\n",
    "_, _, stats_median = median_eval.extract_outputs()\n",
    "\n",
    "print(\"Median Baseline:\")\n",
    "print(f\"  Training median: {train['production'].median():.2f} KBD\")\n",
    "print(f\"  Test RMSE: {stats_median[stats_median['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_median[stats_median['split']=='test']['mae'].values[0]:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Last Value Baseline (Persistence)\n",
    "\n",
    "Forecast = last observed training value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last value: persistence forecast\n",
    "last_spec = null_model(strategy='last')\n",
    "last_fit = last_spec.fit(train, 'production ~ date')\n",
    "last_eval = last_fit.evaluate(test)\n",
    "_, _, stats_last = last_eval.extract_outputs()\n",
    "\n",
    "print(\"Last Value (Persistence) Baseline:\")\n",
    "print(f\"  Last training value: {train['production'].iloc[-1]:.2f} KBD\")\n",
    "print(f\"  Test RMSE: {stats_last[stats_last['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_last[stats_last['split']=='test']['mae'].values[0]:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. naive_reg() - Time Series Baselines\n",
    "\n",
    "These models use time series specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Forecast\n",
    "\n",
    "Same as persistence (last value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive: last observation carried forward\n",
    "naive_spec = naive_reg(strategy='naive')\n",
    "naive_fit = naive_spec.fit(train, 'production ~ date')\n",
    "naive_eval = naive_fit.evaluate(test)\n",
    "_, _, stats_naive = naive_eval.extract_outputs()\n",
    "\n",
    "print(\"Naive Forecast:\")\n",
    "print(f\"  Test RMSE: {stats_naive[stats_naive['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_naive[stats_naive['split']=='test']['mae'].values[0]:.2f} KBD\")\n",
    "print(f\"\\n  (Should match 'last' from null_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seasonal Naive\n",
    "\n",
    "Forecast = value from same month last year (12-month seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal naive: value from same period last year\n",
    "seasonal_spec = naive_reg(strategy='seasonal_naive', seasonal_period=12)\n",
    "seasonal_fit = seasonal_spec.fit(train, 'production ~ date')\n",
    "seasonal_eval = seasonal_fit.evaluate(test)\n",
    "_, _, stats_seasonal = seasonal_eval.extract_outputs()\n",
    "\n",
    "print(\"Seasonal Naive (12-month):\")\n",
    "print(f\"  Test RMSE: {stats_seasonal[stats_seasonal['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_seasonal[stats_seasonal['split']=='test']['mae'].values[0]:.2f} KBD\")\n",
    "print(f\"\\n  Uses value from same month 12 months ago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Drift Method\n",
    "\n",
    "Extrapolates linear trend from first to last training observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift: linear extrapolation from first to last value\n",
    "drift_spec = naive_reg(strategy='drift')\n",
    "drift_fit = drift_spec.fit(train, 'production ~ date')\n",
    "drift_eval = drift_fit.evaluate(test)\n",
    "_, _, stats_drift = drift_eval.extract_outputs()\n",
    "\n",
    "print(\"Drift Method:\")\n",
    "first_val = train['production'].iloc[0]\n",
    "last_val = train['production'].iloc[-1]\n",
    "n_train = len(train)\n",
    "drift_rate = (last_val - first_val) / (n_train - 1)\n",
    "\n",
    "print(f\"  First value: {first_val:.2f} KBD\")\n",
    "print(f\"  Last value: {last_val:.2f} KBD\")\n",
    "print(f\"  Drift rate: {drift_rate:.2f} KBD/month\")\n",
    "print(f\"  Test RMSE: {stats_drift[stats_drift['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_drift[stats_drift['split']=='test']['mae'].values[0]:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Window Average (Moving Average)\n",
    "\n",
    "Forecast = average of last N observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window average: mean of last 6 months\n",
    "window_spec = naive_reg(strategy='window', window_size=6)\n",
    "window_fit = window_spec.fit(train, 'production ~ date')\n",
    "window_eval = window_fit.evaluate(test)\n",
    "_, _, stats_window = window_eval.extract_outputs()\n",
    "\n",
    "print(\"Window Average (6 months):\")\n",
    "recent_avg = train['production'].iloc[-6:].mean()\n",
    "print(f\"  Last 6 months average: {recent_avg:.2f} KBD\")\n",
    "print(f\"  Test RMSE: {stats_window[stats_window['split']=='test']['rmse'].values[0]:.2f} KBD\")\n",
    "print(f\"  Test MAE: {stats_window[stats_window['split']=='test']['mae'].values[0]:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison Framework\n",
    "\n",
    "Compare all baselines side-by-side using WorkflowSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all baseline workflows\n",
    "baseline_models = [\n",
    "    ('mean', null_model(strategy='mean')),\n",
    "    ('median', null_model(strategy='median')),\n",
    "    ('last', null_model(strategy='last')),\n",
    "    ('naive', naive_reg(strategy='naive')),\n",
    "    ('seasonal_naive_12m', naive_reg(strategy='seasonal_naive', seasonal_period=12)),\n",
    "    ('drift', naive_reg(strategy='drift')),\n",
    "    ('window_6m', naive_reg(strategy='window', window_size=6)),\n",
    "    ('window_3m', naive_reg(strategy='window', window_size=3)),\n",
    "]\n",
    "\n",
    "# Create workflows\n",
    "baseline_workflows = []\n",
    "for name, model in baseline_models:\n",
    "    wf = Workflow().add_formula('production ~ date').add_model(model)\n",
    "    baseline_workflows.append(wf)\n",
    "\n",
    "# Create WorkflowSet\n",
    "wf_set_baselines = WorkflowSet.from_workflows(baseline_workflows)\n",
    "\n",
    "print(f\"Created {len(baseline_workflows)} baseline workflows\")\n",
    "print(f\"Workflow IDs: {list(wf_set_baselines.workflows.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all baseline models on train data\n",
    "baseline_results = []\n",
    "for wf_id, wf in wf_set_baselines.workflows.items():\n",
    "    fit = wf.fit(train)\n",
    "    eval_fit = fit.evaluate(test)\n",
    "    _, _, stats = eval_fit.extract_outputs()\n",
    "    \n",
    "    test_stats = stats[stats['split'] == 'test'].iloc[0]\n",
    "    baseline_results.append({\n",
    "        'model': wf_id,\n",
    "        'rmse': test_stats['rmse'],\n",
    "        'mae': test_stats['mae'],\n",
    "        'mape': test_stats['mape']\n",
    "    })\n",
    "\n",
    "baseline_comparison = pd.DataFrame(baseline_results)\n",
    "baseline_comparison = baseline_comparison.sort_values('rmse')\n",
    "\n",
    "print(\"\\nBaseline Model Comparison (Test Set):\")\n",
    "print(\"=\"*70)\n",
    "print(baseline_comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest baseline: {baseline_comparison.iloc[0]['model']}\")\n",
    "print(f\"  RMSE: {baseline_comparison.iloc[0]['rmse']:.2f} KBD\")\n",
    "print(f\"  MAE: {baseline_comparison.iloc[0]['mae']:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baselines vs Advanced Models\n",
    "\n",
    "Compare baselines against advanced models (Linear Regression and Prophet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add advanced models\n",
    "advanced_models = [\n",
    "    ('linear_reg', linear_reg()),\n",
    "    ('prophet', prophet_reg())\n",
    "]\n",
    "\n",
    "# Create workflows for advanced models\n",
    "all_workflows = baseline_workflows.copy()\n",
    "for name, model in advanced_models:\n",
    "    wf = Workflow().add_formula('production ~ date').add_model(model)\n",
    "    all_workflows.append(wf)\n",
    "\n",
    "# Create WorkflowSet with all models\n",
    "wf_set_all = WorkflowSet.from_workflows(all_workflows)\n",
    "\n",
    "print(f\"Total workflows: {len(all_workflows)} (8 baselines + 2 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all models\n",
    "all_results = []\n",
    "for wf_id, wf in wf_set_all.workflows.items():\n",
    "    try:\n",
    "        fit = wf.fit(train)\n",
    "        eval_fit = fit.evaluate(test)\n",
    "        _, _, stats = eval_fit.extract_outputs()\n",
    "        \n",
    "        test_stats = stats[stats['split'] == 'test'].iloc[0]\n",
    "        model_type = 'baseline' if wf_id in wf_set_baselines.workflows else 'advanced'\n",
    "        \n",
    "        all_results.append({\n",
    "            'model': wf_id,\n",
    "            'type': model_type,\n",
    "            'rmse': test_stats['rmse'],\n",
    "            'mae': test_stats['mae'],\n",
    "            'mape': test_stats['mape']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {wf_id} failed - {str(e)[:50]}\")\n",
    "\n",
    "all_comparison = pd.DataFrame(all_results)\n",
    "all_comparison = all_comparison.sort_values('rmse')\n",
    "\n",
    "print(\"\\nAll Models Comparison (Test Set):\")\n",
    "print(\"=\"*80)\n",
    "print(all_comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement over best baseline\n",
    "best_baseline_rmse = all_comparison[all_comparison['type']=='baseline']['rmse'].min()\n",
    "all_comparison['improvement_vs_baseline'] = (\n",
    "    (best_baseline_rmse - all_comparison['rmse']) / best_baseline_rmse * 100\n",
    ")\n",
    "\n",
    "print(\"\\nImprovement vs Best Baseline:\")\n",
    "print(\"=\"*80)\n",
    "print(all_comparison[['model', 'type', 'rmse', 'improvement_vs_baseline']].to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show advanced model performance\n",
    "advanced_results = all_comparison[all_comparison['type']=='advanced']\n",
    "if len(advanced_results) > 0:\n",
    "    print(f\"\\nAdvanced Models:\")\n",
    "    for _, row in advanced_results.iterrows():\n",
    "        if row['improvement_vs_baseline'] > 0:\n",
    "            print(f\"  ✓ {row['model']}: {row['improvement_vs_baseline']:.1f}% better than best baseline\")\n",
    "        else:\n",
    "            print(f\"  ✗ {row['model']}: {abs(row['improvement_vs_baseline']):.1f}% WORSE than best baseline\")\n",
    "            print(f\"    → Investigation needed! Advanced model should beat baselines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### When Each Baseline Works Best\n",
    "\n",
    "1. **Mean/Median**: \n",
    "   - Stationary series with no trend\n",
    "   - Random walk patterns\n",
    "   - Long-term averages\n",
    "\n",
    "2. **Naive/Last Value**:\n",
    "   - Strong persistence (tomorrow ≈ today)\n",
    "   - Short-term forecasts\n",
    "   - Stable recent patterns\n",
    "\n",
    "3. **Seasonal Naive**:\n",
    "   - Strong seasonal patterns\n",
    "   - Consistent year-over-year behavior\n",
    "   - Retail sales, energy demand\n",
    "\n",
    "4. **Drift**:\n",
    "   - Clear linear trends\n",
    "   - No seasonality\n",
    "   - Steady growth/decline\n",
    "\n",
    "5. **Window Average**:\n",
    "   - Smoothing noisy data\n",
    "   - Recent changes more important\n",
    "   - Balancing stability vs responsiveness\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always establish baseline performance FIRST**\n",
    "   - Run all baseline models before complex ones\n",
    "   - Document baseline RMSE/MAE as reference\n",
    "\n",
    "2. **Advanced models MUST beat baselines**\n",
    "   - If linear_reg loses to mean, check for bugs\n",
    "   - If Prophet loses to seasonal_naive, investigate why\n",
    "   - Report \"% improvement over naive forecast\"\n",
    "\n",
    "3. **Use baselines as production fallbacks**\n",
    "   - If advanced model fails, fall back to seasonal_naive\n",
    "   - Baselines never fail (no hyperparameters, no convergence issues)\n",
    "\n",
    "4. **Choose baseline strategy based on data characteristics**\n",
    "   - Check for trend → use drift\n",
    "   - Check for seasonality → use seasonal_naive\n",
    "   - Neither → use mean or window average\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "```python\n",
    "# Production pattern: baseline fallback\n",
    "try:\n",
    "    predictions = advanced_model.predict(new_data)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Advanced model failed: {e}\")\n",
    "    logger.info(\"Falling back to seasonal_naive baseline\")\n",
    "    predictions = baseline_model.predict(new_data)\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Seasonal period mismatch**\n",
    "   - Weekly data: use seasonal_period=52 (not 12)\n",
    "   - Daily data: use seasonal_period=7 or 365\n",
    "\n",
    "2. **Window size too small/large**\n",
    "   - Too small: noisy predictions\n",
    "   - Too large: slow to react to changes\n",
    "   - Rule of thumb: 3-12 observations\n",
    "\n",
    "3. **Ignoring baseline performance**\n",
    "   - If complex model only 2% better → use baseline (simpler)\n",
    "   - If complex model 50% better → definitely worth complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ All `null_model()` strategies (mean, median, last)  \n",
    "✅ All `naive_reg()` strategies (naive, seasonal_naive, drift, window)  \n",
    "✅ Baseline comparison framework with WorkflowSet  \n",
    "✅ Baselines vs advanced models (linear_reg, prophet_reg)  \n",
    "✅ When to use each baseline strategy  \n",
    "✅ Best practices for production deployment  \n",
    "\n",
    "**Key Insight**: Baselines establish the performance floor. Any advanced model should beat the best baseline by a meaningful margin (10-20%+), otherwise the added complexity isn't justified.\n",
    "\n",
    "**Next Steps**:\n",
    "- Example 33: Recursive multistep forecasting\n",
    "- Example 34: Gradient boosting engines comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
