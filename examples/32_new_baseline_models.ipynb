{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 32: Baseline Forecasting Models\n",
    "\n",
    "**New in v1.0.0**: `null_model()` and `naive_reg()` for baseline forecasting\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the NEW baseline forecasting models added in py-tidymodels v1.0.0:\n",
    "\n",
    "### `null_model()` - Statistical Baselines\n",
    "- **mean**: Average of historical values\n",
    "- **median**: Median of historical values\n",
    "- **last**: Last observed value (persistence)\n",
    "\n",
    "### `naive_reg()` - Time Series Baselines\n",
    "- **naive**: Last observation (same as null_model last)\n",
    "- **seasonal_naive**: Value from same period last season\n",
    "- **drift**: Linear trend from first to last observation\n",
    "- **window**: Moving average over recent window\n",
    "\n",
    "## When to Use Baseline Models\n",
    "\n",
    "**Critical for model evaluation**:\n",
    "1. Establish performance floor - any advanced model should beat baselines\n",
    "2. Quick sanity checks - if complex model loses to mean baseline, investigate!\n",
    "3. Production fallbacks - use baselines when advanced models fail\n",
    "4. Benchmark comparisons - report \"% improvement over naive forecast\"\n",
    "\n",
    "**Sometimes baselines are sufficient**:\n",
    "- Stable time series with no trend/seasonality\n",
    "- Limited historical data (< 50 observations)\n",
    "- High forecast uncertainty makes complex models overkill\n",
    "- Interpretability is paramount\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**JODI Crude Oil Production** (Saudi Arabia):\n",
    "- Monthly production from 2002-2024\n",
    "- Units: Thousands of barrels per day (KBD)\n",
    "- Major oil producer with stable production patterns\n",
    "- Source: Joint Organisations Data Initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_parsnip import null_model, naive_reg, linear_reg, prophet_reg\n",
    "from py_workflows import Workflow\n",
    "from py_rsample import initial_time_split\n",
    "from py_yardstick import rmse, mae, mape\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_yardstick import metric_set\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JODI crude production data\n",
    "df = pd.read_csv('../_md/__data/jodi_crude_production_data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Filter to Saudi Arabia only\n",
    "saudi = df[df['country'] == 'Saudi Arabia'].copy()\n",
    "saudi = saudi[['date', 'value']].rename(columns={'value': 'production'})\n",
    "saudi = saudi.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Remove zeros (missing months)\n",
    "saudi = saudi[saudi['production'] > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saudi Arabia crude oil production:\")\n",
    "print(f\"  Records: {len(saudi):,}\")\n",
    "print(f\"  Date range: {saudi['date'].min()} to {saudi['date'].max()}\")\n",
    "print(f\"  Mean production: {saudi['production'].mean():.0f} KBD\")\n",
    "print(f\"  Std dev: {saudi['production'].std():.0f} KBD\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(saudi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (hold out last 12 months)\n",
    "split = initial_time_split(saudi, date_column='date', prop=0.90)\n",
    "train = split.training()\n",
    "test = split.testing()\n",
    "\n",
    "print(f\"Train: {len(train)} months ({train['date'].min()} to {train['date'].max()})\")\n",
    "print(f\"Test:  {len(test)} months ({test['date'].min()} to {test['date'].max()})\")\n",
    "print(f\"\\nTest period: {len(test)} months for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. null_model() - Statistical Baselines\n",
    "\n",
    "These models use simple statistics from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Baseline\n",
    "\n",
    "Forecast = mean of all training values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline: predicts average of training data\n",
    "mean_spec = null_model(strategy='mean')\n",
    "mean_fit = mean_spec.fit(train, 'production ~ date')\n",
    "\n",
    "# Predict on test\n",
    "mean_eval = mean_fit.evaluate(test)\n",
    "outputs_mean, coeffs_mean, stats_mean = mean_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_mean[stats_mean['split'] == 'test']\n",
    "test_rmse = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "test_mape = test_stats[test_stats['metric'] == 'mape']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸŽ¯ Null Model (Mean Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Median Baseline\n",
    "\n",
    "Forecast = median of all training values (robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median baseline: more robust to outliers\n",
    "median_spec = null_model(strategy='median')\n",
    "median_fit = median_spec.fit(train, 'production ~ date')\n",
    "median_eval = median_fit.evaluate(test)\n",
    "_, _, stats_median = median_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_median[stats_median['split'] == 'test']\n",
    "test_rmse_median = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_median = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "test_mape_median = test_stats[test_stats['metric'] == 'mape']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸŽ¯ Null Model (Median Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_median:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_median:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {test_mape_median:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Last Value Baseline (Persistence)\n",
    "\n",
    "Forecast = last observed training value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last value: persistence forecast\n",
    "last_spec = null_model(strategy='last')\n",
    "last_fit = last_spec.fit(train, 'production ~ date')\n",
    "last_eval = last_fit.evaluate(test)\n",
    "_, _, stats_last = last_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_last[stats_last['split'] == 'test']\n",
    "test_rmse_last = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_last = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "test_mape_last = test_stats[test_stats['metric'] == 'mape']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸŽ¯ Null Model (Last Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_last:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_last:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {test_mape_last:.2f}%\")\n",
    "\n",
    "print(f\"\\n  (Should match 'last' from null_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. naive_reg() - Time Series Baselines\n",
    "\n",
    "These models use time series specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Forecast\n",
    "\n",
    "Same as persistence (last value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive: last observation carried forward\n",
    "naive_spec = naive_reg(strategy='naive')\n",
    "naive_fit = naive_spec.fit(train, 'production ~ date')\n",
    "naive_eval = naive_fit.evaluate(test)\n",
    "_, _, stats_naive = naive_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_naive[stats_naive['split'] == 'test']\n",
    "test_rmse_naive = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_naive = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "test_mape_naive = test_stats[test_stats['metric'] == 'mape']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸ”„ Naive Regression (Naive Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_naive:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_naive:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {test_mape_naive:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seasonal Naive\n",
    "\n",
    "Forecast = value from same month last year (12-month seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal naive: value from same period last year\n",
    "seasonal_spec = naive_reg(strategy='seasonal_naive', seasonal_period=12)\n",
    "seasonal_fit = seasonal_spec.fit(train, 'production ~ date')\n",
    "seasonal_eval = seasonal_fit.evaluate(test)\n",
    "_, _, stats_seasonal = seasonal_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_seasonal[stats_seasonal['split'] == 'test']\n",
    "test_rmse_seasonal = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_seasonal = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "test_mape_seasonal = test_stats[test_stats['metric'] == 'mape']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸ”„ Naive Regression (Seasonal Naive) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_seasonal:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_seasonal:.2f} KBD\")\n",
    "print(f\"  Test MAPE: {test_mape_seasonal:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Drift Method\n",
    "\n",
    "Extrapolates linear trend from first to last training observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift: linear extrapolation from first to last value\n",
    "drift_spec = naive_reg(strategy='drift')\n",
    "drift_fit = drift_spec.fit(train, 'production ~ date')\n",
    "drift_eval = drift_fit.evaluate(test)\n",
    "_, _, stats_drift = drift_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_drift[stats_drift['split'] == 'test']\n",
    "test_rmse_drift = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_drift = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸ”„ Naive Regression (Drift Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_drift:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_drift:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Window Average (Moving Average)\n",
    "\n",
    "Forecast = average of last N observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window average: mean of last 6 months\n",
    "window_spec = naive_reg(strategy='window', window_size=6)\n",
    "window_fit = window_spec.fit(train, 'production ~ date')\n",
    "window_eval = window_fit.evaluate(test)\n",
    "_, _, stats_window = window_eval.extract_outputs()\n",
    "\n",
    "# Extract test metrics (LONG format)\n",
    "test_stats = stats_window[stats_window['split'] == 'test']\n",
    "test_rmse_window = test_stats[test_stats['metric'] == 'rmse']['value'].iloc[0]\n",
    "test_mae_window = test_stats[test_stats['metric'] == 'mae']['value'].iloc[0]\n",
    "\n",
    "print(\"ðŸ”„ Naive Regression (Window Strategy) - Test Performance:\")\n",
    "print(f\"  Test RMSE: {test_rmse_window:.2f} KBD\")\n",
    "print(f\"  Test MAE: {test_mae_window:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison Framework\n",
    "\n",
    "Compare all baselines side-by-side using WorkflowSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all baseline workflows\n",
    "baseline_models = [\n",
    "    ('mean', null_model(strategy='mean')),\n",
    "    ('median', null_model(strategy='median')),\n",
    "    ('last', null_model(strategy='last')),\n",
    "    ('naive', naive_reg(strategy='naive')),\n",
    "    ('seasonal_naive_12m', naive_reg(strategy='seasonal_naive', seasonal_period=12)),\n",
    "    ('drift', naive_reg(strategy='drift')),\n",
    "    ('window_6m', naive_reg(strategy='window', window_size=6)),\n",
    "    ('window_3m', naive_reg(strategy='window', window_size=3)),\n",
    "]\n",
    "\n",
    "# Create workflows\n",
    "baseline_workflows = []\n",
    "for name, model in baseline_models:\n",
    "    wf = Workflow().add_formula('production ~ date').add_model(model)\n",
    "    baseline_workflows.append(wf)\n",
    "\n",
    "# Create WorkflowSet\n",
    "wf_set_baselines = WorkflowSet.from_workflows(baseline_workflows)\n",
    "\n",
    "print(f\"Created {len(baseline_workflows)} baseline workflows\")\n",
    "print(f\"Workflow IDs: {list(wf_set_baselines.workflows.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all null_model strategies\n",
    "baseline_comparison = pd.DataFrame([\n",
    "    {'strategy': 'mean', 'rmse': test_rmse, 'mae': test_mae, 'mape': test_mape},\n",
    "    {'strategy': 'median', 'rmse': test_rmse_median, 'mae': test_mae_median, 'mape': test_mape_median},\n",
    "    {'strategy': 'last', 'rmse': test_rmse_last, 'mae': test_mae_last, 'mape': test_mape_last}\n",
    "]).sort_values('rmse')\n",
    "\n",
    "print(\"\\nðŸ“Š Baseline Comparison (null_model strategies):\")\n",
    "print(baseline_comparison.to_string(index=False))\n",
    "\n",
    "print(f\"\\nðŸ† Best baseline strategy: {baseline_comparison.iloc[0]['strategy']}\")\n",
    "print(f\"  RMSE: {baseline_comparison.iloc[0]['rmse']:.2f} KBD\")\n",
    "print(f\"  MAE: {baseline_comparison.iloc[0]['mae']:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baselines vs Advanced Models\n",
    "\n",
    "Compare baselines against advanced models (Linear Regression and Prophet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add advanced models\n",
    "advanced_models = [\n",
    "    ('linear_reg', linear_reg()),\n",
    "    ('prophet', prophet_reg())\n",
    "]\n",
    "\n",
    "# Create workflows for advanced models\n",
    "all_workflows = baseline_workflows.copy()\n",
    "for name, model in advanced_models:\n",
    "    wf = Workflow().add_formula('production ~ date').add_model(model)\n",
    "    all_workflows.append(wf)\n",
    "\n",
    "# Create WorkflowSet with all models\n",
    "wf_set_all = WorkflowSet.from_workflows(all_workflows)\n",
    "\n",
    "print(f\"Total workflows: {len(all_workflows)} (8 baselines + 2 advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ALL strategies (null_model + naive_reg)\n",
    "all_comparison = pd.DataFrame([\n",
    "    {'type': 'baseline', 'strategy': 'mean', 'rmse': test_rmse, 'mae': test_mae, 'mape': test_mape},\n",
    "    {'type': 'baseline', 'strategy': 'median', 'rmse': test_rmse_median, 'mae': test_mae_median, 'mape': test_mape_median},\n",
    "    {'type': 'baseline', 'strategy': 'last', 'rmse': test_rmse_last, 'mae': test_mae_last, 'mape': test_mape_last},\n",
    "    {'type': 'naive', 'strategy': 'naive', 'rmse': test_rmse_naive, 'mae': test_mae_naive, 'mape': test_mape_naive},\n",
    "    {'type': 'naive', 'strategy': 'seasonal_naive', 'rmse': test_rmse_seasonal, 'mae': test_mae_seasonal, 'mape': test_mape_seasonal},\n",
    "    {'type': 'naive', 'strategy': 'drift', 'rmse': test_rmse_drift, 'mae': test_mae_drift, 'mape': np.nan},\n",
    "    {'type': 'naive', 'strategy': 'window', 'rmse': test_rmse_window, 'mae': test_mae_window, 'mape': np.nan}\n",
    "]).sort_values('rmse')\n",
    "\n",
    "print(\"\\nðŸ“Š ALL Strategies Comparison:\")\n",
    "print(all_comparison.to_string(index=False))\n",
    "\n",
    "# Calculate improvement\n",
    "best_baseline_rmse = all_comparison[all_comparison['type']=='baseline']['rmse'].min()\n",
    "all_comparison['improvement_vs_best_baseline'] = \\\n",
    "    (best_baseline_rmse - all_comparison['rmse']) / best_baseline_rmse * 100\n",
    "\n",
    "print(f\"\\nðŸ† Best overall strategy: {all_comparison.iloc[0]['strategy']}\")\n",
    "print(f\"  Type: {all_comparison.iloc[0]['type']}\")\n",
    "print(f\"  RMSE: {all_comparison.iloc[0]['rmse']:.2f} KBD\")\n",
    "print(f\"  MAE: {all_comparison.iloc[0]['mae']:.2f} KBD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show improvement over best baseline\n",
    "print(\"\\nðŸ“ˆ Improvement vs Best Baseline:\")\n",
    "print(all_comparison[['strategy', 'type', 'rmse', 'improvement_vs_best_baseline']].to_string(index=False))\n",
    "\n",
    "# Highlight improvements\n",
    "print(\"\\nðŸŽ¯ Analysis:\")\n",
    "for _, row in all_comparison.iterrows():\n",
    "    if row['improvement_vs_best_baseline'] > 0:\n",
    "        print(f\"  âœ“ {row['strategy']}: {row['improvement_vs_best_baseline']:.1f}% better than best baseline\")\n",
    "    else:\n",
    "        print(f\"  âœ— {row['strategy']}: {abs(row['improvement_vs_best_baseline']):.1f}% WORSE than best baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### When Each Baseline Works Best\n",
    "\n",
    "1. **Mean/Median**: \n",
    "   - Stationary series with no trend\n",
    "   - Random walk patterns\n",
    "   - Long-term averages\n",
    "\n",
    "2. **Naive/Last Value**:\n",
    "   - Strong persistence (tomorrow â‰ˆ today)\n",
    "   - Short-term forecasts\n",
    "   - Stable recent patterns\n",
    "\n",
    "3. **Seasonal Naive**:\n",
    "   - Strong seasonal patterns\n",
    "   - Consistent year-over-year behavior\n",
    "   - Retail sales, energy demand\n",
    "\n",
    "4. **Drift**:\n",
    "   - Clear linear trends\n",
    "   - No seasonality\n",
    "   - Steady growth/decline\n",
    "\n",
    "5. **Window Average**:\n",
    "   - Smoothing noisy data\n",
    "   - Recent changes more important\n",
    "   - Balancing stability vs responsiveness\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always establish baseline performance FIRST**\n",
    "   - Run all baseline models before complex ones\n",
    "   - Document baseline RMSE/MAE as reference\n",
    "\n",
    "2. **Advanced models MUST beat baselines**\n",
    "   - If linear_reg loses to mean, check for bugs\n",
    "   - If Prophet loses to seasonal_naive, investigate why\n",
    "   - Report \"% improvement over naive forecast\"\n",
    "\n",
    "3. **Use baselines as production fallbacks**\n",
    "   - If advanced model fails, fall back to seasonal_naive\n",
    "   - Baselines never fail (no hyperparameters, no convergence issues)\n",
    "\n",
    "4. **Choose baseline strategy based on data characteristics**\n",
    "   - Check for trend â†’ use drift\n",
    "   - Check for seasonality â†’ use seasonal_naive\n",
    "   - Neither â†’ use mean or window average\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "```python\n",
    "# Production pattern: baseline fallback\n",
    "try:\n",
    "    predictions = advanced_model.predict(new_data)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Advanced model failed: {e}\")\n",
    "    logger.info(\"Falling back to seasonal_naive baseline\")\n",
    "    predictions = baseline_model.predict(new_data)\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Seasonal period mismatch**\n",
    "   - Weekly data: use seasonal_period=52 (not 12)\n",
    "   - Daily data: use seasonal_period=7 or 365\n",
    "\n",
    "2. **Window size too small/large**\n",
    "   - Too small: noisy predictions\n",
    "   - Too large: slow to react to changes\n",
    "   - Rule of thumb: 3-12 observations\n",
    "\n",
    "3. **Ignoring baseline performance**\n",
    "   - If complex model only 2% better â†’ use baseline (simpler)\n",
    "   - If complex model 50% better â†’ definitely worth complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "âœ… All `null_model()` strategies (mean, median, last)  \n",
    "âœ… All `naive_reg()` strategies (naive, seasonal_naive, drift, window)  \n",
    "âœ… Baseline comparison framework with WorkflowSet  \n",
    "âœ… Baselines vs advanced models (linear_reg, prophet_reg)  \n",
    "âœ… When to use each baseline strategy  \n",
    "âœ… Best practices for production deployment  \n",
    "\n",
    "**Key Insight**: Baselines establish the performance floor. Any advanced model should beat the best baseline by a meaningful margin (10-20%+), otherwise the added complexity isn't justified.\n",
    "\n",
    "**Next Steps**:\n",
    "- Example 33: Recursive multistep forecasting\n",
    "- Example 34: Gradient boosting engines comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
