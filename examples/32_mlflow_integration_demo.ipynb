{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473e2f82",
   "metadata": {},
   "source": [
    "# MLflow Integration for Model Lifecycle Management\n",
    "\n",
    "This notebook demonstrates MLflow integration for py-tidymodels model persistence and deployment.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. Basic save/load workflow\n",
    "2. Saving workflows with recipes\n",
    "3. Saving grouped/nested models\n",
    "4. Version compatibility checking\n",
    "5. Model signatures and metadata\n",
    "6. Loading and deploying models\n",
    "7. Integration with experiment tracking\n",
    "\n",
    "**Use Case:** Model lifecycle management for production deployment\n",
    "\n",
    "**Why MLflow:**\n",
    "- Standardized model serialization\n",
    "- Version tracking and metadata\n",
    "- Model registry for production deployment\n",
    "- Integration with experiment tracking\n",
    "- Cross-team collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_workflows import workflow\n",
    "from py_recipes import recipe\n",
    "from py_mlflow import save_model, load_model, get_model_info, validate_model_exists\n",
    "from py_yardstick import rmse, mae, r_squared\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create directory for saved models\n",
    "models_dir = Path('mlflow_demo_models')\n",
    "if models_dir.exists():\n",
    "    shutil.rmtree(models_dir)\n",
    "models_dir.mkdir()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Models will be saved to: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58ef2f",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Sales Data\n",
    "\n",
    "Create realistic sales dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data\n",
    "n = 200\n",
    "\n",
    "# Predictors\n",
    "advertising = np.random.uniform(0, 100, n)\n",
    "price = np.random.uniform(10, 50, n)\n",
    "competitor_price = np.random.uniform(10, 50, n)\n",
    "seasonality = np.sin(np.linspace(0, 4*np.pi, n))\n",
    "\n",
    "# True sales relationship\n",
    "sales = (\n",
    "    100 +\n",
    "    1.5 * advertising +\n",
    "    -2.0 * price +\n",
    "    1.0 * competitor_price +\n",
    "    30 * seasonality +\n",
    "    np.random.randn(n) * 5\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'sales': sales,\n",
    "    'advertising': advertising,\n",
    "    'price': price,\n",
    "    'competitor_price': competitor_price,\n",
    "    'seasonality': seasonality\n",
    "})\n",
    "\n",
    "# Split train/test\n",
    "train_data = data.iloc[:160]\n",
    "test_data = data.iloc[160:]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} observations\")\n",
    "print(f\"Test data: {len(test_data)} observations\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e1835",
   "metadata": {},
   "source": [
    "## 2. Basic ModelFit Save/Load\n",
    "\n",
    "Simplest use case: Save and load a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression model\n",
    "spec = linear_reg(penalty=0.1, mixture=0.5)\n",
    "fit = spec.fit(train_data, \"sales ~ advertising + price + competitor_price\")\n",
    "\n",
    "# Evaluate on test data\n",
    "fit_eval = fit.evaluate(test_data)\n",
    "\n",
    "print(\"Model fitted and evaluated!\")\n",
    "print(f\"\\nModel type: {fit.spec.model_type}\")\n",
    "print(f\"Engine: {fit.spec.engine}\")\n",
    "\n",
    "# Get performance metrics\n",
    "outputs, coeffs, stats = fit_eval.extract_outputs()\n",
    "\n",
    "print(f\"\\nTest RMSE: {stats[stats['split'] == 'test']['rmse'].values[0]:.4f}\")\n",
    "print(f\"Test R²: {stats[stats['split'] == 'test']['r_squared'].values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79feb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with MLflow\n",
    "model_path = models_dir / 'basic_linear_model'\n",
    "\n",
    "fit_eval.save_mlflow(\n",
    "    path=str(model_path),\n",
    "    signature='auto',  # Auto-infer input/output schema\n",
    "    input_example=train_data.head(5),  # Save example input\n",
    "    metadata={\n",
    "        'dataset': 'synthetic_sales',\n",
    "        'version': '1.0',\n",
    "        'description': 'Linear regression with elastic net regularization'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(f\"✓ Model successfully serialized with MLflow format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f892e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model exists\n",
    "exists = validate_model_exists(str(model_path))\n",
    "print(f\"Model exists: {exists}\")\n",
    "\n",
    "# Get model info without loading\n",
    "info = get_model_info(str(model_path))\n",
    "\n",
    "print(f\"\\nModel Metadata:\")\n",
    "print(f\"  Model Type: {info['model_type']}\")\n",
    "print(f\"  Engine: {info['engine']}\")\n",
    "print(f\"  Mode: {info['mode']}\")\n",
    "print(f\"  py-tidymodels Version: {info['py_tidymodels_version']}\")\n",
    "print(f\"  Fit Timestamp: {info['fit_timestamp']}\")\n",
    "print(f\"  Custom Metadata: {info['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e03179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "loaded_model = load_model(str(model_path))\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Model type: {loaded_model.spec.model_type}\")\n",
    "print(f\"Engine: {loaded_model.spec.engine}\")\n",
    "\n",
    "# Compare predictions\n",
    "preds_original = fit_eval.predict(test_data)\n",
    "preds_loaded = loaded_model.predict(test_data)\n",
    "\n",
    "predictions_match = np.allclose(\n",
    "    preds_original['.pred'].values,\n",
    "    preds_loaded['.pred'].values\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Predictions match: {predictions_match}\")\n",
    "print(f\"Original prediction (first 5): {preds_original['.pred'].head().values}\")\n",
    "print(f\"Loaded prediction (first 5):   {preds_loaded['.pred'].head().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a67561",
   "metadata": {},
   "source": [
    "## 3. Save WorkflowFit with Recipe\n",
    "\n",
    "Demonstrate saving workflows with preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow with recipe\n",
    "rec = (\n",
    "    recipe()\n",
    "    .step_normalize()  # Normalize all numeric predictors\n",
    "    .step_pca(num_comp=3)  # Reduce to 3 PCA components\n",
    ")\n",
    "\n",
    "wf = workflow().add_recipe(rec).add_model(\n",
    "    rand_forest(trees=50, min_n=5).set_mode('regression')\n",
    ")\n",
    "\n",
    "# Fit workflow\n",
    "wf_fit = wf.fit(train_data)\n",
    "\n",
    "print(\"Workflow fitted!\")\n",
    "print(f\"\\nModel: {wf_fit.spec.model_type}\")\n",
    "print(f\"Recipe steps: {len(rec.steps)}\")\n",
    "print(f\"  - Normalization\")\n",
    "print(f\"  - PCA (3 components)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate workflow\n",
    "wf_eval = wf_fit.evaluate(test_data)\n",
    "outputs_wf, coeffs_wf, stats_wf = wf_eval.extract_outputs()\n",
    "\n",
    "print(f\"\\nWorkflow performance:\")\n",
    "print(f\"Test RMSE: {stats_wf[stats_wf['split'] == 'test']['rmse'].values[0]:.4f}\")\n",
    "print(f\"Test R²: {stats_wf[stats_wf['split'] == 'test']['r_squared'].values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow (saves both recipe and model)\n",
    "workflow_path = models_dir / 'workflow_with_recipe'\n",
    "\n",
    "wf_eval.save_mlflow(\n",
    "    path=str(workflow_path),\n",
    "    signature='auto',\n",
    "    input_example=train_data.head(5),\n",
    "    metadata={\n",
    "        'recipe_steps': len(rec.steps),\n",
    "        'pca_components': 3,\n",
    "        'model_type': 'random_forest'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Workflow saved to: {workflow_path}\")\n",
    "print(\"✓ Both recipe and model serialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workflow\n",
    "loaded_workflow = load_model(str(workflow_path))\n",
    "\n",
    "print(\"\\nWorkflow loaded successfully!\")\n",
    "\n",
    "# Recipe preprocessing is applied automatically during prediction\n",
    "test_preds = loaded_workflow.predict(test_data)\n",
    "\n",
    "print(f\"\\nPredictions generated: {len(test_preds)} rows\")\n",
    "print(f\"✓ Recipe preprocessing applied automatically\")\n",
    "print(f\"\\nFirst 5 predictions: {test_preds['.pred'].head().values}\")\n",
    "\n",
    "# Verify match\n",
    "wf_preds_original = wf_eval.predict(test_data)\n",
    "workflow_match = np.allclose(\n",
    "    wf_preds_original['.pred'].values,\n",
    "    test_preds['.pred'].values\n",
    ")\n",
    "print(f\"\\n✓ Workflow predictions match: {workflow_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb08377",
   "metadata": {},
   "source": [
    "## 4. Save Grouped/Nested Models\n",
    "\n",
    "Demonstrate saving per-group models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped sales data\n",
    "groups = []\n",
    "for store in ['Store_A', 'Store_B', 'Store_C']:\n",
    "    n_store = 60\n",
    "\n",
    "    # Different stores have different dynamics\n",
    "    if store == 'Store_A':\n",
    "        coef_adv = 2.0\n",
    "        coef_price = -3.0\n",
    "    elif store == 'Store_B':\n",
    "        coef_adv = 1.0\n",
    "        coef_price = -1.5\n",
    "    else:  # Store_C\n",
    "        coef_adv = 1.5\n",
    "        coef_price = -2.0\n",
    "\n",
    "    adv = np.random.uniform(0, 100, n_store)\n",
    "    pr = np.random.uniform(10, 50, n_store)\n",
    "    comp_pr = np.random.uniform(10, 50, n_store)\n",
    "\n",
    "    sales_store = (\n",
    "        100 + coef_adv * adv + coef_price * pr + comp_pr +\n",
    "        np.random.randn(n_store) * 5\n",
    "    )\n",
    "\n",
    "    store_df = pd.DataFrame({\n",
    "        'store': store,\n",
    "        'sales': sales_store,\n",
    "        'advertising': adv,\n",
    "        'price': pr,\n",
    "        'competitor_price': comp_pr\n",
    "    })\n",
    "    groups.append(store_df)\n",
    "\n",
    "grouped_data = pd.concat(groups, ignore_index=True)\n",
    "grouped_train = grouped_data.iloc[:150]\n",
    "grouped_test = grouped_data.iloc[150:]\n",
    "\n",
    "print(f\"Grouped data created: {grouped_data['store'].nunique()} stores\")\n",
    "print(f\"\\nStore counts:\")\n",
    "print(grouped_data['store'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit separate model for each store\n",
    "spec_nested = linear_reg()\n",
    "nested_fit = spec_nested.fit_nested(\n",
    "    grouped_train,\n",
    "    formula=\"sales ~ advertising + price + competitor_price\",\n",
    "    group_col=\"store\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFitted {len(nested_fit.group_fits)} models (one per store)\")\n",
    "\n",
    "# Evaluate\n",
    "nested_eval = nested_fit.evaluate(grouped_test)\n",
    "outputs_nested, coeffs_nested, stats_nested = nested_eval.extract_outputs()\n",
    "\n",
    "print(f\"\\nPer-store performance:\")\n",
    "print(stats_nested[stats_nested['split'] == 'test'][['group', 'rmse', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f466b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all group models\n",
    "grouped_path = models_dir / 'store_models'\n",
    "\n",
    "nested_eval.save_mlflow(\n",
    "    path=str(grouped_path),\n",
    "    metadata={\n",
    "        'stores': list(nested_fit.group_fits.keys()),\n",
    "        'group_col': 'store'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nGrouped models saved to: {grouped_path}\")\n",
    "print(\"✓ All 3 store models serialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b897e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model info\n",
    "grouped_info = get_model_info(str(grouped_path))\n",
    "\n",
    "print(\"\\nGrouped Model Metadata:\")\n",
    "print(f\"  Is Grouped: {grouped_info['is_grouped']}\")\n",
    "print(f\"  Group Column: {grouped_info['group_col']}\")\n",
    "print(f\"  Groups: {grouped_info['groups']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grouped models\n",
    "loaded_nested = load_model(str(grouped_path))\n",
    "\n",
    "print(\"\\nGrouped models loaded successfully!\")\n",
    "print(f\"Number of group models: {len(loaded_nested.group_fits)}\")\n",
    "print(f\"Groups: {list(loaded_nested.group_fits.keys())}\")\n",
    "\n",
    "# Predict (automatically routes to correct store model)\n",
    "nested_preds = loaded_nested.predict(grouped_test)\n",
    "\n",
    "print(f\"\\nPredictions generated: {len(nested_preds)} rows\")\n",
    "print(f\"✓ Automatic routing to correct store model\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "print(nested_preds[['.pred', 'store']].head())\n",
    "\n",
    "# Verify predictions match\n",
    "nested_preds_original = nested_eval.predict(grouped_test)\n",
    "nested_match = np.allclose(\n",
    "    nested_preds_original['.pred'].values,\n",
    "    nested_preds['.pred'].values\n",
    ")\n",
    "print(f\"\\n✓ Nested predictions match: {nested_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad117756",
   "metadata": {},
   "source": [
    "## 5. Version Compatibility\n",
    "\n",
    "MLflow tracks py-tidymodels version and ensures compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a683f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with version info\n",
    "version_path = models_dir / 'version_test'\n",
    "\n",
    "fit_eval.save_mlflow(\n",
    "    path=str(version_path),\n",
    "    metadata={'purpose': 'version_compatibility_test'}\n",
    ")\n",
    "\n",
    "# Get version info\n",
    "version_info = get_model_info(str(version_path))\n",
    "\n",
    "print(\"Model Version Information:\")\n",
    "print(f\"  Trained with py-tidymodels: {version_info['py_tidymodels_version']}\")\n",
    "print(f\"  Fit timestamp: {version_info['fit_timestamp']}\")\n",
    "print(f\"  Model type: {version_info['model_type']}\")\n",
    "\n",
    "# Load model (version checked automatically)\n",
    "loaded_version = load_model(str(version_path))\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully (version compatible)\")\n",
    "print(f\"\\nNote: If versions are incompatible, load_model() would raise a warning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a11c0",
   "metadata": {},
   "source": [
    "## 6. Model Signatures\n",
    "\n",
    "MLflow signatures define input/output schemas for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ff17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with auto-inferred signature\n",
    "signature_path = models_dir / 'model_with_signature'\n",
    "\n",
    "fit_eval.save_mlflow(\n",
    "    path=str(signature_path),\n",
    "    signature='auto',  # Infer from input_example\n",
    "    input_example=train_data.head(3)\n",
    ")\n",
    "\n",
    "print(\"Model saved with signature\")\n",
    "\n",
    "# Get model info\n",
    "sig_info = get_model_info(str(signature_path))\n",
    "\n",
    "if 'signature' in sig_info:\n",
    "    print(f\"\\nModel Signature:\")\n",
    "    print(f\"  Inputs: {sig_info['signature']['inputs']}\")\n",
    "    print(f\"  Outputs: {sig_info['signature']['outputs']}\")\n",
    "else:\n",
    "    print(\"\\nSignature not available in model info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e042a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate signature\n",
    "loaded_sig = load_model(str(signature_path))\n",
    "\n",
    "# Valid prediction (matches signature)\n",
    "try:\n",
    "    valid_preds = loaded_sig.predict(test_data)\n",
    "    print(\"✓ Valid prediction succeeded\")\n",
    "    print(f\"  Shape: {valid_preds.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Valid prediction failed: {e}\")\n",
    "\n",
    "# Invalid prediction (missing columns) - would fail\n",
    "try:\n",
    "    invalid_data = test_data.drop(columns=['advertising'])\n",
    "    invalid_preds = loaded_sig.predict(invalid_data)\n",
    "    print(\"Invalid prediction succeeded (unexpected!)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✓ Invalid prediction failed as expected:\")\n",
    "    print(f\"  Error: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9b27f",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Load multiple models and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved models\n",
    "models_to_compare = {\n",
    "    'Linear (Basic)': load_model(str(models_dir / 'basic_linear_model')),\n",
    "    'Random Forest (Recipe)': load_model(str(models_dir / 'workflow_with_recipe'))\n",
    "}\n",
    "\n",
    "# Compare on test data\n",
    "comparison_results = []\n",
    "\n",
    "for model_name, model in models_to_compare.items():\n",
    "    preds = model.predict(test_data)\n",
    "\n",
    "    rmse_val = rmse(test_data['sales'], preds['.pred']).iloc[0]['value']\n",
    "    mae_val = mae(test_data['sales'], preds['.pred']).iloc[0]['value']\n",
    "    r2_val = r_squared(test_data['sales'], preds['.pred']).iloc[0]['value']\n",
    "\n",
    "    comparison_results.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse_val,\n",
    "        'MAE': mae_val,\n",
    "        'R²': r2_val\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"Model Comparison on Test Data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R²']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    ax.bar(comparison_df['Model'], comparison_df[metric], alpha=0.7)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model = comparison_df.loc[comparison_df['RMSE'].idxmin(), 'Model']\n",
    "print(f\"\\n✓ Best model (lowest RMSE): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef59d9",
   "metadata": {},
   "source": [
    "## 8. Production Deployment Workflow\n",
    "\n",
    "Demonstrate full production workflow with model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train multiple candidate models\n",
    "print(\"Training candidate models...\")\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# Model 1: Linear regression\n",
    "spec1 = linear_reg()\n",
    "fit1 = spec1.fit(train_data, \"sales ~ advertising + price + competitor_price\")\n",
    "fit1_eval = fit1.evaluate(test_data)\n",
    "_, _, stats1 = fit1_eval.extract_outputs()\n",
    "rmse1 = stats1[stats1['split'] == 'test']['rmse'].values[0]\n",
    "\n",
    "candidates.append({\n",
    "    'name': 'linear_regression',\n",
    "    'fit': fit1_eval,\n",
    "    'rmse': rmse1\n",
    "})\n",
    "\n",
    "# Model 2: Linear with interactions\n",
    "spec2 = linear_reg()\n",
    "fit2 = spec2.fit(train_data, \"sales ~ advertising + price + competitor_price + I(advertising*price)\")\n",
    "fit2_eval = fit2.evaluate(test_data)\n",
    "_, _, stats2 = fit2_eval.extract_outputs()\n",
    "rmse2 = stats2[stats2['split'] == 'test']['rmse'].values[0]\n",
    "\n",
    "candidates.append({\n",
    "    'name': 'linear_with_interactions',\n",
    "    'fit': fit2_eval,\n",
    "    'rmse': rmse2\n",
    "})\n",
    "\n",
    "# Model 3: Random forest\n",
    "spec3 = rand_forest(trees=50).set_mode('regression')\n",
    "fit3 = spec3.fit(train_data, \"sales ~ advertising + price + competitor_price\")\n",
    "fit3_eval = fit3.evaluate(test_data)\n",
    "_, _, stats3 = fit3_eval.extract_outputs()\n",
    "rmse3 = stats3[stats3['split'] == 'test']['rmse'].values[0]\n",
    "\n",
    "candidates.append({\n",
    "    'name': 'random_forest',\n",
    "    'fit': fit3_eval,\n",
    "    'rmse': rmse3\n",
    "})\n",
    "\n",
    "print(\"\\nCandidate Models:\")\n",
    "for c in candidates:\n",
    "    print(f\"  {c['name']:<30} RMSE: {c['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select best model\n",
    "best_candidate = min(candidates, key=lambda x: x['rmse'])\n",
    "\n",
    "print(f\"\\n✓ Best model: {best_candidate['name']}\")\n",
    "print(f\"  RMSE: {best_candidate['rmse']:.4f}\")\n",
    "\n",
    "# 3. Save production model\n",
    "production_path = models_dir / 'production_model'\n",
    "\n",
    "best_candidate['fit'].save_mlflow(\n",
    "    path=str(production_path),\n",
    "    signature='auto',\n",
    "    input_example=train_data.head(5),\n",
    "    metadata={\n",
    "        'model_name': best_candidate['name'],\n",
    "        'test_rmse': float(best_candidate['rmse']),\n",
    "        'training_date': pd.Timestamp.now().isoformat(),\n",
    "        'status': 'production',\n",
    "        'champion': True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Production model saved to: {production_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ae919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load production model for deployment\n",
    "production_model = load_model(str(production_path))\n",
    "\n",
    "print(\"✓ Production model loaded\")\n",
    "\n",
    "# Get metadata\n",
    "prod_info = get_model_info(str(production_path))\n",
    "print(f\"\\nProduction Model Metadata:\")\n",
    "print(f\"  Model: {prod_info['metadata']['model_name']}\")\n",
    "print(f\"  Test RMSE: {prod_info['metadata']['test_rmse']:.4f}\")\n",
    "print(f\"  Training Date: {prod_info['metadata']['training_date']}\")\n",
    "print(f\"  Status: {prod_info['metadata']['status']}\")\n",
    "\n",
    "# 5. Make production predictions\n",
    "new_data = pd.DataFrame({\n",
    "    'advertising': [50, 75, 30],\n",
    "    'price': [25, 30, 20],\n",
    "    'competitor_price': [28, 32, 22],\n",
    "    'seasonality': [0.5, 0.7, -0.3]\n",
    "})\n",
    "\n",
    "production_preds = production_model.predict(new_data)\n",
    "\n",
    "print(f\"\\nProduction Predictions:\")\n",
    "print(production_preds[['.pred']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06901704",
   "metadata": {},
   "source": [
    "## 9. Model Registry Workflow (Conceptual)\n",
    "\n",
    "While this demo saves models locally, in production you'd integrate with MLflow tracking server and model registry.\n",
    "\n",
    "**Production Setup:**\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "\n",
    "# Start experiment\n",
    "mlflow.set_experiment(\"sales_forecasting\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"linear_reg\")\n",
    "    mlflow.log_param(\"penalty\", 0.1)\n",
    "\n",
    "    # Train model\n",
    "    fit = spec.fit(train_data, formula)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"test_rmse\", rmse_val)\n",
    "\n",
    "    # Save model to registry\n",
    "    fit.save_mlflow(\n",
    "        \"models/sales_model\",\n",
    "        registered_model_name=\"SalesForecaster\"\n",
    "    )\n",
    "\n",
    "# Transition to production\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"SalesForecaster\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "\n",
    "# Load from registry\n",
    "prod_model = mlflow.pyfunc.load_model(\"models:/SalesForecaster/Production\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c889a",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **MLflow Integration Benefits:**\n",
    "   - Standardized model serialization\n",
    "   - Version tracking and metadata\n",
    "   - Model signatures for validation\n",
    "   - Production deployment support\n",
    "   - Cross-team collaboration\n",
    "\n",
    "2. **Save/Load Capabilities:**\n",
    "   - Basic ModelFit objects\n",
    "   - WorkflowFit with recipes\n",
    "   - Grouped/nested models\n",
    "   - Automatic version tracking\n",
    "\n",
    "3. **Model Signatures:**\n",
    "   - Auto-inference from input examples\n",
    "   - Input/output schema validation\n",
    "   - Prevents prediction errors\n",
    "\n",
    "4. **Version Compatibility:**\n",
    "   - Tracks py-tidymodels version\n",
    "   - Warns on incompatible loads\n",
    "   - Reproducibility guaranteed\n",
    "\n",
    "5. **Production Workflow:**\n",
    "   - Train multiple candidates\n",
    "   - Compare on validation set\n",
    "   - Save best model with metadata\n",
    "   - Load for deployment\n",
    "   - Track performance over time\n",
    "\n",
    "6. **Best Practices:**\n",
    "   - Always save metadata (training date, metrics, etc.)\n",
    "   - Use model signatures for validation\n",
    "   - Version models explicitly\n",
    "   - Document model lineage\n",
    "   - Monitor production performance\n",
    "   - Retrain when drift detected\n",
    "\n",
    "7. **Integration Points:**\n",
    "   - Local file storage (demo)\n",
    "   - MLflow tracking server (production)\n",
    "   - Model registry (deployment)\n",
    "   - CI/CD pipelines (automation)\n",
    "   - Monitoring systems (observability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97282f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: List all saved models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVED MODELS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_dir in models_dir.iterdir():\n",
    "    if model_dir.is_dir():\n",
    "        info = get_model_info(str(model_dir))\n",
    "\n",
    "        print(f\"\\nModel: {model_dir.name}\")\n",
    "        print(f\"  Type: {info['model_type']}\")\n",
    "        print(f\"  Engine: {info['engine']}\")\n",
    "        print(f\"  Mode: {info['mode']}\")\n",
    "        print(f\"  Grouped: {info.get('is_grouped', False)}\")\n",
    "        print(f\"  Timestamp: {info['fit_timestamp']}\")\n",
    "        if 'metadata' in info and info['metadata']:\n",
    "            print(f\"  Metadata: {info['metadata']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"MLflow provides robust model lifecycle management for py-tidymodels!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
