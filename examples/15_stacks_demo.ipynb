{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py_stacks: Model Ensembling via Stacking\n",
    "\n",
    "This notebook demonstrates **py_stacks**, which implements model stacking (ensembling) using meta-learning with elastic net regularization.\n",
    "\n",
    "## What is Model Stacking?\n",
    "\n",
    "Model stacking (or stacked generalization) combines predictions from multiple base models using a meta-learner:\n",
    "\n",
    "1. Train multiple diverse base models\n",
    "2. Collect their predictions (meta-features)\n",
    "3. Train a meta-learner to optimally combine predictions\n",
    "4. The ensemble often outperforms individual models\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "- **stacks()** - Create empty ensemble\n",
    "- **add_candidates()** - Add base model predictions\n",
    "- **blend_predictions()** - Fit meta-learner with elastic net\n",
    "- **get_model_weights()** - Extract model contributions\n",
    "- **compare_to_candidates()** - Compare ensemble vs base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import py-tidymodels packages\n",
    "from py_parsnip import linear_reg, rand_forest, decision_tree\n",
    "from py_recipes import recipe, step_date, step_lag, step_normalize\n",
    "from py_workflows import workflow\n",
    "from py_rsample import initial_time_split\n",
    "from py_stacks import stacks\n",
    "from py_visualize import plot_model_comparison\n",
    "\n",
    "print(\"âœ“ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create Sample Data\n",
    "\n",
    "We'll create a time series dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2020-01-01', periods=400, freq='D')\n",
    "time_index = np.arange(len(dates))\n",
    "\n",
    "# Complex pattern: trend + multiple seasonalities + noise\n",
    "trend = time_index * 0.3\n",
    "weekly_season = 8 * np.sin(2 * np.pi * time_index / 7)\n",
    "monthly_season = 15 * np.sin(2 * np.pi * time_index / 30)\n",
    "noise = np.random.randn(len(dates)) * 5\n",
    "\n",
    "y = trend + weekly_season + monthly_season + noise + 100\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': y\n",
    "})\n",
    "\n",
    "# Split data\n",
    "split = initial_time_split(data, prop=0.75)\n",
    "train_data = split.training()\n",
    "test_data = split.testing()\n",
    "\n",
    "print(f\"Total observations: {len(data)}\")\n",
    "print(f\"Training: {len(train_data)} observations\")\n",
    "print(f\"Testing: {len(test_data)} observations\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {data['value'].mean():.2f}\")\n",
    "print(f\"  Std: {data['value'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train Multiple Base Models\n",
    "\n",
    "We'll create diverse base models with different strengths:\n",
    "- **Linear Regression**: Good for linear trends\n",
    "- **Ridge Regression**: Regularized linear model\n",
    "- **Lasso Regression**: Sparse linear model\n",
    "- **Random Forest**: Captures non-linear patterns\n",
    "- **Decision Tree**: Simple non-linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature engineering recipe\n",
    "rec = (\n",
    "    recipe(value ~ date, data=train_data)\n",
    "    .step_date('date', features=['month', 'week', 'doy', 'dow'])\n",
    "    .step_lag('value', lags=[1, 7, 14, 30])\n",
    "    .step_normalize(['value_lag_1', 'value_lag_7', 'value_lag_14', 'value_lag_30'])\n",
    ")\n",
    "\n",
    "print(\"âœ“ Recipe created with date features and lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Linear Regression\n",
    "wf_linear = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(linear_reg())\n",
    ")\n",
    "\n",
    "fit_linear = wf_linear.fit(train_data)\n",
    "pred_linear = fit_linear.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Model 1: Linear Regression fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Ridge Regression (L2 regularization)\n",
    "wf_ridge = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(linear_reg(penalty=0.1, mixture=0.0))  # mixture=0 â†’ Ridge\n",
    ")\n",
    "\n",
    "fit_ridge = wf_ridge.fit(train_data)\n",
    "pred_ridge = fit_ridge.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Model 2: Ridge Regression fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Lasso Regression (L1 regularization)\n",
    "wf_lasso = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(linear_reg(penalty=0.1, mixture=1.0))  # mixture=1 â†’ Lasso\n",
    ")\n",
    "\n",
    "fit_lasso = wf_lasso.fit(train_data)\n",
    "pred_lasso = fit_lasso.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Model 3: Lasso Regression fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Random Forest\n",
    "wf_rf = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(rand_forest(trees=100, min_n=5, mode='regression'))\n",
    ")\n",
    "\n",
    "fit_rf = wf_rf.fit(train_data)\n",
    "pred_rf = fit_rf.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Model 4: Random Forest fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Decision Tree\n",
    "wf_tree = (\n",
    "    workflow()\n",
    "    .add_recipe(rec)\n",
    "    .add_model(decision_tree(min_n=10, mode='regression'))\n",
    ")\n",
    "\n",
    "fit_tree = wf_tree.fit(train_data)\n",
    "pred_tree = fit_tree.predict(test_data)\n",
    "\n",
    "print(\"âœ“ Model 5: Decision Tree fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Individual Model Performance\n",
    "\n",
    "Before ensembling, let's see how each model performs individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stats from each model\n",
    "_, _, stats_linear = fit_linear.extract_outputs()\n",
    "_, _, stats_ridge = fit_ridge.extract_outputs()\n",
    "_, _, stats_lasso = fit_lasso.extract_outputs()\n",
    "_, _, stats_rf = fit_rf.extract_outputs()\n",
    "_, _, stats_tree = fit_tree.extract_outputs()\n",
    "\n",
    "# Visualize comparison\n",
    "fig = plot_model_comparison(\n",
    "    stats_list=[stats_linear, stats_ridge, stats_lasso, stats_rf, stats_tree],\n",
    "    model_names=[\"Linear\", \"Ridge\", \"Lasso\", \"Random Forest\", \"Decision Tree\"],\n",
    "    metrics=[\"rmse\", \"mae\", \"r_squared\"],\n",
    "    split=\"test\",\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Individual Model Performance\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print test RMSE for each model\n",
    "print(\"\\nTest RMSE by model:\")\n",
    "test_rmses = {\n",
    "    \"Linear\": stats_linear[stats_linear['metric'] == 'rmse']['value'].values[1],  # test split\n",
    "    \"Ridge\": stats_ridge[stats_ridge['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Lasso\": stats_lasso[stats_lasso['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Random Forest\": stats_rf[stats_rf['metric'] == 'rmse']['value'].values[1],\n",
    "    \"Decision Tree\": stats_tree[stats_tree['metric'] == 'rmse']['value'].values[1]\n",
    "}\n",
    "\n",
    "for model, rmse in sorted(test_rmses.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {model:20s}: {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Best individual model: {min(test_rmses, key=test_rmses.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Ensemble with Stacking\n",
    "\n",
    "Now we'll use **py_stacks** to combine these models via meta-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions from each model\n",
    "outputs_linear, _, _ = fit_linear.extract_outputs()\n",
    "outputs_ridge, _, _ = fit_ridge.extract_outputs()\n",
    "outputs_lasso, _, _ = fit_lasso.extract_outputs()\n",
    "outputs_rf, _, _ = fit_rf.extract_outputs()\n",
    "outputs_tree, _, _ = fit_tree.extract_outputs()\n",
    "\n",
    "# Filter to test set predictions (for meta-learning)\n",
    "test_outputs_linear = outputs_linear[outputs_linear['split'] == 'test'].copy()\n",
    "test_outputs_ridge = outputs_ridge[outputs_ridge['split'] == 'test'].copy()\n",
    "test_outputs_lasso = outputs_lasso[outputs_lasso['split'] == 'test'].copy()\n",
    "test_outputs_rf = outputs_rf[outputs_rf['split'] == 'test'].copy()\n",
    "test_outputs_tree = outputs_tree[outputs_tree['split'] == 'test'].copy()\n",
    "\n",
    "# Rename .pred columns to avoid conflicts\n",
    "test_outputs_linear = test_outputs_linear.rename(columns={'.pred': '.pred'})\n",
    "test_outputs_ridge = test_outputs_ridge.rename(columns={'.pred': '.pred'})\n",
    "test_outputs_lasso = test_outputs_lasso.rename(columns={'.pred': '.pred'})\n",
    "test_outputs_rf = test_outputs_rf.rename(columns={'.pred': '.pred'})\n",
    "test_outputs_tree = test_outputs_tree.rename(columns={'.pred': '.pred'})\n",
    "\n",
    "print(\"âœ“ Extracted predictions from all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacks ensemble\n",
    "ensemble = (\n",
    "    stacks()\n",
    "    .add_candidates(test_outputs_linear, name=\"linear\")\n",
    "    .add_candidates(test_outputs_ridge, name=\"ridge\")\n",
    "    .add_candidates(test_outputs_lasso, name=\"lasso\")\n",
    "    .add_candidates(test_outputs_rf, name=\"random_forest\")\n",
    "    .add_candidates(test_outputs_tree, name=\"decision_tree\")\n",
    "    .blend_predictions(\n",
    "        penalty=0.01,        # Small penalty for regularization\n",
    "        mixture=1.0,         # Lasso (L1) for sparsity\n",
    "        non_negative=True    # Weights must be >= 0 (interpretability)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ“ Ensemble created with 5 base models\")\n",
    "print(\"âœ“ Meta-learner: Elastic Net with non-negative constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Model Weights\n",
    "\n",
    "The meta-learner assigns weights to each base model. Higher weights = more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model weights\n",
    "weights = ensemble.get_model_weights()\n",
    "\n",
    "print(\"Model Weights and Contributions:\")\n",
    "print(\"=\" * 70)\n",
    "print(weights.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"  â€¢ weight: Meta-learner coefficient for each model\")\n",
    "print(\"  â€¢ contribution_pct: Percentage contribution to ensemble\")\n",
    "print(\"  â€¢ Models with weight=0 are not used by the ensemble\")\n",
    "print(\"  â€¢ Non-negative constraint ensures weights >= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights (excluding intercept)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "model_weights = weights[weights['model'] != '(Intercept)'].copy()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=model_weights['model'],\n",
    "    y=model_weights['weight'],\n",
    "    text=model_weights['contribution_pct'].apply(lambda x: f\"{x:.1f}%\"),\n",
    "    textposition='auto',\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Model Weights in Ensemble\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Weight\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š The ensemble learns to emphasize models that complement each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Ensemble to Base Models\n",
    "\n",
    "Does the ensemble outperform individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comparison\n",
    "comparison = ensemble.compare_to_candidates()\n",
    "\n",
    "print(\"Ensemble vs Base Models (Test Set):\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Results:\")\n",
    "best_model = comparison.iloc[0]['model']\n",
    "best_rmse = comparison.iloc[0]['rmse']\n",
    "\n",
    "if best_model == 'Ensemble':\n",
    "    improvement = comparison.iloc[1]['rmse'] - best_rmse\n",
    "    pct_improvement = (improvement / comparison.iloc[1]['rmse']) * 100\n",
    "    print(f\"  âœ“ Ensemble is the best model!\")\n",
    "    print(f\"  âœ“ Improved RMSE by {improvement:.4f} ({pct_improvement:.2f}%)\")\n",
    "else:\n",
    "    print(f\"  â€¢ Best model: {best_model}\")\n",
    "    ensemble_rank = comparison[comparison['model'] == 'Ensemble'].index[0] + 1\n",
    "    print(f\"  â€¢ Ensemble rank: #{ensemble_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "# Highlight ensemble\n",
    "colors = ['red' if model == 'Ensemble' else 'steelblue' \n",
    "          for model in comparison['model']]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison['model'],\n",
    "    y=comparison['rmse'],\n",
    "    marker_color=colors,\n",
    "    text=comparison['rmse'].apply(lambda x: f\"{x:.4f}\"),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Test RMSE: Ensemble vs Base Models (Lower is Better)\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"RMSE\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Red bar = Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get Ensemble Metrics\n",
    "\n",
    "View detailed performance metrics for the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ensemble metrics\n",
    "metrics = ensemble.get_metrics()\n",
    "\n",
    "print(\"Ensemble Performance Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for _, row in metrics.iterrows():\n",
    "    print(f\"{row['metric']:12s}: {row['value']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Metrics calculated on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Experiment with Different Penalties\n",
    "\n",
    "The penalty parameter controls regularization strength. Let's compare different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different penalty values\n",
    "penalties = [0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for pen in penalties:\n",
    "    ensemble_temp = (\n",
    "        stacks()\n",
    "        .add_candidates(test_outputs_linear, name=\"linear\")\n",
    "        .add_candidates(test_outputs_ridge, name=\"ridge\")\n",
    "        .add_candidates(test_outputs_lasso, name=\"lasso\")\n",
    "        .add_candidates(test_outputs_rf, name=\"random_forest\")\n",
    "        .add_candidates(test_outputs_tree, name=\"decision_tree\")\n",
    "        .blend_predictions(penalty=pen, mixture=1.0, non_negative=True)\n",
    "    )\n",
    "    \n",
    "    metrics_temp = ensemble_temp.get_metrics()\n",
    "    rmse = metrics_temp[metrics_temp['metric'] == 'rmse']['value'].values[0]\n",
    "    \n",
    "    # Count non-zero weights\n",
    "    weights_temp = ensemble_temp.get_model_weights()\n",
    "    n_nonzero = (weights_temp[weights_temp['model'] != '(Intercept)']['weight'] > 0.001).sum()\n",
    "    \n",
    "    results.append({\n",
    "        'penalty': pen,\n",
    "        'rmse': rmse,\n",
    "        'n_models_used': n_nonzero\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Effect of Penalty on Ensemble:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Observations:\")\n",
    "print(\"  â€¢ Lower penalty â†’ More models used (less regularization)\")\n",
    "print(\"  â€¢ Higher penalty â†’ Fewer models used (more sparsity)\")\n",
    "print(\"  â€¢ Trade-off between model complexity and performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize penalty effect\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df['penalty'],\n",
    "    y=results_df['rmse'],\n",
    "    mode='lines+markers',\n",
    "    name='RMSE',\n",
    "    line=dict(color='steelblue', width=2),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Effect of Penalty on Ensemble RMSE\",\n",
    "    xaxis_title=\"Penalty (log scale)\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_title=\"RMSE\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Why Use py_stacks?\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Improved Performance**: Ensembles often outperform individual models\n",
    "2. **Automatic Weight Learning**: Meta-learner finds optimal model combination\n",
    "3. **Interpretability**: Non-negative weights show each model's contribution\n",
    "4. **Regularization**: Elastic net prevents overfitting and promotes sparsity\n",
    "5. **Flexibility**: Easy to add/remove candidate models\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Diversity**: Use diverse base models (linear, tree-based, etc.)\n",
    "2. **Quality**: Start with good individual models\n",
    "3. **Regularization**: Tune penalty parameter to avoid overfitting\n",
    "4. **Non-negativity**: Use `non_negative=True` for interpretability\n",
    "5. **Validation**: Always evaluate on held-out test set\n",
    "\n",
    "### When to Stack\n",
    "\n",
    "- When you have multiple good models with different strengths\n",
    "- When individual models make different types of errors\n",
    "- When prediction accuracy is critical\n",
    "- When you can afford the extra computational cost\n",
    "\n",
    "### Method Chaining API\n",
    "\n",
    "```python\n",
    "ensemble = (\n",
    "    stacks()\n",
    "    .add_candidates(pred1, name=\"model_1\")\n",
    "    .add_candidates(pred2, name=\"model_2\")\n",
    "    .add_candidates(pred3, name=\"model_3\")\n",
    "    .blend_predictions(penalty=0.01, non_negative=True)\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "weights = ensemble.get_model_weights()\n",
    "metrics = ensemble.get_metrics()\n",
    "comparison = ensemble.compare_to_candidates()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
