{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Time Series Forecasting with NHITS and NBEATS\n",
    "\n",
    "This notebook demonstrates deep learning time series forecasting using NeuralForecast models:\n",
    "- **NHITS** (Neural Hierarchical Interpolation for Time Series)\n",
    "- **NBEATS** (Neural Basis Expansion Analysis for Time Series)\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Data Preparation\n",
    "2. NHITS: Multi-Scale Forecasting\n",
    "3. NBEATS: Interpretable Decomposition\n",
    "4. Comparing NHITS vs NBEATS\n",
    "5. Working with Exogenous Variables\n",
    "6. Workflow Integration\n",
    "7. GPU Acceleration\n",
    "8. Comparison with Traditional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_parsnip import nhits_reg, nbeats_reg, prophet_reg, arima_reg\n",
    "from py_recipes import recipe\n",
    "from py_workflows import workflow\n",
    "from py_yardstick import rmse, mae, r_squared, metric_set\n",
    "from py_rsample import initial_time_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic daily sales data with trend, weekly seasonality, and noise\n",
    "n_days = 730  # 2 years\n",
    "dates = pd.date_range(start='2021-01-01', periods=n_days, freq='D')\n",
    "\n",
    "# Components\n",
    "trend = np.linspace(100, 150, n_days)  # Linear growth\n",
    "weekly_seasonality = 20 * np.sin(2 * np.pi * np.arange(n_days) / 7)  # Weekly cycle\n",
    "noise = np.random.normal(0, 5, n_days)\n",
    "\n",
    "# Combine components\n",
    "sales = trend + weekly_seasonality + noise\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "# Add exogenous variables (for NHITS demonstrations)\n",
    "data['price'] = 50 + np.random.normal(0, 5, n_days)\n",
    "data['promo'] = np.random.binomial(1, 0.2, n_days)  # 20% promo days\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(data['date'], data['sales'], alpha=0.7, label='Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Daily Sales Data (with trend and weekly seasonality)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (80/20)\n",
    "split = initial_time_split(data, prop=0.8)\n",
    "train_data = split.train_set\n",
    "test_data = split.test_set\n",
    "\n",
    "print(f\"Train: {len(train_data)} observations ({train_data['date'].min()} to {train_data['date'].max()})\")\n",
    "print(f\"Test: {len(test_data)} observations ({test_data['date'].min()} to {test_data['date'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NHITS: Multi-Scale Forecasting\n",
    "\n",
    "NHITS (Neural Hierarchical Interpolation for Time Series) uses a multi-scale architecture to capture patterns at different time resolutions:\n",
    "- **Long-term trends** (low frequency)\n",
    "- **Medium-term patterns** (mid frequency)\n",
    "- **Short-term dynamics** (high frequency)\n",
    "\n",
    "**When to use NHITS:**\n",
    "- Need to include exogenous variables\n",
    "- Long forecast horizons (30+ steps)\n",
    "- Multi-scale time series patterns\n",
    "- Maximum accuracy is priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NHITS model\n",
    "nhits_spec = nhits_reg(\n",
    "    horizon=7,                     # Forecast 7 days ahead\n",
    "    input_size=28,                 # Use 28 days of history (4 weeks)\n",
    "    n_freq_downsample=[8, 4, 1],   # 3 stacks: long/medium/short term\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=100,                 # Quick training for demo (use 1000+ in production)\n",
    "    batch_size=32,\n",
    "    early_stop_patience_steps=20,\n",
    "    device='auto',                 # Auto-select GPU if available\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"NHITS model specification created\")\n",
    "print(f\"Model type: {nhits_spec.model_type}\")\n",
    "print(f\"Engine: {nhits_spec.engine}\")\n",
    "print(f\"Mode: {nhits_spec.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NHITS model (univariate - no exogenous variables)\n",
    "print(\"Training NHITS model...\")\n",
    "nhits_fit = nhits_spec.fit(train_data, \"sales ~ date\")\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "nhits_preds = nhits_fit.predict(test_data, type='numeric')\n",
    "print(f\"Predictions shape: {nhits_preds.shape}\")\n",
    "nhits_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "nhits_fit_eval = nhits_fit.evaluate(test_data)\n",
    "\n",
    "# Extract outputs\n",
    "nhits_outputs, nhits_coeffs, nhits_stats = nhits_fit_eval.extract_outputs()\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nNHITS Performance Metrics:\")\n",
    "print(nhits_stats[nhits_stats['split'] == 'test'][['split', 'rmse', 'mae', 'r_squared', 'train_time', 'device']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NHITS predictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "train_outputs = nhits_outputs[nhits_outputs['split'] == 'train']\n",
    "plt.plot(train_outputs['dates'], train_outputs['actuals'], alpha=0.5, label='Train Actuals', color='blue')\n",
    "plt.plot(train_outputs['dates'], train_outputs['fitted'], alpha=0.5, label='Train Fitted', color='cyan')\n",
    "\n",
    "# Test data\n",
    "test_outputs = nhits_outputs[nhits_outputs['split'] == 'test']\n",
    "plt.plot(test_outputs['dates'], test_outputs['actuals'], label='Test Actuals', color='green', linewidth=2)\n",
    "plt.plot(test_outputs['dates'], test_outputs['fitted'], label='NHITS Forecast', color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.axvline(test_data['date'].min(), color='black', linestyle=':', alpha=0.5, label='Train/Test Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('NHITS Forecasting Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NBEATS: Interpretable Decomposition\n",
    "\n",
    "NBEATS (Neural Basis Expansion Analysis for Time Series) provides interpretable forecasts by decomposing the series into:\n",
    "- **Trend** (polynomial basis)\n",
    "- **Seasonality** (harmonic basis)\n",
    "- **Generic** (learned basis)\n",
    "\n",
    "**When to use NBEATS:**\n",
    "- Pure univariate forecasting (no exogenous variables needed)\n",
    "- Interpretability is important\n",
    "- Want to understand trend vs seasonality contributions\n",
    "- Medium forecast horizons (7-30 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NBEATS model with interpretable stacks\n",
    "nbeats_spec = nbeats_reg(\n",
    "    horizon=7,                              # 7-day forecast\n",
    "    input_size=28,                          # 28-day lookback\n",
    "    stack_types=['trend', 'seasonality'],   # Interpretable decomposition\n",
    "    n_polynomials=2,                        # Quadratic trend\n",
    "    n_harmonics=7,                          # Weekly seasonality (7 harmonics)\n",
    "    n_blocks=[1, 1],                        # 1 block per stack\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=100,\n",
    "    batch_size=32,\n",
    "    early_stop_patience_steps=20,\n",
    "    device='auto',\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"NBEATS model specification created\")\n",
    "print(f\"Stack types: {nbeats_spec.args['stack_types']}\")\n",
    "print(f\"Trend complexity: {nbeats_spec.args['n_polynomials']} (polynomial degree)\")\n",
    "print(f\"Seasonality complexity: {nbeats_spec.args['n_harmonics']} (harmonics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NBEATS model (univariate only)\n",
    "print(\"Training NBEATS model...\")\n",
    "nbeats_fit = nbeats_spec.fit(train_data, \"sales ~ date\")\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "nbeats_preds = nbeats_fit.predict(test_data, type='numeric')\n",
    "nbeats_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "nbeats_fit_eval = nbeats_fit.evaluate(test_data)\n",
    "nbeats_outputs, nbeats_coeffs, nbeats_stats = nbeats_fit_eval.extract_outputs()\n",
    "\n",
    "print(\"\\nNBEATS Performance Metrics:\")\n",
    "print(nbeats_stats[nbeats_stats['split'] == 'test'][['split', 'rmse', 'mae', 'r_squared', 'train_time', 'device']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NBEATS predictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "train_outputs = nbeats_outputs[nbeats_outputs['split'] == 'train']\n",
    "plt.plot(train_outputs['dates'], train_outputs['actuals'], alpha=0.5, label='Train Actuals', color='blue')\n",
    "plt.plot(train_outputs['dates'], train_outputs['fitted'], alpha=0.5, label='Train Fitted', color='cyan')\n",
    "\n",
    "# Test data\n",
    "test_outputs = nbeats_outputs[nbeats_outputs['split'] == 'test']\n",
    "plt.plot(test_outputs['dates'], test_outputs['actuals'], label='Test Actuals', color='green', linewidth=2)\n",
    "plt.plot(test_outputs['dates'], test_outputs['fitted'], label='NBEATS Forecast', color='orange', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.axvline(test_data['date'].min(), color='black', linestyle=':', alpha=0.5, label='Train/Test Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('NBEATS Forecasting Results (Interpretable Stacks)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing NHITS vs NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics side-by-side\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['NHITS', 'NBEATS'],\n",
    "    'RMSE': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['rmse'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['rmse'].values[0]\n",
    "    ],\n",
    "    'MAE': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['mae'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['mae'].values[0]\n",
    "    ],\n",
    "    'R²': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['r_squared'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['r_squared'].values[0]\n",
    "    ],\n",
    "    'Train Time (s)': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['train_time'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['train_time'].values[0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Highlight better model\n",
    "best_rmse = comparison.loc[comparison['RMSE'].idxmin(), 'Model']\n",
    "print(f\"\\n✓ Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# NHITS\n",
    "test_nhits = nhits_outputs[nhits_outputs['split'] == 'test']\n",
    "axes[0].plot(test_nhits['dates'], test_nhits['actuals'], label='Actual', color='green', linewidth=2)\n",
    "axes[0].plot(test_nhits['dates'], test_nhits['fitted'], label='NHITS Forecast', color='red', linewidth=2, linestyle='--')\n",
    "axes[0].fill_between(test_nhits['dates'], test_nhits['actuals'], test_nhits['fitted'], alpha=0.2, color='red')\n",
    "axes[0].set_title(f'NHITS - RMSE: {comparison.loc[0, \"RMSE\"]:.2f}, R²: {comparison.loc[0, \"R²\"]:.4f}')\n",
    "axes[0].set_ylabel('Sales')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# NBEATS\n",
    "test_nbeats = nbeats_outputs[nbeats_outputs['split'] == 'test']\n",
    "axes[1].plot(test_nbeats['dates'], test_nbeats['actuals'], label='Actual', color='green', linewidth=2)\n",
    "axes[1].plot(test_nbeats['dates'], test_nbeats['fitted'], label='NBEATS Forecast', color='orange', linewidth=2, linestyle='--')\n",
    "axes[1].fill_between(test_nbeats['dates'], test_nbeats['actuals'], test_nbeats['fitted'], alpha=0.2, color='orange')\n",
    "axes[1].set_title(f'NBEATS - RMSE: {comparison.loc[1, \"RMSE\"]:.2f}, R²: {comparison.loc[1, \"R²\"]:.4f}')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Sales')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Exogenous Variables (NHITS Only)\n",
    "\n",
    "NHITS supports exogenous variables, while NBEATS is univariate only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NHITS with exogenous variables\n",
    "print(\"Training NHITS with exogenous variables (price, promo)...\")\n",
    "nhits_exog_fit = nhits_spec.fit(train_data, \"sales ~ price + promo + date\")\n",
    "print(\"✓ Training complete\")\n",
    "\n",
    "# Predict\n",
    "nhits_exog_preds = nhits_exog_fit.predict(test_data, type='numeric')\n",
    "\n",
    "# Evaluate\n",
    "nhits_exog_fit_eval = nhits_exog_fit.evaluate(test_data)\n",
    "_, _, nhits_exog_stats = nhits_exog_fit_eval.extract_outputs()\n",
    "\n",
    "print(\"\\nNHITS with Exogenous Variables:\")\n",
    "print(nhits_exog_stats[nhits_exog_stats['split'] == 'test'][['split', 'rmse', 'mae', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBEATS with exogenous variables (will warn)\n",
    "print(\"\\nTrying NBEATS with exogenous variables (will warn and ignore them)...\")\n",
    "nbeats_exog_fit = nbeats_spec.fit(train_data, \"sales ~ price + promo + date\")\n",
    "print(\"\\n↑ Notice the warning above: NBEATS ignores exogenous variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow Integration\n",
    "\n",
    "DL models integrate seamlessly with py_recipes and py_workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recipe with time series preprocessing\n",
    "from py_recipes import all_numeric_predictors, step_lag, step_rolling, step_normalize\n",
    "\n",
    "dl_recipe = (\n",
    "    recipe(train_data, \"sales ~ date\")\n",
    "    .step_lag(['sales'], lags=[1, 7])  # 1-day and 1-week lags\n",
    "    .step_rolling(['sales'], window=7, stats=['mean'])  # 7-day rolling average\n",
    "    .step_normalize(all_numeric_predictors())  # Normalize features\n",
    ")\n",
    "\n",
    "print(\"Recipe created with time series preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "nhits_workflow = (\n",
    "    workflow()\n",
    "    .add_recipe(dl_recipe)\n",
    "    .add_model(nhits_spec)\n",
    ")\n",
    "\n",
    "print(\"Workflow created\")\n",
    "\n",
    "# Note: Due to lag features, we lose some initial rows\n",
    "# Use data after sufficient history\n",
    "train_subset = train_data.iloc[7:]  # Skip first 7 days\n",
    "test_subset = test_data.iloc[7:]\n",
    "\n",
    "# Fit workflow\n",
    "print(\"\\nTraining workflow...\")\n",
    "nhits_wf_fit = nhits_workflow.fit(train_subset)\n",
    "print(\"✓ Workflow training complete\")\n",
    "\n",
    "# Predict\n",
    "nhits_wf_preds = nhits_wf_fit.predict(test_subset)\n",
    "print(f\"\\nWorkflow predictions shape: {nhits_wf_preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GPU Acceleration\n",
    "\n",
    "Check GPU availability and compare training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "from py_parsnip.utils import detect_available_devices, get_optimal_device\n",
    "\n",
    "devices = detect_available_devices()\n",
    "optimal = get_optimal_device()\n",
    "\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(f\"  - {device}\")\n",
    "print(f\"\\nOptimal device: {optimal}\")\n",
    "\n",
    "if 'cuda' in devices:\n",
    "    print(\"\\n✓ GPU available - training will be 10-50x faster!\")\n",
    "elif 'mps' in devices:\n",
    "    print(\"\\n✓ Apple Silicon GPU (MPS) available - training will be 5-15x faster!\")\n",
    "else:\n",
    "    print(\"\\n⚠ No GPU available - using CPU (slower but works)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CPU vs GPU training time (if GPU available)\n",
    "import time\n",
    "\n",
    "# CPU model\n",
    "nhits_cpu = nhits_reg(horizon=7, input_size=28, max_steps=100, device='cpu', random_seed=42)\n",
    "start = time.time()\n",
    "nhits_cpu.fit(train_data.iloc[:200], \"sales ~ date\")  # Subset for faster demo\n",
    "cpu_time = time.time() - start\n",
    "print(f\"CPU training time: {cpu_time:.2f} seconds\")\n",
    "\n",
    "# GPU model (if available)\n",
    "if optimal != 'cpu':\n",
    "    nhits_gpu = nhits_reg(horizon=7, input_size=28, max_steps=100, device=optimal, random_seed=42)\n",
    "    start = time.time()\n",
    "    nhits_gpu.fit(train_data.iloc[:200], \"sales ~ date\")\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"{optimal.upper()} training time: {gpu_time:.2f} seconds\")\n",
    "    print(f\"\\nSpeedup: {cpu_time / gpu_time:.1f}x faster on {optimal.upper()}\")\n",
    "else:\n",
    "    print(\"\\nGPU not available - skipping GPU benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Traditional Models\n",
    "\n",
    "Compare NHITS/NBEATS with Prophet and ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet\n",
    "print(\"Training Prophet...\")\n",
    "prophet_spec = prophet_reg()\n",
    "prophet_fit = prophet_spec.fit(train_data, \"sales ~ date\")\n",
    "prophet_fit_eval = prophet_fit.evaluate(test_data)\n",
    "_, _, prophet_stats = prophet_fit_eval.extract_outputs()\n",
    "print(\"✓ Prophet complete\")\n",
    "\n",
    "# Train ARIMA\n",
    "print(\"\\nTraining ARIMA...\")\n",
    "arima_spec = arima_reg(non_seasonal_ar=1, non_seasonal_differences=1, non_seasonal_ma=1)\n",
    "arima_fit = arima_spec.fit(train_data, \"sales ~ date\")\n",
    "arima_fit_eval = arima_fit.evaluate(test_data)\n",
    "_, _, arima_stats = arima_fit_eval.extract_outputs()\n",
    "print(\"✓ ARIMA complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "all_models = pd.DataFrame({\n",
    "    'Model': ['NHITS', 'NBEATS', 'Prophet', 'ARIMA'],\n",
    "    'RMSE': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['rmse'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['rmse'].values[0],\n",
    "        prophet_stats[prophet_stats['split'] == 'test']['rmse'].values[0],\n",
    "        arima_stats[arima_stats['split'] == 'test']['rmse'].values[0]\n",
    "    ],\n",
    "    'MAE': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['mae'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['mae'].values[0],\n",
    "        prophet_stats[prophet_stats['split'] == 'test']['mae'].values[0],\n",
    "        arima_stats[arima_stats['split'] == 'test']['mae'].values[0]\n",
    "    ],\n",
    "    'R²': [\n",
    "        nhits_stats[nhits_stats['split'] == 'test']['r_squared'].values[0],\n",
    "        nbeats_stats[nbeats_stats['split'] == 'test']['r_squared'].values[0],\n",
    "        prophet_stats[prophet_stats['split'] == 'test']['r_squared'].values[0],\n",
    "        arima_stats[arima_stats['split'] == 'test']['r_squared'].values[0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by RMSE\n",
    "all_models = all_models.sort_values('RMSE').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS COMPARISON (sorted by RMSE)\")\n",
    "print(\"=\"*60)\n",
    "print(all_models.to_string(index=False))\n",
    "print(\"\\n✓ Best model:\", all_models.iloc[0]['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(all_models['Model'], all_models['RMSE'], color=['red', 'orange', 'blue', 'green'])\n",
    "axes[0].set_ylabel('RMSE (lower is better)')\n",
    "axes[0].set_title('Model Comparison: RMSE')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# R² comparison\n",
    "axes[1].bar(all_models['Model'], all_models['R²'], color=['red', 'orange', 'blue', 'green'])\n",
    "axes[1].set_ylabel('R² (higher is better)')\n",
    "axes[1].set_title('Model Comparison: R²')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **NHITS**:\n",
    "   - Multi-scale architecture (long/medium/short term patterns)\n",
    "   - Supports exogenous variables\n",
    "   - Best for complex patterns and long horizons\n",
    "\n",
    "2. **NBEATS**:\n",
    "   - Interpretable decomposition (trend + seasonality)\n",
    "   - Univariate only (no exogenous variables)\n",
    "   - Best for understanding forecast components\n",
    "\n",
    "3. **When to Use DL vs Traditional**:\n",
    "   - **Use DL** (NHITS/NBEATS) when:\n",
    "     - Large datasets (500+ observations)\n",
    "     - Complex patterns\n",
    "     - GPU available\n",
    "     - Maximum accuracy needed\n",
    "   \n",
    "   - **Use Traditional** (Prophet/ARIMA) when:\n",
    "     - Small datasets (< 500 observations)\n",
    "     - Simple patterns\n",
    "     - Interpretability critical\n",
    "     - Fast inference needed\n",
    "\n",
    "4. **GPU Acceleration**:\n",
    "   - CUDA (NVIDIA): 10-50x faster\n",
    "   - MPS (Apple M1/M2): 5-15x faster\n",
    "   - CPU: Always available, slower but works\n",
    "\n",
    "### Next Steps:\n",
    "- Try hyperparameter tuning with `py_tune`\n",
    "- Experiment with different architectures\n",
    "- Test on your own time series data\n",
    "- Explore hybrid models (NBEATS + XGBoost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "py-tidymodels2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
