{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be862fc",
   "metadata": {},
   "source": [
    "# SHAP Interpretability with py-tidymodels\n",
    "\n",
    "This notebook demonstrates model interpretability using SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "**Topics Covered:**\n",
    "1. SHAP explanations for different model types (tree, linear)\n",
    "2. Auto-explainer selection\n",
    "3. Global feature importance\n",
    "4. Local explanations (single observations)\n",
    "5. SHAP with workflows and recipes\n",
    "6. Grouped model SHAP (per-group feature importance)\n",
    "7. Identifying prediction errors with SHAP\n",
    "\n",
    "**Use Case:** Customer churn prediction with explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from py_parsnip import linear_reg, rand_forest, decision_tree\n",
    "from py_workflows import workflow\n",
    "from py_recipes import recipe\n",
    "from py_interpret import ShapEngine\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6c180",
   "metadata": {},
   "source": [
    "## 1. Generate Customer Churn Data\n",
    "\n",
    "Create realistic customer data with:\n",
    "- Demographics (age, tenure)\n",
    "- Engagement metrics (login frequency, feature usage)\n",
    "- Service attributes (plan type, support calls)\n",
    "- Target: Churn probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic customer churn data\n",
    "n = 300\n",
    "\n",
    "# Customer demographics\n",
    "age = np.random.randint(18, 70, n)\n",
    "tenure_months = np.random.randint(1, 60, n)\n",
    "\n",
    "# Engagement metrics\n",
    "logins_per_month = np.random.poisson(10, n)\n",
    "features_used = np.random.randint(0, 20, n)\n",
    "avg_session_minutes = np.random.exponential(30, n)\n",
    "\n",
    "# Service attributes\n",
    "plan_type = np.random.choice(['Basic', 'Premium', 'Enterprise'], n, p=[0.5, 0.3, 0.2])\n",
    "support_calls = np.random.poisson(2, n)\n",
    "\n",
    "# Create churn probability (hidden true relationship)\n",
    "# Higher churn if:\n",
    "# - Short tenure\n",
    "# - Low engagement\n",
    "# - Many support calls\n",
    "# - Basic plan\n",
    "churn_logit = (\n",
    "    -2.0 +  # Baseline\n",
    "    -0.05 * tenure_months +  # Longer tenure = lower churn\n",
    "    -0.1 * logins_per_month +  # More logins = lower churn\n",
    "    -0.05 * features_used +  # More features = lower churn\n",
    "    0.3 * support_calls +  # More support = higher churn\n",
    "    -0.02 * age +  # Older customers = lower churn\n",
    "    (1.0 if plan_type == 'Basic' else -0.5)  # Basic plan = higher churn\n",
    ")\n",
    "\n",
    "churn_prob = 1 / (1 + np.exp(-churn_logit))\n",
    "churn = (np.random.rand(n) < churn_prob).astype(float)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'churn': churn,\n",
    "    'age': age,\n",
    "    'tenure_months': tenure_months,\n",
    "    'logins_per_month': logins_per_month,\n",
    "    'features_used': features_used,\n",
    "    'avg_session_minutes': avg_session_minutes,\n",
    "    'plan_type': plan_type,\n",
    "    'support_calls': support_calls\n",
    "})\n",
    "\n",
    "# For regression demo, use continuous target\n",
    "data['churn_risk'] = churn_prob + np.random.randn(n) * 0.1\n",
    "\n",
    "# Split into train/test\n",
    "train_data = data.iloc[:240]\n",
    "test_data = data.iloc[240:]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} observations\")\n",
    "print(f\"Test data: {len(test_data)} observations\")\n",
    "print(f\"\\nChurn rate: {data['churn'].mean():.1%}\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by churn\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "features_to_plot = ['age', 'tenure_months', 'logins_per_month',\n",
    "                     'features_used', 'avg_session_minutes', 'support_calls']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "\n",
    "    churned = train_data[train_data['churn'] == 1][feature]\n",
    "    retained = train_data[train_data['churn'] == 0][feature]\n",
    "\n",
    "    ax.hist(retained, alpha=0.5, label='Retained', bins=20)\n",
    "    ax.hist(churned, alpha=0.5, label='Churned', bins=20)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca78bec",
   "metadata": {},
   "source": [
    "## 2. Basic SHAP with Linear Regression\n",
    "\n",
    "SHAP auto-selects LinearExplainer for linear models (fast and exact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression model\n",
    "spec_linear = linear_reg()\n",
    "fit_linear = spec_linear.fit(train_data, 'churn_risk ~ age + tenure_months + logins_per_month + features_used + support_calls')\n",
    "\n",
    "print(\"Linear model fitted!\")\n",
    "print(f\"Model type: {fit_linear.spec.model_type}\")\n",
    "print(f\"Engine: {fit_linear.spec.engine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values (auto-selects LinearExplainer)\n",
    "shap_linear = fit_linear.explain(test_data, check_additivity=False)\n",
    "\n",
    "print(\"SHAP values computed!\")\n",
    "print(f\"\\nSHAP DataFrame shape: {shap_linear.shape}\")\n",
    "print(f\"Columns: {shap_linear.columns.tolist()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(shap_linear.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global feature importance (mean |SHAP|)\n",
    "importance_linear = shap_linear.groupby('variable')['abs_shap'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nGlobal Feature Importance (Linear Model):\")\n",
    "for var, imp in importance_linear.items():\n",
    "    print(f\"  {var:<25} {imp:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "importance_linear.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Global Feature Importance (Linear Model)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de2a42",
   "metadata": {},
   "source": [
    "## 3. SHAP with Random Forest (TreeExplainer)\n",
    "\n",
    "TreeExplainer is fast and exact for tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7816675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit random forest model\n",
    "spec_rf = rand_forest(trees=100, min_n=5).set_mode('regression')\n",
    "fit_rf = spec_rf.fit(train_data, 'churn_risk ~ age + tenure_months + logins_per_month + features_used + support_calls')\n",
    "\n",
    "print(\"Random Forest model fitted!\")\n",
    "print(f\"Trees: {fit_rf.spec.args.get('trees', 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe438e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP (auto-selects TreeExplainer - fast!)\n",
    "shap_rf = fit_rf.explain(test_data, check_additivity=False)\n",
    "\n",
    "print(\"SHAP values computed with TreeExplainer!\")\n",
    "print(f\"Shape: {shap_rf.shape}\")\n",
    "\n",
    "# Global importance\n",
    "importance_rf = shap_rf.groupby('variable')['abs_shap'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "for var, imp in importance_rf.items():\n",
    "    print(f\"  {var:<25} {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance: Linear vs Random Forest\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Linear': importance_linear,\n",
    "    'Random_Forest': importance_rf\n",
    "}).fillna(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "comparison_df.plot(kind='barh', ax=ax, width=0.8)\n",
    "ax.set_xlabel('Mean |SHAP Value|')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Feature Importance Comparison: Linear vs Random Forest')\n",
    "ax.legend(title='Model Type')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Random Forest captures non-linear relationships\")\n",
    "print(\"- Different models may emphasize different features\")\n",
    "print(\"- SHAP provides model-agnostic comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74eb17",
   "metadata": {},
   "source": [
    "## 4. Local Explanations (Single Observation)\n",
    "\n",
    "Explain individual predictions with SHAP waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a high-risk customer (high predicted churn)\n",
    "preds_rf = fit_rf.predict(test_data, type='numeric')\n",
    "test_with_preds = test_data.copy()\n",
    "test_with_preds['pred_risk'] = preds_rf['.pred'].values\n",
    "\n",
    "# Find highest risk customer\n",
    "high_risk_idx = test_with_preds['pred_risk'].idxmax()\n",
    "high_risk_customer = test_data.loc[[high_risk_idx]]\n",
    "\n",
    "print(\"High-Risk Customer Profile:\")\n",
    "print(high_risk_customer.T)\n",
    "print(f\"\\nPredicted churn risk: {test_with_preds.loc[high_risk_idx, 'pred_risk']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP for this customer\n",
    "shap_single = fit_rf.explain(high_risk_customer, check_additivity=False)\n",
    "\n",
    "print(f\"\\nSHAP Explanation for Customer {high_risk_idx}:\")\n",
    "print(f\"Base value (average prediction): {shap_single['base_value'].iloc[0]:.4f}\")\n",
    "print(f\"Final prediction: {shap_single['prediction'].iloc[0]:.4f}\")\n",
    "print(f\"\\nFeature Contributions:\")\n",
    "\n",
    "# Sort by absolute impact\n",
    "shap_sorted = shap_single.sort_values('abs_shap', ascending=False)\n",
    "\n",
    "for _, row in shap_sorted.iterrows():\n",
    "    sign = \"+\" if row['shap_value'] >= 0 else \"\"\n",
    "    direction = \"↑ increases\" if row['shap_value'] >= 0 else \"↓ decreases\"\n",
    "    print(f\"  {row['variable']:<25} {sign}{row['shap_value']:>8.4f}  {direction} risk (value={row['feature_value']:.2f})\")\n",
    "\n",
    "# Verify additivity\n",
    "total_shap = shap_single['shap_value'].sum()\n",
    "base = shap_single['base_value'].iloc[0]\n",
    "pred = shap_single['prediction'].iloc[0]\n",
    "print(f\"\\nAdditivity Check:\")\n",
    "print(f\"  sum(SHAP) + base = {total_shap + base:.4f}\")\n",
    "print(f\"  prediction        = {pred:.4f}\")\n",
    "print(f\"  difference        = {abs((total_shap + base) - pred):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize waterfall plot\n",
    "def plot_waterfall(shap_df, customer_id):\n",
    "    \"\"\"Create waterfall plot for SHAP values.\"\"\"\n",
    "    shap_sorted = shap_df.sort_values('shap_value', ascending=True)\n",
    "\n",
    "    base_value = shap_sorted['base_value'].iloc[0]\n",
    "    prediction = shap_sorted['prediction'].iloc[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Start with base value\n",
    "    y_pos = 0\n",
    "    cumulative = base_value\n",
    "\n",
    "    colors = ['red' if v < 0 else 'green' for v in shap_sorted['shap_value']]\n",
    "\n",
    "    for idx, (_, row) in enumerate(shap_sorted.iterrows()):\n",
    "        shap_val = row['shap_value']\n",
    "        next_cumulative = cumulative + shap_val\n",
    "\n",
    "        # Draw bar\n",
    "        ax.barh(\n",
    "            y_pos,\n",
    "            abs(shap_val),\n",
    "            left=min(cumulative, next_cumulative),\n",
    "            color=colors[idx],\n",
    "            alpha=0.7,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        # Label\n",
    "        label = f\"{row['variable'][:20]}\\n{row['feature_value']:.2f}\"\n",
    "        ax.text(\n",
    "            cumulative + shap_val/2,\n",
    "            y_pos,\n",
    "            f\"{shap_val:+.3f}\",\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            fontweight='bold',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "        ax.text(\n",
    "            ax.get_xlim()[0],\n",
    "            y_pos,\n",
    "            label,\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "        cumulative = next_cumulative\n",
    "        y_pos += 1\n",
    "\n",
    "    # Add base and prediction lines\n",
    "    ax.axvline(base_value, color='blue', linestyle='--', label=f'Base: {base_value:.3f}', linewidth=2)\n",
    "    ax.axvline(prediction, color='red', linestyle='--', label=f'Prediction: {prediction:.3f}', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Churn Risk')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'SHAP Waterfall Plot - Customer {customer_id}')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_waterfall(shap_single, high_risk_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b76e5a",
   "metadata": {},
   "source": [
    "## 5. SHAP with Workflows and Recipes\n",
    "\n",
    "SHAP works seamlessly with preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow with recipe\n",
    "rec = (\n",
    "    recipe()\n",
    "    .step_normalize()  # Normalize numeric features\n",
    ")\n",
    "\n",
    "wf = workflow().add_recipe(rec).add_model(\n",
    "    rand_forest(trees=100, min_n=5).set_mode('regression')\n",
    ")\n",
    "\n",
    "wf_fit = wf.fit(train_data)\n",
    "print(\"Workflow with recipe fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ca6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP on normalized features (applied automatically)\n",
    "shap_wf = wf_fit.explain(test_data, check_additivity=False)\n",
    "\n",
    "importance_wf = shap_wf.groupby('variable')['abs_shap'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature Importance (with normalization):\")\n",
    "for var, imp in importance_wf.items():\n",
    "    print(f\"  {var:<25} {imp:.4f}\")\n",
    "\n",
    "# Compare with non-normalized\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "importance_rf.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Mean |SHAP|')\n",
    "axes[0].set_title('Without Normalization')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "importance_wf.plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_xlabel('Mean |SHAP|')\n",
    "axes[1].set_title('With Normalization')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle('Effect of Normalization on SHAP Importance', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9841de",
   "metadata": {},
   "source": [
    "## 6. Grouped Model SHAP (Per-Group Feature Importance)\n",
    "\n",
    "Analyze feature importance separately for different customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped data (by plan type)\n",
    "grouped_data = train_data.copy()\n",
    "grouped_test = test_data.copy()\n",
    "\n",
    "print(f\"Groups (plan types): {grouped_data['plan_type'].unique()}\")\n",
    "print(f\"\\nGroup sizes:\")\n",
    "print(grouped_data['plan_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54336cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit nested models (separate model per plan type)\n",
    "spec_nested = rand_forest(trees=100, min_n=5).set_mode('regression')\n",
    "\n",
    "nested_fit = spec_nested.fit_nested(\n",
    "    grouped_data,\n",
    "    'churn_risk ~ age + tenure_months + logins_per_month + features_used + support_calls',\n",
    "    group_col='plan_type'\n",
    ")\n",
    "\n",
    "print(f\"Fitted {len(nested_fit.group_fits)} models (one per plan type)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP per group\n",
    "shap_grouped = nested_fit.explain(grouped_test, check_additivity=False)\n",
    "\n",
    "print(\"\\nSHAP values computed for all groups!\")\n",
    "print(f\"Shape: {shap_grouped.shape}\")\n",
    "print(f\"Groups: {shap_grouped['group'].unique()}\")\n",
    "\n",
    "# Feature importance by group\n",
    "importance_by_group = shap_grouped.groupby(['group', 'variable'])['abs_shap'].mean().unstack()\n",
    "\n",
    "print(\"\\nFeature Importance by Plan Type:\")\n",
    "print(importance_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f96b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize group differences\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, group in enumerate(['Basic', 'Premium', 'Enterprise']):\n",
    "    if group in importance_by_group.index:\n",
    "        importance_by_group.loc[group].sort_values(ascending=True).plot(\n",
    "            kind='barh',\n",
    "            ax=axes[idx],\n",
    "            color='steelblue',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        axes[idx].set_title(f'{group} Plan')\n",
    "        axes[idx].set_xlabel('Mean |SHAP|')\n",
    "        if idx == 0:\n",
    "            axes[idx].set_ylabel('Feature')\n",
    "\n",
    "plt.suptitle('Feature Importance by Customer Segment', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Different customer segments have different risk drivers\")\n",
    "print(\"- Basic plan: Support calls more important\")\n",
    "print(\"- Premium/Enterprise: Engagement metrics more important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054510ee",
   "metadata": {},
   "source": [
    "## 7. Identifying Prediction Errors with SHAP\n",
    "\n",
    "Use SHAP to understand when and why the model makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and calculate errors\n",
    "preds_test = fit_rf.predict(test_data, type='numeric')\n",
    "test_analysis = test_data.copy()\n",
    "test_analysis['prediction'] = preds_test['.pred'].values\n",
    "test_analysis['actual'] = test_data['churn_risk'].values\n",
    "test_analysis['error'] = test_analysis['actual'] - test_analysis['prediction']\n",
    "test_analysis['abs_error'] = abs(test_analysis['error'])\n",
    "\n",
    "# Find worst predictions\n",
    "worst_idx = test_analysis['abs_error'].nlargest(3).index\n",
    "\n",
    "print(\"Worst Predictions:\")\n",
    "print(test_analysis.loc[worst_idx, ['actual', 'prediction', 'error', 'abs_error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain worst predictions\n",
    "worst_customers = test_data.loc[worst_idx]\n",
    "shap_worst = fit_rf.explain(worst_customers, check_additivity=False)\n",
    "\n",
    "# Analyze each bad prediction\n",
    "for customer_id in worst_idx:\n",
    "    customer_shap = shap_worst[shap_worst.index.get_level_values(0).isin([customer_id])]\n",
    "\n",
    "    actual = test_analysis.loc[customer_id, 'actual']\n",
    "    predicted = test_analysis.loc[customer_id, 'prediction']\n",
    "    error = test_analysis.loc[customer_id, 'error']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Customer {customer_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Actual: {actual:.3f}  |  Predicted: {predicted:.3f}  |  Error: {error:+.3f}\")\n",
    "    print(f\"\\nTop SHAP Contributors:\")\n",
    "\n",
    "    top_shap = customer_shap.nlargest(3, 'abs_shap')\n",
    "    for _, row in top_shap.iterrows():\n",
    "        sign = \"↑\" if row['shap_value'] >= 0 else \"↓\"\n",
    "        print(f\"  {row['variable']:<25} {sign} {row['shap_value']:>8.4f}  (value={row['feature_value']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d352d52",
   "metadata": {},
   "source": [
    "## 8. SHAP Dependence Plots\n",
    "\n",
    "Visualize how feature values affect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP dependence plot\n",
    "def plot_shap_dependence(shap_df, feature):\n",
    "    \"\"\"Plot SHAP values vs feature values.\"\"\"\n",
    "    feature_data = shap_df[shap_df['variable'] == feature].copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        feature_data['feature_value'],\n",
    "        feature_data['shap_value'],\n",
    "        c=feature_data['abs_shap'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel(f'{feature} (Feature Value)')\n",
    "    ax.set_ylabel('SHAP Value (Impact on Prediction)')\n",
    "    ax.set_title(f'SHAP Dependence Plot: {feature}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('|SHAP Value|')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for key features\n",
    "for feature in ['tenure_months', 'support_calls', 'logins_per_month']:\n",
    "    plot_shap_dependence(shap_rf, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b0d08",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **SHAP Advantages:**\n",
    "   - Model-agnostic explanations\n",
    "   - Theoretically grounded (Shapley values)\n",
    "   - Additivity property\n",
    "   - Local and global interpretability\n",
    "\n",
    "2. **Explainer Types:**\n",
    "   - TreeExplainer: Fast, exact for tree models\n",
    "   - LinearExplainer: Fast, exact for linear models\n",
    "   - KernelExplainer: Slow, model-agnostic fallback\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - Global importance: Which features matter most?\n",
    "   - Local explanations: Why this prediction?\n",
    "   - Model comparison: Different models, same metric\n",
    "   - Error analysis: When does model fail?\n",
    "   - Grouped models: Heterogeneous feature importance\n",
    "\n",
    "4. **Best Practices:**\n",
    "   - Use auto-selection (fast explainers when possible)\n",
    "   - Check additivity for verification\n",
    "   - Compare multiple models\n",
    "   - Analyze error cases\n",
    "   - Consider grouped models for heterogeneous data\n",
    "\n",
    "5. **Integration:**\n",
    "   - Works with ModelFit and WorkflowFit\n",
    "   - Handles preprocessing automatically\n",
    "   - Supports nested/grouped models\n",
    "   - Returns tidy DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Global importance comparison\n",
    "comparison_df.plot(kind='barh', ax=axes[0, 0], width=0.7)\n",
    "axes[0, 0].set_title('Model Comparison: Feature Importance')\n",
    "axes[0, 0].set_xlabel('Mean |SHAP|')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. SHAP distribution by feature\n",
    "shap_rf_pivot = shap_rf.pivot_table(\n",
    "    values='shap_value',\n",
    "    columns='variable',\n",
    "    aggfunc=list\n",
    ")\n",
    "shap_distributions = []\n",
    "labels = []\n",
    "for col in importance_rf.index[:5]:  # Top 5 features\n",
    "    vals = [v for sublist in shap_rf[shap_rf['variable'] == col]['shap_value'].tolist()\n",
    "            for v in ([sublist] if not isinstance(sublist, list) else sublist)]\n",
    "    shap_distributions.append(vals)\n",
    "    labels.append(col)\n",
    "\n",
    "axes[0, 1].boxplot(shap_distributions, labels=labels, vert=True)\n",
    "axes[0, 1].set_title('SHAP Value Distribution (Top 5 Features)')\n",
    "axes[0, 1].set_ylabel('SHAP Value')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Prediction scatter with SHAP coloring\n",
    "test_shap_summary = shap_rf.groupby(level=0)['abs_shap'].sum()\n",
    "scatter = axes[1, 0].scatter(\n",
    "    test_with_preds['pred_risk'],\n",
    "    test_with_preds.loc[test_shap_summary.index, 'churn_risk'],\n",
    "    c=test_shap_summary,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Predicted Risk')\n",
    "axes[1, 0].set_ylabel('Actual Risk')\n",
    "axes[1, 0].set_title('Predictions (colored by total |SHAP|)')\n",
    "cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "cbar.set_label('Total |SHAP|')\n",
    "\n",
    "# 4. Group-wise importance heatmap\n",
    "if 'Basic' in importance_by_group.index:\n",
    "    im = axes[1, 1].imshow(importance_by_group.values, cmap='YlOrRd', aspect='auto')\n",
    "    axes[1, 1].set_xticks(range(len(importance_by_group.columns)))\n",
    "    axes[1, 1].set_xticklabels(importance_by_group.columns, rotation=45, ha='right')\n",
    "    axes[1, 1].set_yticks(range(len(importance_by_group.index)))\n",
    "    axes[1, 1].set_yticklabels(importance_by_group.index)\n",
    "    axes[1, 1].set_title('Feature Importance Heatmap by Segment')\n",
    "    plt.colorbar(im, ax=axes[1, 1], label='Mean |SHAP|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP provides powerful model interpretability for any model type!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
