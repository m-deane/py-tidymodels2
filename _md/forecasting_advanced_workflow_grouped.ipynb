{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced Forecasting Workflow: Complete Tidymodels Pipeline - NEW Grouped API\n",
    "\n",
    "This notebook demonstrates the **complete tidymodels workflow** with **NEW grouped modeling**:\n",
    "- **WorkflowSets**: Multi-model and multi-preprocessing comparison\n",
    "- **Complex Recipes**: Advanced feature engineering pipelines\n",
    "- **Grouped Modeling**: NEW `fit_nested()` for per-group evaluation\n",
    "- **Hyperparameter Tuning**: Grid search optimization\n",
    "\n",
    "**Pattern**: Best practices across all tidymodels layers with NEW API\n",
    "\n",
    "## Workflow Overview:\n",
    "1. Data loading and panel structure\n",
    "2. Define complex preprocessing strategies\n",
    "3. Define multiple model specifications\n",
    "4. Create WorkflowSet from cross product\n",
    "5. **NEW**: Evaluate all workflows across ALL groups\n",
    "6. **NEW**: Rank and select best workflow using group-aware metrics\n",
    "7. Hyperparameter tuning for best workflow\n",
    "8. Apply optimized workflow to all groups\n",
    "9. Performance analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/matthewdeane/Documents/Data%20Science/python/_projects/py-tidymodels/_md\n",
      "\u001b[31mERROR: file:///Users/matthewdeane/Documents/Data%20Science/python/_projects/py-tidymodels/_md does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import linear_reg, rand_forest, boost_tree\n",
    "from py_rsample import initial_split, training, testing, time_series_cv\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_tune import tune, grid_regular, tune_grid, fit_resamples, finalize_workflow\n",
    "from py_workflowsets import WorkflowSet\n",
    "\n",
    "# Recipe imports\n",
    "from py_recipes import recipe\n",
    "from py_recipes.selectors import all_numeric_predictors, all_nominal_predictors\n",
    "from py_visualize import plot_forecast\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2713 Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load refinery margins panel data with multiple countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1920, 20)\n",
      "Countries: ['Algeria', 'Denmark', 'Germany', 'Italy', 'Netherlands', 'Norway', 'Romania', 'Russian Federation', 'Turkey', 'United Kingdom']\n",
      "Date range: 2006-01-01 00:00:00 to 2021-12-01 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>refinery_kbd</th>\n",
       "      <th>brent</th>\n",
       "      <th>dubai</th>\n",
       "      <th>wti</th>\n",
       "      <th>brent_cracking_nw_europe</th>\n",
       "      <th>brent_hydroskimming_nw_europe</th>\n",
       "      <th>urals_cracking_nw_europe</th>\n",
       "      <th>urals_hydroskimming_nw_europe</th>\n",
       "      <th>es_sider_cracking_med</th>\n",
       "      <th>es_sider_hydroskimming_med</th>\n",
       "      <th>urals_cracking_med</th>\n",
       "      <th>urals_hydroskimming_med</th>\n",
       "      <th>dubai_cracking_singapore</th>\n",
       "      <th>dubai_hydroskimming_singapore</th>\n",
       "      <th>tapis_hydroskimming_singapore</th>\n",
       "      <th>x50_50_hls_lls_cracking_usgc</th>\n",
       "      <th>x30_70_wcs_bakken_cracking_usmc</th>\n",
       "      <th>bakken_coking_usmc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>450.0000</td>\n",
       "      <td>63.57</td>\n",
       "      <td>58.31</td>\n",
       "      <td>65.48</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>172.9948</td>\n",
       "      <td>63.57</td>\n",
       "      <td>58.31</td>\n",
       "      <td>65.48</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2270.5419</td>\n",
       "      <td>63.57</td>\n",
       "      <td>58.31</td>\n",
       "      <td>65.48</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Italy</td>\n",
       "      <td>1859.7058</td>\n",
       "      <td>63.57</td>\n",
       "      <td>58.31</td>\n",
       "      <td>65.48</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>907.5555</td>\n",
       "      <td>63.57</td>\n",
       "      <td>58.31</td>\n",
       "      <td>65.48</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      country  refinery_kbd  brent  dubai    wti  \\\n",
       "0 2006-01-01      Algeria      450.0000  63.57  58.31  65.48   \n",
       "1 2006-01-01      Denmark      172.9948  63.57  58.31  65.48   \n",
       "2 2006-01-01      Germany     2270.5419  63.57  58.31  65.48   \n",
       "3 2006-01-01        Italy     1859.7058  63.57  58.31  65.48   \n",
       "4 2006-01-01  Netherlands      907.5555  63.57  58.31  65.48   \n",
       "\n",
       "   brent_cracking_nw_europe  brent_hydroskimming_nw_europe  \\\n",
       "0                      3.12                          -2.51   \n",
       "1                      3.12                          -2.51   \n",
       "2                      3.12                          -2.51   \n",
       "3                      3.12                          -2.51   \n",
       "4                      3.12                          -2.51   \n",
       "\n",
       "   urals_cracking_nw_europe  urals_hydroskimming_nw_europe  \\\n",
       "0                       5.3                          -2.18   \n",
       "1                       5.3                          -2.18   \n",
       "2                       5.3                          -2.18   \n",
       "3                       5.3                          -2.18   \n",
       "4                       5.3                          -2.18   \n",
       "\n",
       "   es_sider_cracking_med  es_sider_hydroskimming_med  urals_cracking_med  \\\n",
       "0                   4.81                       -0.17                 6.8   \n",
       "1                   4.81                       -0.17                 6.8   \n",
       "2                   4.81                       -0.17                 6.8   \n",
       "3                   4.81                       -0.17                 6.8   \n",
       "4                   4.81                       -0.17                 6.8   \n",
       "\n",
       "   urals_hydroskimming_med  dubai_cracking_singapore  \\\n",
       "0                    -1.52                      2.47   \n",
       "1                    -1.52                      2.47   \n",
       "2                    -1.52                      2.47   \n",
       "3                    -1.52                      2.47   \n",
       "4                    -1.52                      2.47   \n",
       "\n",
       "   dubai_hydroskimming_singapore  tapis_hydroskimming_singapore  \\\n",
       "0                          -3.16                          -1.79   \n",
       "1                          -3.16                          -1.79   \n",
       "2                          -3.16                          -1.79   \n",
       "3                          -3.16                          -1.79   \n",
       "4                          -3.16                          -1.79   \n",
       "\n",
       "   x50_50_hls_lls_cracking_usgc  x30_70_wcs_bakken_cracking_usmc  \\\n",
       "0                         -5.68                             0.38   \n",
       "1                         -5.68                             0.38   \n",
       "2                         -5.68                             0.38   \n",
       "3                         -5.68                             0.38   \n",
       "4                         -5.68                             0.38   \n",
       "\n",
       "   bakken_coking_usmc  \n",
       "0                2.57  \n",
       "1                2.57  \n",
       "2                2.57  \n",
       "3                2.57  \n",
       "4                2.57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import data\n",
    "raw_data = pd.read_csv('__data/refinery_margins.csv')\n",
    "df = raw_data.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Countries: {sorted(df['country'].unique())}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1440 rows\n",
      "Test: 480 rows\n",
      "\n",
      "Observations per country (train): 144\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "split = initial_split(df, prop=0.75, seed=123)\n",
    "train_data = training(split)\n",
    "test_data = testing(split)\n",
    "\n",
    "print(f\"Training: {train_data.shape[0]} rows\")\n",
    "print(f\"Test: {test_data.shape[0]} rows\")\n",
    "print(f\"\\nObservations per country (train): {len(train_data) // train_data['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Define Complex Preprocessing Strategies\n",
    "\n",
    "Create a variety of preprocessing approaches from simple to complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 8 preprocessing strategies defined\n",
      "  1. Minimal formula (2 predictors)\n",
      "  2. All predictors formula\n",
      "  3. Normalization\n",
      "  4. PCA (5 components)\n",
      "  5. Correlation filter + normalization\n",
      "  6. RF feature selection (top 5) + normalization\n",
      "  7. Polynomial features (degree 2)\n",
      "  8. Complex pipeline (impute + select + PCA)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Minimal (baseline)\n",
    "formula_minimal = \"refinery_kbd ~ brent + dubai\"\n",
    "\n",
    "# Strategy 2: All predictors\n",
    "formula_all = \"refinery_kbd ~ .\"\n",
    "\n",
    "# Strategy 3: Normalization\n",
    "rec_normalized = (\n",
    "    recipe()\n",
    "    .step_normalize(all_numeric_predictors())\n",
    ")\n",
    "\n",
    "# Strategy 4: PCA with normalization\n",
    "rec_pca = (\n",
    "    recipe()\n",
    "    .step_normalize(all_numeric_predictors())\n",
    "    .step_pca(all_numeric_predictors(), num_comp=5)\n",
    ")\n",
    "\n",
    "# Strategy 5: Correlation filter + normalization\n",
    "rec_corr = (\n",
    "    recipe()\n",
    "    .step_select_corr(outcome='refinery_kbd', threshold=0.9, method='multicollinearity')\n",
    "    .step_normalize(all_numeric_predictors())\n",
    ")\n",
    "\n",
    "# Strategy 6: Feature selection (RF importance) + normalization\n",
    "rec_rf_select = (\n",
    "    recipe()\n",
    "    .step_normalize(all_numeric_predictors())\n",
    "    .step_filter_rf_importance(outcome='refinery_kbd', top_n=5)\n",
    ")\n",
    "\n",
    "# Strategy 7: Polynomial features (degree 2)\n",
    "rec_poly = (\n",
    "    recipe()\n",
    "    .step_normalize(all_numeric_predictors())\n",
    "    .step_poly(all_numeric_predictors(), degree=2, include_interactions=False, inplace=False)\n",
    ")\n",
    "\n",
    "# Strategy 8: Complex pipeline (imputation + selection + PCA)\n",
    "rec_complex = (\n",
    "    recipe()\n",
    "    .step_impute_median(all_numeric_predictors())\n",
    "    .step_normalize(all_numeric_predictors())\n",
    "    .step_filter_rf_importance(outcome='refinery_kbd', top_n=8)\n",
    "    .step_pca(all_numeric_predictors(), num_comp=4)\n",
    ")\n",
    "\n",
    "print(\"\u2713 8 preprocessing strategies defined\")\n",
    "print(\"  1. Minimal formula (2 predictors)\")\n",
    "print(\"  2. All predictors formula\")\n",
    "print(\"  3. Normalization\")\n",
    "print(\"  4. PCA (5 components)\")\n",
    "print(\"  5. Correlation filter + normalization\")\n",
    "print(\"  6. RF feature selection (top 5) + normalization\")\n",
    "print(\"  7. Polynomial features (degree 2)\")\n",
    "print(\"  8. Complex pipeline (impute + select + PCA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Define Model Specifications\n",
    "\n",
    "Create model specs, some with tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 3 model specifications defined\n",
      "  - Linear Regression\n",
      "  - Random Forest (fixed)\n",
      "  - XGBoost (fixed)\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Linear Regression (no tuning)\n",
    "spec_lr = linear_reg().set_engine(\"sklearn\")\n",
    "\n",
    "# Model 2: Random Forest (fixed params for initial screening)\n",
    "spec_rf_fixed = rand_forest(trees=100, mtry=3, min_n=5).set_mode(\"regression\")\n",
    "\n",
    "# Model 3: XGBoost (fixed params for initial screening)\n",
    "spec_xgb_fixed = boost_tree(trees=100, tree_depth=4, learn_rate=0.1).set_engine(\"xgboost\")\n",
    "\n",
    "print(\"\u2713 3 model specifications defined\")\n",
    "print(\"  - Linear Regression\")\n",
    "print(\"  - Random Forest (fixed)\")\n",
    "print(\"  - XGBoost (fixed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Create WorkflowSet\n",
    "\n",
    "Combine preprocessing strategies with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 workflows from cross product\n",
      "\n",
      "Workflow IDs:\n",
      "  - minimal_linear_reg_1\n",
      "  - minimal_rand_forest_2\n",
      "  - minimal_boost_tree_3\n",
      "  - all_pred_linear_reg_1\n",
      "  - all_pred_rand_forest_2\n",
      "  - all_pred_boost_tree_3\n",
      "  - normalized_linear_reg_1\n",
      "  - normalized_rand_forest_2\n",
      "  - normalized_boost_tree_3\n",
      "  - pca_linear_reg_1\n",
      "  - pca_rand_forest_2\n",
      "  - pca_boost_tree_3\n",
      "  - corr_filter_linear_reg_1\n",
      "  - corr_filter_rand_forest_2\n",
      "  - corr_filter_boost_tree_3\n",
      "  - rf_select_linear_reg_1\n",
      "  - rf_select_rand_forest_2\n",
      "  - rf_select_boost_tree_3\n",
      "  - poly_linear_reg_1\n",
      "  - poly_rand_forest_2\n",
      "  - poly_boost_tree_3\n",
      "  - complex_linear_reg_1\n",
      "  - complex_rand_forest_2\n",
      "  - complex_boost_tree_3\n"
     ]
    }
   ],
   "source": [
    "# Create WorkflowSet (8 preproc \u00d7 3 models = 24 workflows)\n",
    "wf_set = WorkflowSet.from_cross(\n",
    "    preproc=[\n",
    "        formula_minimal,\n",
    "        formula_all,\n",
    "        rec_normalized,\n",
    "        rec_pca,\n",
    "        rec_corr,\n",
    "        rec_rf_select,\n",
    "        rec_poly,\n",
    "        rec_complex\n",
    "    ],\n",
    "    models=[\n",
    "        spec_lr,\n",
    "        spec_rf_fixed,\n",
    "        spec_xgb_fixed\n",
    "    ],\n",
    "    ids=[\"minimal\", \"all_pred\", \"normalized\", \"pca\", \"corr_filter\", \"rf_select\", \"poly\", \"complex\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(wf_set.workflows)} workflows from cross product\")\n",
    "print(\"\\nWorkflow IDs:\")\n",
    "for wf_id in wf_set.workflows.keys():\n",
    "    print(f\"  - {wf_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Workflows Across All Groups\n",
    "\n",
    "**NEW**: Use `fit_nested()` to screen all workflows on all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all workflows across ALL groups\n",
    "print(f\"Evaluating {len(wf_set.workflows)} workflows across {train_data['country'].nunique()} groups...\")\n",
    "print(f\"Total models to fit: {len(wf_set.workflows) * train_data['country'].nunique()}\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "# Use NEW WorkflowSet.fit_nested() method\n",
    "results = wf_set.fit_nested(train_data, group_col='country')\n",
    "\n",
    "print(\"\\n\u2713 All workflows fitted across all groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rank and Select Best Workflow\n",
    "\n",
    "**NEW**: Use group-aware ranking to select best workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank workflows by average RMSE across all groups\n",
    "ranked_overall = results.rank_results('rmse', split='train', by_group=False, n=10)\n",
    "\n",
    "print(\"Top 10 workflows (average across all groups):\")\n",
    "display(ranked_overall)\n",
    "\n",
    "# Get best workflow\n",
    "best_wf_id = results.extract_best_workflow('rmse', split='train', by_group=False)\n",
    "best_wf_mean_rmse = ranked_overall.iloc[0]['mean']\n",
    "\n",
    "print(f\"\\nBest workflow: {best_wf_id}\")\n",
    "print(f\"Mean RMSE across groups: {best_wf_mean_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = results.autoplot('rmse', split='train', by_group=False, top_n=10)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nShows average RMSE \u00b1 std across all groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check per-group preferences\n",
    "best_by_group = results.extract_best_workflow('rmse', split='train', by_group=True)\n",
    "\n",
    "print(\"Best workflow per group:\")\n",
    "display(best_by_group)\n",
    "\n",
    "unique_workflows = best_by_group['wflow_id'].nunique()\n",
    "print(f\"\\nNumber of unique best workflows: {unique_workflows}\")\n",
    "if unique_workflows > 1:\n",
    "    print(\"\u2192 Different groups prefer different workflows (heterogeneous patterns)\")\n",
    "else:\n",
    "    print(\"\u2192 All groups agree on same workflow (homogeneous patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning for Best Workflow\n",
    "\n",
    "Take the best workflow and tune its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessing from best workflow\n",
    "preproc_strategies = {\n",
    "    \"minimal\": formula_minimal,\n",
    "    \"all\": formula_all,\n",
    "    \"normalized\": rec_normalized,\n",
    "    \"pca\": rec_pca,\n",
    "    \"corr\": rec_corr,\n",
    "    \"rf\": rec_rf_select,\n",
    "    \"poly\": rec_poly,\n",
    "    \"complex\": rec_complex\n",
    "}\n",
    "\n",
    "# Determine preprocessing\n",
    "if 'minimal' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['minimal']\n",
    "elif 'all_pred' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['all']\n",
    "elif 'pca' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['pca']\n",
    "elif 'corr' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['corr']\n",
    "elif 'rf_select' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['rf']\n",
    "elif 'poly' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['poly']\n",
    "elif 'complex' in best_wf_id:\n",
    "    best_preproc = preproc_strategies['complex']\n",
    "else:\n",
    "    best_preproc = preproc_strategies['normalized']\n",
    "\n",
    "# Create tunable model\n",
    "if 'rand_forest' in best_wf_id:\n",
    "    spec_tune = rand_forest(\n",
    "        trees=tune('trees'),\n",
    "        mtry=tune('mtry'),\n",
    "        min_n=tune('min_n')\n",
    "    ).set_mode(\"regression\")\n",
    "    \n",
    "    grid = grid_regular({\n",
    "        'trees': {'range': (50, 200), 'trans': 'identity'},\n",
    "        'mtry': {'range': (2, 6), 'trans': 'identity'},\n",
    "        'min_n': {'range': (5, 20), 'trans': 'identity'}\n",
    "    }, levels=3)\n",
    "    \n",
    "elif 'boost_tree' in best_wf_id:\n",
    "    spec_tune = boost_tree(\n",
    "        trees=tune('trees'),\n",
    "        tree_depth=tune('tree_depth'),\n",
    "        learn_rate=tune('learn_rate')\n",
    "    ).set_engine(\"xgboost\")\n",
    "    \n",
    "    grid = grid_regular({\n",
    "        'trees': {'range': (50, 200), 'trans': 'identity'},\n",
    "        'tree_depth': {'range': (3, 8), 'trans': 'identity'},\n",
    "        'learn_rate': {'range': (0.01, 0.3), 'trans': 'log'}\n",
    "    }, levels=3)\n",
    "else:\n",
    "    spec_tune = None\n",
    "    grid = None\n",
    "    print(\"Linear regression - no tuning needed\")\n",
    "\n",
    "if spec_tune is not None:\n",
    "    print(f\"Created tunable workflow with {len(grid)} parameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tuning if applicable\n",
    "if spec_tune is not None:\n",
    "    # Create workflow\n",
    "    if isinstance(best_preproc, str):\n",
    "        wf_tune = workflow().add_formula(best_preproc).add_model(spec_tune)\n",
    "    else:\n",
    "        wf_tune = workflow().add_recipe(best_preproc).add_model(spec_tune)\n",
    "    \n",
    "    # Create CV folds\n",
    "    cv_folds = time_series_cv(\n",
    "        train_data,\n",
    "        date_column='date',\n",
    "        initial='18 months',\n",
    "        assess='3 months',\n",
    "        skip='2 months',\n",
    "        cumulative=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Tuning hyperparameters...\")\n",
    "    print(f\"Grid: {len(grid)} combinations \u00d7 {len(cv_folds.splits)} folds\")\n",
    "    \n",
    "    tune_results = tune_grid(\n",
    "        wf_tune,\n",
    "        resamples=cv_folds,\n",
    "        grid=grid,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u2713 Tuning complete\")\n",
    "    print(\"\\nTop 5 parameter combinations:\")\n",
    "    display(tune_results.show_best(metric='rmse', n=5, maximize=False))\n",
    "    \n",
    "    best_params = tune_results.select_best(metric='rmse', maximize=False)\n",
    "    final_wf = finalize_workflow(wf_tune, best_params)\n",
    "else:\n",
    "    final_wf = wf_set.workflows[best_wf_id]\n",
    "    print(\"Using original workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Apply Optimized Workflow to All Groups\n",
    "\n",
    "Fit the optimized workflow to all countries and evaluate on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to all groups\n",
    "print(f\"Fitting optimized workflow to all {train_data['country'].nunique()} groups...\")\n",
    "\n",
    "fit_nested = final_wf.fit_nested(train_data, group_col='country')\n",
    "fit_nested = fit_nested.evaluate(test_data)\n",
    "\n",
    "print(\"\\n\u2713 Optimized workflow fitted to all groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test performance\n",
    "outputs, coefs, stats = fit_nested.extract_outputs()\n",
    "test_stats = stats[stats['split'] == 'test']\n",
    "\n",
    "# Pivot for display\n",
    "test_stats_pivot = test_stats.pivot_table(\n",
    "    index='group',\n",
    "    columns='metric',\n",
    "    values='value'\n",
    ").reset_index()\n",
    "\n",
    "print(\"Test Performance by Country:\")\n",
    "display(test_stats_pivot[['group', 'rmse', 'mae', 'r_squared']].sort_values('rmse'))\n",
    "\n",
    "print(\"\\nOverall Test Statistics:\")\n",
    "print(f\"Mean RMSE: {test_stats_pivot['rmse'].mean():.4f}\")\n",
    "print(f\"Std RMSE: {test_stats_pivot['rmse'].std():.4f}\")\n",
    "print(f\"Mean R\u00b2: {test_stats_pivot['r_squared'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Visualize forecast and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecasts for all groups\n",
    "fig = plot_forecast(\n",
    "    fit_nested,\n",
    "    title=f\"Optimized {best_wf_id} - All Groups\",\n",
    "    height=1000\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison by country\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "test_stats_sorted = test_stats_pivot.sort_values('rmse')\n",
    "\n",
    "axes[0].barh(test_stats_sorted['group'], test_stats_sorted['rmse'])\n",
    "axes[0].set_xlabel('RMSE')\n",
    "axes[0].set_title('Test RMSE by Country')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(test_stats_sorted['group'], test_stats_sorted['mae'])\n",
    "axes[1].set_xlabel('MAE')\n",
    "axes[1].set_title('Test MAE by Country')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "axes[2].barh(test_stats_sorted['group'], test_stats_sorted['r_squared'])\n",
    "axes[2].set_xlabel('R\u00b2')\n",
    "axes[2].set_title('Test R\u00b2 by Country')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a **complete tidymodels workflow with NEW grouped modeling**:\n",
    "\n",
    "### 1. Comprehensive Model Screening\n",
    "- 8 preprocessing strategies (formulas + complex recipes)\n",
    "- 3 model types (Linear, Random Forest, XGBoost)\n",
    "- 24 total workflows evaluated\n",
    "\n",
    "### 2. NEW Grouped Evaluation\n",
    "- **`fit_nested()`**: Fit all workflows across all groups\n",
    "- **`rank_results()`**: Rank by average performance across groups\n",
    "- **`extract_best_workflow()`**: Select winner based on all groups\n",
    "- **`autoplot()`**: Visualize with error bars\n",
    "\n",
    "### 3. Hyperparameter Optimization\n",
    "- Grid search on best workflow\n",
    "- Time series CV for robust estimates\n",
    "- Final workflow with optimized parameters\n",
    "\n",
    "### 4. Production Deployment\n",
    "- Apply optimized workflow to all groups\n",
    "- Per-country performance analysis\n",
    "- Comprehensive visualization\n",
    "\n",
    "### Key Advantages of NEW API:\n",
    "\n",
    "**Before:**\n",
    "```python\n",
    "# Evaluate on ONE group\n",
    "train_germany = train_data[train_data['country'] == 'Germany']\n",
    "for wf_id, wf in wf_set.workflows.items():\n",
    "    results = fit_resamples(wf, cv_folds, metrics)\n",
    "    # ... manual aggregation ...\n",
    "# Hope Germany patterns generalize\n",
    "```\n",
    "\n",
    "**After:**\n",
    "```python\n",
    "# Evaluate on ALL groups\n",
    "results = wf_set.fit_nested(train_data, group_col='country')\n",
    "best_wf_id = results.extract_best_workflow('rmse')\n",
    "# Selection based on ALL groups\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- **More robust**: Selection based on all groups, not single group\n",
    "- **Detects heterogeneity**: Identifies if groups need different workflows\n",
    "- **Simpler code**: Single method call vs manual loops\n",
    "- **Better generalization**: Average performance across diverse patterns\n",
    "\n",
    "### Next Steps:\n",
    "- Try `per_group_prep=True` for group-specific preprocessing\n",
    "- Experiment with `fit_global()` for comparison\n",
    "- Add ensemble methods combining top workflows\n",
    "- Implement Bayesian optimization for hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}