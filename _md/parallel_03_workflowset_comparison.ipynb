{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing Demo 3: WorkflowSet Multi-Model Comparison\n",
    "\n",
    "This notebook demonstrates parallel execution for multi-model comparison:\n",
    "- `WorkflowSet.from_cross()` - Create all preprocessing √ó model combinations\n",
    "- `WorkflowSet.fit_resamples()` - Parallel evaluation across CV folds\n",
    "- Progress tracking with `verbose=True`\n",
    "- Results ranking and visualization\n",
    "- Performance comparisons (sequential vs parallel)\n",
    "\n",
    "**Key Features Demonstrated:**\n",
    "- ‚úÖ `n_jobs` parameter for WorkflowSet evaluation\n",
    "- ‚úÖ CPU warning system for multi-workflow tasks\n",
    "- ‚úÖ Automatic workflow ranking\n",
    "- ‚úÖ Speedup measurements for large workflow sets\n",
    "- ‚úÖ Best model selection and finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory and install\n",
    "import os\n",
    "os.chdir('..')\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_recipes import recipe, all_numeric_predictors\n",
    "from py_rsample import vfold_cv, initial_split, training, testing\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_tune.parallel_utils import get_cpu_count\n",
    "\n",
    "# Seaborn styling\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('__data/preem.csv')\n",
    "df = raw_data.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "split = initial_split(df, prop=0.75, seed=123)\n",
    "train_data = training(split)\n",
    "test_data = testing(split)\n",
    "\n",
    "print(f\"Training set: {train_data.shape[0]} rows\")\n",
    "print(f\"Test set: {test_data.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base formula and metrics\n",
    "FORMULA = \"target ~ .\"\n",
    "metrics = metric_set(rmse, mae, r_squared)\n",
    "\n",
    "print(f\"Formula: {FORMULA}\")\n",
    "print(f\"Metrics: rmse, mae, r_squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "cpu_count = get_cpu_count()\n",
    "print(f\"‚úì Detected {cpu_count} CPU cores\")\n",
    "print(f\"‚úì Joblib backend: loky (multiprocessing)\")\n",
    "print(f\"\\nThis system can efficiently run up to {cpu_count} parallel jobs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create WorkflowSet\n",
    "\n",
    "We'll create multiple preprocessing strategies and models, then combine them into a WorkflowSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing strategies\n",
    "formulas = [\n",
    "    \"target ~ .\",  # All features\n",
    "]\n",
    "\n",
    "recipes = [\n",
    "    recipe().step_normalize(all_numeric_predictors()),\n",
    "    recipe().step_normalize(all_numeric_predictors()).step_pca(all_numeric_predictors(), num_comp=3),\n",
    "    recipe().step_poly(all_numeric_predictors(), degree=2)\n",
    "]\n",
    "\n",
    "# Combine formulas and recipes\n",
    "preproc = formulas + recipes\n",
    "\n",
    "print(f\"Preprocessing strategies: {len(preproc)}\")\n",
    "print(\"  1. Formula (minimal): target ~ .\")\n",
    "print(\"  2. Recipe (normalized): Normalize all numeric\")\n",
    "print(\"  3. Recipe (PCA): Normalize + PCA (3 components)\")\n",
    "print(\"  4. Recipe (polynomial): Polynomial features (degree 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [\n",
    "    linear_reg(),\n",
    "    linear_reg(penalty=0.1, mixture=1.0).set_engine(\"sklearn\"),  # Lasso\n",
    "    rand_forest(trees=100, min_n=5).set_mode('regression')\n",
    "]\n",
    "\n",
    "print(f\"Models: {len(models)}\")\n",
    "print(\"  1. Linear Regression (OLS)\")\n",
    "print(\"  2. Linear Regression (Lasso, penalty=0.1)\")\n",
    "print(\"  3. Random Forest (100 trees, min_n=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WorkflowSet from all combinations\n",
    "wf_set = WorkflowSet.from_cross(preproc=preproc, models=models)\n",
    "\n",
    "n_workflows = len(wf_set.workflows)\n",
    "print(f\"\\n‚úì Created WorkflowSet with {n_workflows} workflows\")\n",
    "print(f\"  ({len(preproc)} preprocessing strategies √ó {len(models)} models)\")\n",
    "\n",
    "# Show workflow IDs\n",
    "print(\"\\nWorkflow IDs:\")\n",
    "for wf_id in wf_set.workflows.keys():\n",
    "    print(f\"  - {wf_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sequential vs Parallel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CV folds\n",
    "folds = vfold_cv(train_data, v=5, seed=123)\n",
    "n_folds = len(folds)\n",
    "total_fits = n_workflows * n_folds\n",
    "\n",
    "print(f\"CV Configuration:\")\n",
    "print(f\"  Workflows: {n_workflows}\")\n",
    "print(f\"  CV folds: {n_folds}\")\n",
    "print(f\"  Total fits: {total_fits} ({n_workflows} √ó {n_folds})\")\n",
    "print(f\"\\nThis is a good candidate for parallel execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Execution (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential evaluation\n",
    "print(f\"Running SEQUENTIAL WorkflowSet evaluation ({total_fits} fits)...\")\n",
    "start = time.time()\n",
    "results_seq = wf_set.fit_resamples(\n",
    "    resamples=folds,\n",
    "    metrics=metrics,\n",
    "    n_jobs=1,  # Sequential\n",
    "    verbose=True\n",
    ")\n",
    "seq_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Sequential execution completed in {seq_time:.2f} seconds\")\n",
    "print(f\"  ({seq_time / total_fits:.2f} seconds per fit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sequential results\n",
    "metrics_seq = results_seq.collect_metrics()\n",
    "print(\"\\nTop 5 workflows (sequential, by RMSE):\")\n",
    "display(results_seq.rank_results('rmse', n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution with All Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel evaluation\n",
    "print(f\"Running PARALLEL WorkflowSet evaluation ({total_fits} fits, n_jobs=-1)...\")\n",
    "start = time.time()\n",
    "results_par = wf_set.fit_resamples(\n",
    "    resamples=folds,\n",
    "    metrics=metrics,\n",
    "    n_jobs=-1,  # Use all cores\n",
    "    verbose=True\n",
    ")\n",
    "par_time = time.time() - start\n",
    "\n",
    "speedup = seq_time / par_time\n",
    "efficiency = (speedup / cpu_count) * 100\n",
    "\n",
    "print(f\"\\n‚úì Parallel execution completed in {par_time:.2f} seconds\")\n",
    "print(f\"  ({par_time / total_fits:.2f} seconds per fit)\")\n",
    "print(f\"‚úì Speedup: {speedup:.2f}x\")\n",
    "print(f\"‚úì Efficiency: {efficiency:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results are identical\n",
    "metrics_par = results_par.collect_metrics()\n",
    "\n",
    "print(\"Consistency Check (top workflow):\")\n",
    "top_wf_id = results_seq.rank_results('rmse', n=1).iloc[0]['wflow_id']\n",
    "\n",
    "seq_metrics = metrics_seq[metrics_seq['wflow_id'] == top_wf_id]\n",
    "par_metrics = metrics_par[metrics_par['wflow_id'] == top_wf_id]\n",
    "\n",
    "for metric in ['rmse', 'mae', 'r_squared']:\n",
    "    seq_val = seq_metrics[seq_metrics['metric'] == metric]['mean'].values[0]\n",
    "    par_val = par_metrics[par_metrics['metric'] == metric]['mean'].values[0]\n",
    "    \n",
    "    match = np.isclose(seq_val, par_val, rtol=1e-10)\n",
    "    status = \"‚úì IDENTICAL\" if match else \"‚úó DIFFERENT\"\n",
    "    print(f\"  {metric}: {status} ({seq_val:.6f})\")\n",
    "\n",
    "print(\"\\n‚úì All parallel executions produce identical results to sequential!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "perf_df = pd.DataFrame({\n",
    "    'Configuration': ['Sequential', f'Parallel ({cpu_count} cores)'],\n",
    "    'n_jobs': [1, -1],\n",
    "    'Time (s)': [seq_time, par_time],\n",
    "    'Time per fit (s)': [seq_time / total_fits, par_time / total_fits],\n",
    "    'Speedup': [1.0, speedup],\n",
    "    'Efficiency (%)': [100.0, efficiency]\n",
    "})\n",
    "\n",
    "display(perf_df)\n",
    "\n",
    "# Plot performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total time comparison\n",
    "ax1.bar(perf_df['Configuration'], perf_df['Time (s)'], color=['gray', 'green'])\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title(f'WorkflowSet Evaluation Time ({n_workflows} workflows √ó {n_folds} folds)')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "ax2.bar(perf_df['Configuration'], perf_df['Speedup'], color=['gray', 'green'])\n",
    "ax2.set_ylabel('Speedup (x)')\n",
    "ax2.set_title('Speedup vs Sequential')\n",
    "ax2.axhline(y=1, color='r', linestyle='--', label='Baseline')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Results Analysis and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank all workflows\n",
    "ranked = results_par.rank_results('rmse', n=n_workflows)\n",
    "\n",
    "print(f\"All {n_workflows} workflows ranked by RMSE:\")\n",
    "display(ranked[['wflow_id', 'rmse_mean', 'mae_mean', 'r_squared_mean', 'rank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize workflow comparison\n",
    "fig = results_par.autoplot('rmse')\n",
    "fig.update_layout(\n",
    "    title=f'WorkflowSet Performance Comparison ({n_workflows} workflows)',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics across workflows\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics_to_plot = ['rmse', 'mae', 'r_squared']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    metric_data = metrics_par[metrics_par['metric'] == metric].sort_values('mean')\n",
    "    \n",
    "    axes[i].barh(range(len(metric_data)), metric_data['mean'], color=color, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(metric_data)))\n",
    "    axes[i].set_yticklabels(metric_data['wflow_id'], fontsize=8)\n",
    "    axes[i].set_xlabel(metric.upper())\n",
    "    axes[i].set_title(f'{metric.upper()} by Workflow')\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Highlight best\n",
    "    if metric == 'r_squared':\n",
    "        best_idx = metric_data['mean'].idxmax()\n",
    "    else:\n",
    "        best_idx = metric_data['mean'].idxmin()\n",
    "    \n",
    "    best_pos = list(metric_data.index).index(best_idx)\n",
    "    axes[i].barh(best_pos, metric_data.loc[best_idx, 'mean'], color='gold', alpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Best Workflow Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best workflow\n",
    "best_wf_id = ranked.iloc[0]['wflow_id']\n",
    "best_workflow = wf_set[best_wf_id]\n",
    "\n",
    "print(f\"Best workflow: {best_wf_id}\")\n",
    "print(f\"\\nPerformance metrics:\")\n",
    "best_metrics = ranked.iloc[0]\n",
    "print(f\"  RMSE: {best_metrics['rmse']:.4f} (¬±{best_metrics.get('rmse_std', 0):.4f})\")\n",
    "print(f\"  MAE:  {best_metrics['mae']:.4f} (¬±{best_metrics.get('mae_std', 0):.4f})\")\n",
    "print(f\"  R¬≤:   {best_metrics['r_squared']:.4f} (¬±{best_metrics.get('r_squared_std', 0):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best workflow on full training set\n",
    "print(f\"\\nFitting best workflow ({best_wf_id}) on full training data...\")\n",
    "best_fit = best_workflow.fit(train_data)\n",
    "print(\"‚úì Fit complete\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_fit = best_fit.evaluate(test_data)\n",
    "\n",
    "# Calculate metrics\n",
    "predictions = best_fit.predict(test_data)\n",
    "test_metrics_df = pd.DataFrame()\n",
    "for metric_fn in [rmse, mae, r_squared]:\n",
    "    metric_result = metric_fn(test_data['target'], predictions['.pred'])\n",
    "    test_metrics_df = pd.concat([test_metrics_df, metric_result], ignore_index=True)\n",
    "\n",
    "print(\"\\nTest set performance:\")\n",
    "display(test_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: CPU Warning Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller WorkflowSet to trigger warning\n",
    "small_wf_set = WorkflowSet.from_cross(\n",
    "    preproc=[\"target ~ .\"],\n",
    "    models=[linear_reg()]\n",
    ")\n",
    "\n",
    "print(f\"Small WorkflowSet has {len(small_wf_set.workflows)} workflow.\")\n",
    "print(f\"Requesting {cpu_count * 2} workers...\\n\")\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    \n",
    "    results_warn = small_wf_set.fit_resamples(\n",
    "        resamples=folds,\n",
    "        metrics=metrics,\n",
    "        n_jobs=cpu_count * 2,  # More workers than tasks\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if w:\n",
    "        print(\"‚ö†Ô∏è  WARNING TRIGGERED:\")\n",
    "        print(f\"    {w[0].message}\")\n",
    "        print(f\"\\nüí° Recommendation: For 1 workflow √ó 5 folds, use n_jobs=5 or fewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PARALLEL WORKFLOWSET PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSystem: {cpu_count} CPU cores\")\n",
    "print(f\"WorkflowSet: {n_workflows} workflows ({len(preproc)} prep √ó {len(models)} models)\")\n",
    "print(f\"CV: {n_folds} folds\")\n",
    "print(f\"Total fits: {total_fits}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Sequential: {seq_time:.2f}s ({seq_time/total_fits:.3f}s per fit)\")\n",
    "print(f\"  Parallel ({cpu_count} cores): {par_time:.2f}s ({par_time/total_fits:.3f}s per fit)\")\n",
    "print(f\"  Speedup: {speedup:.2f}x\")\n",
    "print(f\"  Efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "print(f\"\\nBest workflow: {best_wf_id}\")\n",
    "print(f\"  CV RMSE: {best_metrics['rmse']:.4f}\")\n",
    "test_rmse = test_metrics_df[test_metrics_df['metric'] == 'rmse']['value'].values[0]\n",
    "print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Use parallel WorkflowSet (n_jobs=-1) when:\")\n",
    "print(f\"   - Comparing many workflows (>{cpu_count})\")\n",
    "print(f\"   - Each workflow takes >1 second per fold\")\n",
    "print(f\"   - Total fits > 20-30 (workflows √ó folds)\")\n",
    "print(f\"   - Using complex preprocessing (recipes with PCA, polynomial features)\")\n",
    "print(f\"   - Using complex models (random forest, boosting)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Use sequential (n_jobs=1) when:\")\n",
    "print(f\"   - Few workflows (< {cpu_count})\")\n",
    "print(f\"   - Simple/fast workflows\")\n",
    "print(f\"   - Total execution time < 30 seconds\")\n",
    "print(f\"   - Debugging workflow issues\")\n",
    "\n",
    "print(f\"\\nüí° Tips:\")\n",
    "print(f\"   - Always use verbose=True to monitor progress\")\n",
    "print(f\"   - Watch for CPU warnings - they help optimize performance\")\n",
    "print(f\"   - Use rank_results() to identify best workflows\")\n",
    "print(f\"   - Use autoplot() for quick visual comparison\")\n",
    "print(f\"   - Parallel speedup scales with number of workflows and fold complexity\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
