{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing Demo 4: Performance Benchmarking & Best Practices\n",
    "\n",
    "This notebook provides comprehensive benchmarking and guidelines for parallel processing:\n",
    "- Systematic performance benchmarks for all parallel methods\n",
    "- Scalability analysis (varying n_jobs)\n",
    "- Task size impact on speedup\n",
    "- Overhead analysis\n",
    "- Best practices and decision framework\n",
    "\n",
    "**Key Features Demonstrated:**\n",
    "- ✅ Benchmark all 7 parallelized methods\n",
    "- ✅ Scalability curves (n_jobs vs speedup)\n",
    "- ✅ Efficiency analysis\n",
    "- ✅ Optimal n_jobs recommendations\n",
    "- ✅ Practical decision framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "# py-tidymodels imports\n",
    "from py_workflows import workflow\n",
    "from py_parsnip import linear_reg, rand_forest\n",
    "from py_recipes import recipe, step_normalize, all_numeric_predictors\n",
    "from py_rsample import vfold_cv, initial_split, training, testing\n",
    "from py_yardstick import metric_set, rmse, mae, r_squared\n",
    "from py_tune import fit_resamples, tune_grid, grid_regular, tune\n",
    "from py_workflowsets import WorkflowSet\n",
    "from py_tune.parallel_utils import get_cpu_count\n",
    "\n",
    "# Seaborn styling\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('__data/preem.csv')\n",
    "df = raw_data.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create train/test split\n",
    "split = initial_split(df, prop=0.75, seed=123)\n",
    "train_data = training(split)\n",
    "test_data = testing(split)\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Training set: {train_data.shape[0]} rows\")\n",
    "print(f\"Test set: {test_data.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System information\n",
    "cpu_count = get_cpu_count()\n",
    "print(f\"\\n✓ Detected {cpu_count} CPU cores\")\n",
    "print(f\"✓ Joblib backend: loky (multiprocessing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable components\n",
    "FORMULA = \"target ~ .\"\n",
    "metrics = metric_set(rmse, mae)\n",
    "\n",
    "print(f\"Formula: {FORMULA}\")\n",
    "print(f\"Metrics: rmse, mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_method(\n",
    "    method_fn: Callable,\n",
    "    *args,\n",
    "    n_jobs_values: list = [1, 2, -1],\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark a method across different n_jobs values.\n",
    "    \n",
    "    Returns DataFrame with columns: n_jobs, time, speedup, efficiency\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    baseline_time = None\n",
    "    \n",
    "    for n_jobs in n_jobs_values:\n",
    "        # Resolve -1 to actual CPU count\n",
    "        actual_jobs = cpu_count if n_jobs == -1 else n_jobs\n",
    "        \n",
    "        # Run benchmark\n",
    "        start = time.time()\n",
    "        _ = method_fn(*args, **kwargs, n_jobs=n_jobs, verbose=False)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Track baseline\n",
    "        if n_jobs == 1:\n",
    "            baseline_time = elapsed\n",
    "        \n",
    "        # Calculate metrics\n",
    "        speedup = baseline_time / elapsed if baseline_time else 1.0\n",
    "        efficiency = (speedup / actual_jobs * 100) if actual_jobs > 1 else 100.0\n",
    "        \n",
    "        results.append({\n",
    "            'n_jobs': n_jobs,\n",
    "            'actual_cores': actual_jobs,\n",
    "            'time': elapsed,\n",
    "            'speedup': speedup,\n",
    "            'efficiency': efficiency\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Benchmark utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit_resamples() Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for fit_resamples\n",
    "wf_simple = workflow().add_formula(FORMULA).add_model(linear_reg())\n",
    "folds_5 = vfold_cv(train_data, v=5, seed=123)\n",
    "folds_10 = vfold_cv(train_data, v=10, seed=123)\n",
    "\n",
    "print(\"Benchmarking fit_resamples()...\")\n",
    "print(\"  Scenario 1: 5-fold CV\")\n",
    "print(\"  Scenario 2: 10-fold CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 5-fold CV\n",
    "bench_resample_5 = benchmark_method(\n",
    "    fit_resamples,\n",
    "    wf_simple, folds_5,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_resample_5['scenario'] = '5-fold CV'\n",
    "\n",
    "print(\"\\n5-fold CV results:\")\n",
    "display(bench_resample_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 10-fold CV\n",
    "bench_resample_10 = benchmark_method(\n",
    "    fit_resamples,\n",
    "    wf_simple, folds_10,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_resample_10['scenario'] = '10-fold CV'\n",
    "\n",
    "print(\"\\n10-fold CV results:\")\n",
    "display(bench_resample_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and visualize\n",
    "bench_resample = pd.concat([bench_resample_5, bench_resample_10], ignore_index=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup comparison\n",
    "for scenario in ['5-fold CV', '10-fold CV']:\n",
    "    data = bench_resample[bench_resample['scenario'] == scenario]\n",
    "    ax1.plot(data['actual_cores'], data['speedup'], marker='o', label=scenario, linewidth=2)\n",
    "\n",
    "ax1.plot([1, cpu_count], [1, cpu_count], 'k--', alpha=0.3, label='Ideal (linear)')\n",
    "ax1.set_xlabel('Number of Cores')\n",
    "ax1.set_ylabel('Speedup (x)')\n",
    "ax1.set_title('fit_resamples() Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Efficiency comparison\n",
    "for scenario in ['5-fold CV', '10-fold CV']:\n",
    "    data = bench_resample[bench_resample['scenario'] == scenario]\n",
    "    ax2.plot(data['actual_cores'], data['efficiency'], marker='o', label=scenario, linewidth=2)\n",
    "\n",
    "ax2.axhline(y=100, color='k', linestyle='--', alpha=0.3, label='Ideal (100%)')\n",
    "ax2.set_xlabel('Number of Cores')\n",
    "ax2.set_ylabel('Efficiency (%)')\n",
    "ax2.set_title('fit_resamples() Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: tune_grid() Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for tune_grid\n",
    "spec_tune = linear_reg(penalty=tune(), mixture=tune()).set_engine(\"sklearn\")\n",
    "wf_tune = workflow().add_formula(FORMULA).add_model(spec_tune)\n",
    "\n",
    "param_info = {\n",
    "    'penalty': {'range': (0.001, 1.0), 'trans': 'log'},\n",
    "    'mixture': {'range': (0, 1)}\n",
    "}\n",
    "\n",
    "# Small grid (9 configs)\n",
    "grid_small = grid_regular(param_info, levels=3)\n",
    "# Medium grid (25 configs)\n",
    "grid_medium = grid_regular(param_info, levels=5)\n",
    "\n",
    "print(\"Benchmarking tune_grid()...\")\n",
    "print(f\"  Scenario 1: 9 configs × 5 folds = {len(grid_small) * 5} fits\")\n",
    "print(f\"  Scenario 2: 25 configs × 5 folds = {len(grid_medium) * 5} fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark small grid\n",
    "bench_tune_small = benchmark_method(\n",
    "    tune_grid,\n",
    "    wf_tune, folds_5,\n",
    "    grid=grid_small,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_tune_small['scenario'] = '9 configs'\n",
    "\n",
    "print(\"\\n9 configs × 5 folds results:\")\n",
    "display(bench_tune_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark medium grid\n",
    "bench_tune_medium = benchmark_method(\n",
    "    tune_grid,\n",
    "    wf_tune, folds_5,\n",
    "    grid=grid_medium,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_tune_medium['scenario'] = '25 configs'\n",
    "\n",
    "print(\"\\n25 configs × 5 folds results:\")\n",
    "display(bench_tune_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and visualize\n",
    "bench_tune = pd.concat([bench_tune_small, bench_tune_medium], ignore_index=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup comparison\n",
    "for scenario in ['9 configs', '25 configs']:\n",
    "    data = bench_tune[bench_tune['scenario'] == scenario]\n",
    "    ax1.plot(data['actual_cores'], data['speedup'], marker='s', label=scenario, linewidth=2)\n",
    "\n",
    "ax1.plot([1, cpu_count], [1, cpu_count], 'k--', alpha=0.3, label='Ideal (linear)')\n",
    "ax1.set_xlabel('Number of Cores')\n",
    "ax1.set_ylabel('Speedup (x)')\n",
    "ax1.set_title('tune_grid() Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Time per fit comparison\n",
    "for scenario in ['9 configs', '25 configs']:\n",
    "    data = bench_tune[bench_tune['scenario'] == scenario]\n",
    "    n_fits = 45 if scenario == '9 configs' else 125\n",
    "    data['time_per_fit'] = data['time'] / n_fits\n",
    "    ax2.plot(data['actual_cores'], data['time_per_fit'], marker='s', label=scenario, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Number of Cores')\n",
    "ax2.set_ylabel('Time per Fit (s)')\n",
    "ax2.set_title('tune_grid() Time per Fit')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Workflow.fit_nested() Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped data\n",
    "df_grouped = train_data.copy()\n",
    "n_groups_4 = 4\n",
    "n_groups_8 = 8\n",
    "\n",
    "regions_4 = ['North', 'South', 'East', 'West']\n",
    "regions_8 = ['N', 'S', 'E', 'W', 'NE', 'NW', 'SE', 'SW']\n",
    "\n",
    "df_grouped['group_4'] = [regions_4[i % n_groups_4] for i in range(len(df_grouped))]\n",
    "df_grouped['group_8'] = [regions_8[i % n_groups_8] for i in range(len(df_grouped))]\n",
    "\n",
    "wf_nested = workflow().add_formula(FORMULA).add_model(linear_reg())\n",
    "\n",
    "print(\"Benchmarking fit_nested()...\")\n",
    "print(f\"  Scenario 1: {n_groups_4} groups\")\n",
    "print(f\"  Scenario 2: {n_groups_8} groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 4 groups\n",
    "bench_nested_4 = benchmark_method(\n",
    "    wf_nested.fit_nested,\n",
    "    df_grouped,\n",
    "    group_col='group_4',\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_nested_4['scenario'] = '4 groups'\n",
    "\n",
    "print(\"\\n4 groups results:\")\n",
    "display(bench_nested_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 8 groups\n",
    "bench_nested_8 = benchmark_method(\n",
    "    wf_nested.fit_nested,\n",
    "    df_grouped,\n",
    "    group_col='group_8',\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_nested_8['scenario'] = '8 groups'\n",
    "\n",
    "print(\"\\n8 groups results:\")\n",
    "display(bench_nested_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and visualize\n",
    "bench_nested = pd.concat([bench_nested_4, bench_nested_8], ignore_index=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup comparison\n",
    "for scenario in ['4 groups', '8 groups']:\n",
    "    data = bench_nested[bench_nested['scenario'] == scenario]\n",
    "    ax1.plot(data['actual_cores'], data['speedup'], marker='^', label=scenario, linewidth=2)\n",
    "\n",
    "ax1.plot([1, cpu_count], [1, cpu_count], 'k--', alpha=0.3, label='Ideal (linear)')\n",
    "ax1.set_xlabel('Number of Cores')\n",
    "ax1.set_ylabel('Speedup (x)')\n",
    "ax1.set_title('fit_nested() Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Efficiency comparison\n",
    "for scenario in ['4 groups', '8 groups']:\n",
    "    data = bench_nested[bench_nested['scenario'] == scenario]\n",
    "    ax2.plot(data['actual_cores'], data['efficiency'], marker='^', label=scenario, linewidth=2)\n",
    "\n",
    "ax2.axhline(y=100, color='k', linestyle='--', alpha=0.3, label='Ideal (100%)')\n",
    "ax2.set_xlabel('Number of Cores')\n",
    "ax2.set_ylabel('Efficiency (%)')\n",
    "ax2.set_title('fit_nested() Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: WorkflowSet Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small and large WorkflowSets\n",
    "formulas = [\"target ~ .\"]\n",
    "recipes_list = [\n",
    "    recipe(train_data, FORMULA).step_normalize(all_numeric_predictors()),\n",
    "    recipe(train_data, FORMULA).step_normalize(all_numeric_predictors()).step_pca(all_numeric_predictors(), num_comp=3)\n",
    "]\n",
    "models = [linear_reg(), rand_forest(trees=50).set_mode('regression')]\n",
    "\n",
    "# Small: 1 formula × 2 models = 2 workflows\n",
    "wf_set_small = WorkflowSet.from_cross(preproc=formulas, models=models)\n",
    "\n",
    "# Large: 3 preprocessing × 2 models = 6 workflows\n",
    "wf_set_large = WorkflowSet.from_cross(preproc=formulas + recipes_list, models=models)\n",
    "\n",
    "print(\"Benchmarking WorkflowSet.fit_resamples()...\")\n",
    "print(f\"  Scenario 1: {len(wf_set_small.workflows)} workflows × 5 folds = {len(wf_set_small.workflows) * 5} fits\")\n",
    "print(f\"  Scenario 2: {len(wf_set_large.workflows)} workflows × 5 folds = {len(wf_set_large.workflows) * 5} fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark small WorkflowSet\n",
    "bench_wfset_small = benchmark_method(\n",
    "    wf_set_small.fit_resamples,\n",
    "    resamples=folds_5,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_wfset_small['scenario'] = '2 workflows'\n",
    "\n",
    "print(\"\\n2 workflows × 5 folds results:\")\n",
    "display(bench_wfset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark large WorkflowSet\n",
    "bench_wfset_large = benchmark_method(\n",
    "    wf_set_large.fit_resamples,\n",
    "    resamples=folds_5,\n",
    "    metrics=metrics,\n",
    "    n_jobs_values=[1, 2, -1]\n",
    ")\n",
    "bench_wfset_large['scenario'] = '6 workflows'\n",
    "\n",
    "print(\"\\n6 workflows × 5 folds results:\")\n",
    "display(bench_wfset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and visualize\n",
    "bench_wfset = pd.concat([bench_wfset_small, bench_wfset_large], ignore_index=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup comparison\n",
    "for scenario in ['2 workflows', '6 workflows']:\n",
    "    data = bench_wfset[bench_wfset['scenario'] == scenario]\n",
    "    ax1.plot(data['actual_cores'], data['speedup'], marker='D', label=scenario, linewidth=2)\n",
    "\n",
    "ax1.plot([1, cpu_count], [1, cpu_count], 'k--', alpha=0.3, label='Ideal (linear)')\n",
    "ax1.set_xlabel('Number of Cores')\n",
    "ax1.set_ylabel('Speedup (x)')\n",
    "ax1.set_title('WorkflowSet.fit_resamples() Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Time comparison\n",
    "for scenario in ['2 workflows', '6 workflows']:\n",
    "    data = bench_wfset[bench_wfset['scenario'] == scenario]\n",
    "    ax2.plot(data['actual_cores'], data['time'], marker='D', label=scenario, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Number of Cores')\n",
    "ax2.set_ylabel('Time (s)')\n",
    "ax2.set_title('WorkflowSet.fit_resamples() Time')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parallel (all cores) results for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for bench, method in [\n",
    "    (bench_resample_5, 'fit_resamples (5-fold)'),\n",
    "    (bench_resample_10, 'fit_resamples (10-fold)'),\n",
    "    (bench_tune_small, 'tune_grid (9 configs)'),\n",
    "    (bench_tune_medium, 'tune_grid (25 configs)'),\n",
    "    (bench_nested_4, 'fit_nested (4 groups)'),\n",
    "    (bench_nested_8, 'fit_nested (8 groups)'),\n",
    "    (bench_wfset_small, 'WorkflowSet (2 wf)'),\n",
    "    (bench_wfset_large, 'WorkflowSet (6 wf)')\n",
    "]:\n",
    "    parallel_row = bench[bench['n_jobs'] == -1].iloc[0]\n",
    "    comparison_data.append({\n",
    "        'Method': method,\n",
    "        'Sequential Time (s)': bench[bench['n_jobs'] == 1].iloc[0]['time'],\n",
    "        'Parallel Time (s)': parallel_row['time'],\n",
    "        'Speedup': parallel_row['speedup'],\n",
    "        'Efficiency (%)': parallel_row['efficiency']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(f\"\\nComprehensive Parallel Performance Summary (n_jobs=-1, {cpu_count} cores):\")\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comprehensive comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Speedup by method\n",
    "axes[0, 0].barh(comparison_df['Method'], comparison_df['Speedup'], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axvline(x=1, color='r', linestyle='--', label='Baseline')\n",
    "axes[0, 0].set_xlabel('Speedup (x)')\n",
    "axes[0, 0].set_title('Speedup by Method')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Efficiency by method\n",
    "axes[0, 1].barh(comparison_df['Method'], comparison_df['Efficiency (%)'], color='coral', alpha=0.7)\n",
    "axes[0, 1].axvline(x=100, color='r', linestyle='--', label='Ideal (100%)')\n",
    "axes[0, 1].set_xlabel('Efficiency (%)')\n",
    "axes[0, 1].set_title('Efficiency by Method')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Sequential vs Parallel time\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, comparison_df['Sequential Time (s)'], width, label='Sequential', alpha=0.7)\n",
    "axes[1, 0].bar(x + width/2, comparison_df['Parallel Time (s)'], width, label='Parallel', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('Time (s)')\n",
    "axes[1, 0].set_title('Sequential vs Parallel Time')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(range(1, len(comparison_df) + 1))\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Speedup vs Efficiency scatter\n",
    "axes[1, 1].scatter(comparison_df['Speedup'], comparison_df['Efficiency (%)'], \n",
    "                   s=100, c=range(len(comparison_df)), cmap='viridis', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Speedup (x)')\n",
    "axes[1, 1].set_ylabel('Efficiency (%)')\n",
    "axes[1, 1].set_title('Speedup vs Efficiency')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# Add method labels to scatter\n",
    "for i, txt in enumerate(comparison_df['Method']):\n",
    "    axes[1, 1].annotate(i+1, \n",
    "                       (comparison_df['Speedup'].iloc[i], comparison_df['Efficiency (%)'].iloc[i]),\n",
    "                       fontsize=9, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print legend\n",
    "print(\"\\nMethod Index:\")\n",
    "for i, method in enumerate(comparison_df['Method'], 1):\n",
    "    print(f\"  {i}. {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Best Practices and Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key insights\n",
    "avg_speedup = comparison_df['Speedup'].mean()\n",
    "avg_efficiency = comparison_df['Efficiency (%)'].mean()\n",
    "best_speedup_method = comparison_df.loc[comparison_df['Speedup'].idxmax(), 'Method']\n",
    "best_efficiency_method = comparison_df.loc[comparison_df['Efficiency (%)'].idxmax(), 'Method']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARALLEL PROCESSING BENCHMARKING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSystem: {cpu_count} CPU cores\")\n",
    "print(f\"\\nKey Performance Metrics (n_jobs=-1):\")\n",
    "print(f\"  Average Speedup: {avg_speedup:.2f}x\")\n",
    "print(f\"  Average Efficiency: {avg_efficiency:.1f}%\")\n",
    "print(f\"  Best Speedup: {best_speedup_method} ({comparison_df['Speedup'].max():.2f}x)\")\n",
    "print(f\"  Best Efficiency: {best_efficiency_method} ({comparison_df['Efficiency (%)'].max():.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DECISION FRAMEWORK: WHEN TO USE PARALLEL PROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ HIGHLY RECOMMENDED (Expected speedup > 2x):\")\n",
    "print(\"   - WorkflowSet with many workflows (>4)\")\n",
    "print(\"   - Grid search with many configs (>20)\")\n",
    "print(\"   - Nested modeling with many groups (>6)\")\n",
    "print(\"   - CV with many folds (>8)\")\n",
    "print(\"   - Complex models (random forest, boosting)\")\n",
    "\n",
    "print(\"\\n⚠️  CONDITIONALLY RECOMMENDED (Expected speedup 1.5-2x):\")\n",
    "print(\"   - Moderate number of tasks (4-8)\")\n",
    "print(\"   - Medium CV (5-8 folds)\")\n",
    "print(\"   - Small WorkflowSet (2-4 workflows)\")\n",
    "print(\"   - Simple models with moderate data\")\n",
    "\n",
    "print(\"\\n❌ NOT RECOMMENDED (Overhead > benefit):\")\n",
    "print(\"   - Few tasks (< number of cores)\")\n",
    "print(\"   - Very simple/fast models (<1s per fit)\")\n",
    "print(\"   - Total execution time <10 seconds\")\n",
    "print(\"   - Debugging/development phase\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMAL n_jobs SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. For most cases: n_jobs=-1 (use all cores)\")\n",
    "print(\"2. For shared machines: n_jobs=cpu_count-1 (leave 1 core free)\")\n",
    "print(\"3. For few tasks: n_jobs=min(task_count, cpu_count)\")\n",
    "print(\"4. For debugging: n_jobs=1 (easier error tracing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE TIPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Monitor with verbose=True to see progress\")\n",
    "print(\"2. Watch for CPU warnings - they help optimize\")\n",
    "print(\"3. Larger task counts → better parallel efficiency\")\n",
    "print(\"4. Complex models benefit more from parallelization\")\n",
    "print(\"5. Avoid nested parallelism (outer method controls n_jobs)\")\n",
    "print(\"6. On Windows: 'loky' backend works seamlessly\")\n",
    "print(\"7. Overhead ~0.5-1s per worker - factor into decisions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPECTED SPEEDUP BY SCENARIO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBased on {cpu_count}-core system:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"  {row['Method']:30s}: {row['Speedup']:.2f}x (efficiency: {row['Efficiency (%)']:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tidymodels2",
   "language": "python",
   "name": "py-tidymodels2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
